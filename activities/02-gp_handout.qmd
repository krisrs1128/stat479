---
title: "GP Code Snippets"
format: html
execute-dir: .
execute:
    echo: true
    message: false
    warning: false
    cache: true
---

```{r}
#| echo: false
library(tidyverse)
library(MASS)
theme_set(theme_classic() + theme(panel.border= element_rect(fill = NA, linewidth = .5)))
set.seed(2026)
```

## Setup

**Goal.** Given data $\{x_{i}, y_{i}\}_{i = 1}^{N}$ following an unknown nonlinear curve $f$,

   1. Predict $f\left(x_{m}\right)^{*}$ at test points $x_{1}^{\ast}, \dots, x_{M}^{\ast}$.
   2. Quantify associated prediction uncertainties.

**Requirements.**

   1. Incorporate domain knowledge about the function.
   2. Use interpretable hyperparameters.

**Approach.** Encode domain knowledge in a probability distribution over functions. Before seeing data, we specify a "prior" $p\left(\mathbf{f}^*\right)$. After observing data, we update to a posterior $p\left(\mathbf{f}^* \vert \mathbf{y}, \mathbf{X}\right)$ that assigns high probability only to functions consistent with observations. (This is an example of Bayesian inference).

## Multivariate Normal

1. **1D case**. $\mathcal{N}\left(\mu, \sigma^2\right)$ with mean $\mu$ and variance $\sigma^2$.

```{r}
z <- rnorm(500, mean = 2, sd = 3)
```

```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 2
df_points <- tibble(value = z) |> mutate(type = "Raw Points", y = 0)
df_hist <- tibble(value = z) |> mutate(type = "Histogram")

ggplot() +
  geom_point(data = df_points, aes(x = value, y = y), alpha = 0.6, shape = 3) +
  geom_histogram(data = df_hist, aes(x = value, y = ..count..), bins = 30, fill = "grey80", color = "black") +
  facet_grid(~ type, scales = "free_y")
```

2. **2D case.** $\mathcal{N}(\boldsymbol{\mu}, \Sigma)$ where $\boldsymbol{\mu} \in \mathbb{R}^2$ and
$$\Sigma = \begin{bmatrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{bmatrix}$$

The diagonal controls spread in each direction and the covariance $\sigma_{12}$ controls correlation: $\rho = \sigma_{12}/(\sigma_1\sigma_2)$

```{r}
mu <- c(0, 0)
Sigma <- matrix(c(1, 0.8, 0.8, 1), 2, 2)
z <- mvrnorm(n = 500, mu = mu, Sigma = Sigma)
```

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 3
n <- 500
mu <- c(0, 0)
Sigmas <- list(
  "Equal variances, no cov" = matrix(c(1, 0, 0, 1), 2, 2),
  "Different variances, no cov" = matrix(c(4, 0, 0, 0.25), 2, 2),
  "Correlated (rho = 0.8)" = matrix(c(1, 0.8, 0.8, 1), 2, 2)
)

df_list <- imap_dfr(Sigmas, ~{
  sim <- mvrnorm(n, mu, .x)
  tibble(x = sim[,1], y = sim[,2], scenario = .y) |>
    mutate(sd1 = sqrt(.x[1,1]), sd2 = sqrt(.x[2,2]), cov12 = .x[1,2], rho = cov12/(sd1*sd2))
}) |>
  mutate(scenario = factor(scenario, levels = c("Equal variances, no cov", "Different variances, no cov", "Correlated (rho = 0.8)")))

ann <- df_list |> distinct(scenario, sd1, sd2, cov12, rho) |>
  mutate(label = paste0("sd1=", round(sd1,2), ", sd2=", round(sd2,2), "\n",
                        "cov=", round(cov12,2), ", rho=", round(rho,2)))

ggplot(df_list, aes(x = x, y = y)) +
  geom_point(alpha = 0.4, size = 0.7) +
  facet_wrap(~scenario) +
  coord_fixed() +
  labs(x = "Coordinate 1", y = "Coordinate 2", title = "Bivariate normal examples") +
  geom_text(data = ann, aes(x = Inf, y = -Inf, label = label), hjust = 1.1, vjust = -0.1, inherit.aes = FALSE)
```

3. **2D Conditioning.** If $(X_1, X_2) \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)$ and we observe $X_1 = x_1$, then $X_2 | X_1 = x_1$ is univariate normal with:

   - Conditional mean: $\mu_{2|1} = \mu_2 + \rho\frac{\sigma_2}{\sigma_1}(x_1 - \mu_1)$. The adjustment is $\rho$ times $x_1$'s z-score, converted to $X_2$ units.
   - Conditional variance: $\sigma_{2|1}^2 = \sigma_2^2(1 - \rho^2)$. Higher correlation means lower conditional variance (more certainty about $X_2$ after seeing $X_1$).

4. **D-dimensional case:** $\mathcal{N}(\boldsymbol{\mu}, \Sigma)$ where $\boldsymbol{\mu} \in \mathbb{R}^D$ and $\Sigma \in \mathbb{R}^{D \times D}$. Diagonal $\sigma_{ii}^{2}$ controls each dimension's spread. Correlation $\rho_{ij} = \frac{\sigma_{ij}}{\sigma_{ii}\sigma_{jj}}$ between dimensions $i$ and $j$.

```{r}
D <- 100
mu <- rep(0, D)
Sigma <- matrix(0.9, D, D)
diag(Sigma) <- 1
z <- mvrnorm(n = 20, mu = mu, Sigma = Sigma)
```

```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 3
#| out-width: 40%
ggplot(as_tibble(z), aes(V1, V2)) +
    geom_point(alpha = 0.6, size = 2) +
    geom_vline(xintercept = 0) +
    geom_hline(yintercept = 0) +
    labs(x = "First coordinate z[1]", y = "Second coordinate z[2]")
```

5. **General conditioning.** Partition variables into $\mathbf{X}_A$ (unobserved) and $\mathbf{X}_B$ (observed). Then $\mathbf{X}_A | \mathbf{X}_B = \mathbf{x}_B$ is multivariate normal with:
   - Mean: $\boldsymbol{\mu}_{A|B} = \boldsymbol{\mu}_A + \Sigma_{AB}\Sigma_{BB}^{-1}(\mathbf{x}_B - \boldsymbol{\mu}_B)$.
   - Covariance: $\Sigma_{A|B} = \Sigma_{AA} - \Sigma_{AB}\Sigma_{BB}^{-1}\Sigma_{BA}$

## Structured Priors

Suppose we want to simulate smooth random functions $f^*$ evaluated at $0, 0.01, \ldots, 0.99$. We represent the function as a 100-dimensional vector:
$$\mathbf{f}^* = [f^*(0), f^*(0.01), f^*(0.02), \ldots, f^*(0.99)]^\top$$
Each grid point corresponds to one dimension. So $f^*(0)$ is dimension 1, $f^*(0.01)$ is dimension 2, etc.

1. **Idea 1: Constant Correlation**. Sample $\mathbf{f}^* \sim \mathcal{N}(0, \Sigma)$ with constant correlation $\rho_{ij} = \rho$ for all pairs.

   - If $\rho = 0$: all points are independent, giving white noise.
   - If $\rho = 0.99$: all points move together, giving nearly flat functions.

   ```{r}
   #| echo: false
   #| fig-width: 6
   #| fig-height: 4
   #| out-width: 60%
    D <- 100
    n <- 20
    mu <- rep(0, D)

    make_data <- function(rho) {
    Sigma <- diag(1, D)
    if (rho > 0) Sigma[] <- rho; diag(Sigma) <- 1

    mvrnorm(n, mu, Sigma) %>%
        as_tibble(.name_repair = ~paste0("d", seq_len(D))) %>%
        mutate(sample_id = row_number()) %>%
        pivot_longer(starts_with("d"),
                    names_to = "dimension",
                    names_transform = list(dimension = ~as.integer(str_remove(., "d"))),
                    values_to = "value") %>%
        mutate(scenario = paste0("rho[jk]==", rho))
    }

    bind_rows(make_data(0), make_data(0.99)) %>%
    ggplot(aes(dimension, value, group = sample_id, color = sample_id == n)) +
    geom_line() +
    scale_color_manual(values = c("grey", "#e50d0d"), guide = "none") +
    facet_wrap(~scenario, ncol = 1, labeller = label_parsed) +
    labs(x = "Dimension (j)", y = "Value",
        title = "Multivariate normal (D=100): independent vs highly correlated")
   ```

5. **Idea 2: Block Correlation.** Divide the grid into blocks. Within each block, set high correlation (e.g., $\rho = 0.99$). Between blocks, no correlation. Looks better but still not smooth.

   ```{r}
   #| echo: false
   #| fig-width: 5
   #| fig-height: 3.5
   #| out-width: 60%
    D_blk <- 100
    n_blk <- 20
    block_size <- 25
    rho_within <- 0.99

    # Create block-diagonal covariance matrix
    Sigma_blk <- matrix(0, D_blk, D_blk)
    for (start in seq(1, D_blk, by = block_size)) {
    inds <- start:(start + block_size - 1)
    Sigma_blk[inds, inds] <- rho_within
    }
    diag(Sigma_blk) <- 1

    mvrnorm(n_blk, rep(0, D_blk), Sigma_blk) %>%
    as_tibble(.name_repair = ~paste0("d", seq_len(D_blk))) %>%
    mutate(sample_id = row_number()) %>%
    pivot_longer(starts_with("d"),
                names_to = "dimension",
                names_transform = list(dimension = ~as.integer(str_remove(., "d"))),
                values_to = "value") %>%
    mutate(scenario = paste0("rho[blk]==", rho_within)) %>%
    ggplot(aes(dimension, value, group = sample_id, color = sample_id == n_blk)) +
    geom_line() +
    scale_color_manual(values = c("grey", "#e50d0d"), guide = "none") +
    scale_x_continuous(breaks = seq(1, D_blk, by = 10)) +
    facet_wrap(~scenario, labeller = label_parsed) +
    labs(x = "Dimension (j)", y = "Value",
        title = "Multivariate normal (D=100): block-diagonal covariance")
   ```

   _Exercise: How would the plot change if we had not used the same correlation within each block?_

1. **Idea 3: Distanced-based correlation**: Let correlation decay with distance. For points $x, x'$, set:
$$\sigma_{ij} = k(x, x') = \exp\left[-\frac{1}{2}(x - x')^2\right]$$

   Nearby points have high correlation; distant points nearly independent.

   ```{r}
x <- seq(0, 0.99, by = 0.01)
D <- length(x)

K <- matrix(0, nrow = D, ncol = D)
for (i in seq_len(D)) {
    for (j in seq_len(D)) {
        K[i, j] <- exp(-0.5 * (x[i] - x[j])^2)
    }
}

z <- mvrnorm(n = 20, mu = rep(0, D), Sigma = K)
   ```

   ```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 2.5
#| out-width: 60%
df1 <- as_tibble(z) |>
    mutate(sample_id = row_number()) |>
    pivot_longer(cols = -sample_id, names_to = "dimension", values_to = "value") |>
  mutate(dimension = as.integer(str_remove(dimension, "V")))

ggplot(df1, aes(x = dimension, y = value)) +
  geom_line(aes(group = factor(sample_id), col = sample_id == n)) +
  scale_color_manual(values = c("grey", "#e50d0d"), guide = FALSE) +
  labs(x = "Dimension (j)", y = "Value",
    title = "Multivariate normal w/ decaying covariance")
   ```

   We have simulated our first GP!

## Kernel Functions

1. The function $k(x, x')$ is called a **kernel** and encodes our domain knowledge about the function we want to learn.

   - Smooth functions. The example above is an example of an RBF kernel with $\sigma_{f} = \ell = 1$.
   $$k_{\text{RBF}}(x, x') = \sigma_f^2 \exp\left(-\frac{(x - x')^2}{2\ell^2}\right)$$

   - Periodic functions.
   $$k_{\text{per}}(x, x') = \sigma_f^2 \exp\left(-\frac{2}{\ell^2}\sin^2\left(\frac{\pi|x-x'|}{p}\right)\right)$$

   If we substitute this line into our earlier code snippet, we periodic curves.
   ```{r}
   K[i, j] <- exp(-2 * sin(pi * abs(x[i] - x[j]) / 0.2)^2)
   ```

   ```{r}
   #| echo: false
   #| fig-width: 5
   #| fig-height: 2.5
   #| out-width: 60%
    for (i in seq_len(D)) {
        for (j in seq_len(D)) {
            K[i, j] <- exp(-2 * sin(pi * abs(x[i] - x[j]) / 0.25) ^ 2)
        }
    }

    mu <- rep(0, D)
    z <- mvrnorm(n = 20, mu = mu, Sigma = K) # draws 20 curves
    df1 <- as_tibble(z) |>
        mutate(sample_id = row_number()) |>
        pivot_longer(cols = -sample_id, names_to = "dimension", values_to = "value") |>
    mutate(dimension = as.integer(str_remove(dimension, "V")))

    ggplot(df1, aes(x = dimension, y = value)) +
    geom_line(aes(group = factor(sample_id), col = sample_id == n)) +
    scale_color_manual(values = c("grey", "#e50d0d"), guide = FALSE) +
    labs(x = "Dimension (j)", y = "Value",
        title = "Multivariate normal w/ periodic kernel")
   ```

1. The kernel hyperparameters encode domain knowledge,
    - $\ell$: lengthscale (controls smoothness)
    - $\sigma_f^2$: overall function variance
    - $p$: period

4. We can define new kernels through addition and multiplication. For smooth + periodic behavior:
   $$k_{\text{fancy}}(x, x') = k_{\text{RBF}}(x, x') + k_{\text{per}}(x, x')$$
   The associated functions reflect both types of domain knowledge:

   ```{r}
   #| echo: false
   #| fig-width: 5
   #| fig-height: 2.5
   #| out-width: 60%
    K <- matrix(0, nrow = D, ncol = D)
    for (i in seq_len(D)) {
        for (j in seq_len(D)) {
            rbf <- 2 * exp(-(0.5 / .05) * (x[i] - x[j]) ^ 2)
            periodic <- exp(-2 * sin(pi * (x[i] - x[j]) / 0.2) ^ 2)
            K[i, j] <- rbf + periodic
        }
    }

    mu <- rep(0, D)
    z <- mvrnorm(n = 20, mu = mu, Sigma = K) # draws 20 curves

    df1 <- as_tibble(z) %>% mutate(sample_id = row_number()) %>%
    pivot_longer(cols = -sample_id, names_to = "dimension", values_to = "value") %>%
    mutate(dimension = as.integer(str_remove(dimension, "V")))

    ggplot(df1, aes(x = dimension, y = value)) +
    geom_line(aes(group = factor(sample_id), col = sample_id == n)) +
    scale_color_manual(values = c("grey", "#e50d0d"), guide = FALSE) +
    labs(x = "Dimension (j)", y = "Value",
        title = "RBF + Periodic Kernel")
   ```


5. We call this prior over functions a **Gaussian Process**,
$$f \sim \mathcal{GP}(0, k(x, x'))$$

## Updating from Data

1. Recall our goal is to predict $f(x^*)$ given observations $y_1, \ldots, y_N$ at $x_1, \ldots, x_N$. We assume $y_{i}$ are noisy observations,
$$\begin{align}
y_i &= f(x_i) + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0, \sigma_n^2)\\
f &\sim \mathcal{GP}(0, k(x, x'))
\end{align}$$

1. The main idea is to use conditioning. Stack the unknown $f(x^*)$ with observations $\mathbf{y} = [y_1, \ldots, y_N]^\top$. It turns out that,

   $$\begin{bmatrix} f(x^*) \\ \mathbf{y} \end{bmatrix} \sim \mathcal{N}\left(\mathbf{0}, \begin{bmatrix} k(x^*, x^*) & \mathbf{K}_* \\ \mathbf{K}_*^\top & \mathbf{K} + \sigma_n^2\mathbf{I} \end{bmatrix}\right)$$

   where:

   - $\mathbf{K}_*$: row vector with entries $k(x^*, x_i)$ (similarity of $x^*$ to training points)
   - $\mathbf{K}$: $N \times N$ matrix with entries $K_{ij} = k(x_i, x_j)$ (training point similarities)
   - $\sigma_n^2\mathbf{I}$: observation noise

1. We make predictions using the conditional mean of the first coordinate given the rest. Plugging in $\mathbf{X}_{A} = f\left(x^\ast\right)$ and $\mathbf{X}_{B} = \mathbf{y}$ into our earlier conditioning formula yields,
   $$\mu_* = \mathbf{K}_*(\mathbf{K} + \sigma_n^2\mathbf{I})^{-1}\mathbf{y}$$ {#eq-gp-mean}

   This is a weighted average: training points similar to $x^*$ get higher weight.

4. For $M$ test points $\mathbf{f}_* = [f(x_1^*), \ldots, f(x_M^*)]^\top$, we apply the same conditioning approach but now need to track correlations among the test points with one another:

   $$\begin{bmatrix} \mathbf{f}_* \\ \mathbf{y} \end{bmatrix} \sim \mathcal{N}\left(\mathbf{0}, \begin{bmatrix} \mathbf{K}_{**} & \mathbf{K}_* \\ \mathbf{K}_*^\top & \mathbf{K} + \sigma_n^2\mathbf{I} \end{bmatrix}\right)$$

   where:
   - $\mathbf{K}_{**}$: $M \times M$ matrix with entries $(\mathbf{K}_{**})_{ij} = k(x_i^*, x_j^*)$ (test point similarities)
   - $\mathbf{K}_*$: $M \times N$ matrix with entries $(\mathbf{K}_*)_{ij} = k(x_i^*, x_j)$ (test-to-training similarities)

   Applying the conditioning formula gives @eq-gp-mean again, but now $\mathbf{K}_*$ is a matrix, so $\boldsymbol{\mu}_*$ is a vector with one entry for each of the test locations $x_{1}^{\ast}, \dots, x_{M}^{\ast}$.

## Quantifying Uncertainty

1. At the start, we mentioned that our goal wasn't just to make predictions at $x_{1}^{*}, \dots, x_{M}^{*}$ but also to report the associated uncertainties. Recall from the bivariate case that conditioning gave us both a mean _and a variance_. The variance tells us how much uncertainty remains after observing $X_1$. The same is true here: the conditioning formula also gives a covariance matrix that quantifies our uncertainty about $\mathbf{f}_*$ after observing $\mathbf{y}$.

1. Specifically, the conditional covariance formula yields:
   $$
   \boldsymbol{\Sigma}_{*} = \mathbf{K}_{**} - \mathbf{K}_{*}(\mathbf{K} + \sigma_n^2\mathbf{I})^{-1}\mathbf{K}_{*}^\top
   $$
   - $\mathbf{K}_{**}$ is the covariance before we collect any data.
   - The subtracted term $\mathbf{K}_{*}(\mathbf{K} + \sigma_n^2\mathbf{I})^{-1}\mathbf{K}_{*}^\top$ describes the uncertainty reduction from conditioning
   - The diagonal entries give the variance at each test location: $\text{Var}(f(x_i^*) | \mathbf{y})$

## Code Example

1. Observe 10 noisy points from a quadratic:
   ```{r}
   x_obs <- runif(10)
   sigma_n <- 0.1
   y_obs <- 3 * (x_obs - 0.5)^2 - 0.5 + rnorm(10, 0, sigma_n)
   N <- length(x_obs)
   ```

   ```{r}
   #| echo: false
   #| fig-width: 5
   #| fig-height: 2.5
   #| out-width: 60%
df_obs <- tibble(x = x_obs, value = y_obs)
ggplot() +
  geom_point(data = df_obs, aes(x = x, y = value),
             col = "#a80de5ff", size = 3)
   ```

2. Define test grid and covariances:
   ```{r}
x <- seq(0, 0.99, by = 0.01)
D <- length(x)

# K_{**}: test-test similarities
K <- matrix(0, nrow = D, ncol = D)
for (i in seq_len(D)) {
    for (j in seq_len(D)) {
        K[i, j] <- exp(-0.5 * (x[i] - x[j])^2)
    }
}

# K_*: test-training similarities
K_star <- matrix(0, nrow = D, ncol = N)
for (i in seq_len(D)) {
    for (j in seq_len(N)) {
        K_star[i, j] <- exp(-0.5 * (x[i] - x_obs[j])^2)
    }
}

# K: training-training similarities
K_obs <- matrix(0, nrow = N, ncol = N)
for (i in seq_len(N)) {
    for (j in seq_len(N)) {
        K_obs[i, j] <- exp(-0.5 * (x_obs[i] - x_obs[j])^2)
    }
}
   ```

3. Apply conditioning formulas:

   ```{r}
mu_cond <- K_star %*% solve(K_obs + sigma_n^2 * diag(N)) %*% y_obs
Sigma_cond <- K - K_star %*% solve(K_obs + sigma_n^2 * diag(N)) %*% t(K_star)
var_cond <- diag(Sigma_cond) # defines ribbon width
   ```

   ```{r}
   #| echo: false
   #| fig-width: 5
   #| fig-height: 2.5
   #| out-width: 60%
df_mean <- tibble(
  x = x,
  mean = as.vector(mu_cond),
  lower = mean - 2 * sqrt(var_cond),
  upper = mean + 2 * sqrt(var_cond)
)

df_obs <- tibble(x = x_obs, value = y_obs)
ggplot() +
  geom_ribbon(data = df_mean, aes(x = x, ymin = lower, ymax = upper),
              fill = "#36ae9eff", alpha = 0.5) +
  geom_line(data = df_mean, aes(x = x, y = mean),
            col = "#36ae9eff", linewidth = 1) +
  geom_point(data = df_obs, aes(x = x, y = value),
             col = "#a80de5ff", size = 3)
   ```


4. Sample posterior functions:

```{r}
f_post <- mvrnorm(n = 20, mu = mu_cond, Sigma = Sigma_cond)
```

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 2.5
#| out-width: 60%
df_post <- as_tibble(f_post, .name_repair = ~paste0("x", seq_len(ncol(f_post)))) %>%
  mutate(sample_id = row_number()) %>%
  pivot_longer(starts_with("x"),
               names_to = "dimension",
               names_transform = list(dimension = ~as.integer(str_remove(., "x"))),
               values_to = "value") %>%
  mutate(x = x[dimension])

df_obs <- tibble(x = x_obs, value = y_obs)

ggplot() +
  geom_line(data = df_post, aes(x = x, y = value, group = sample_id),
            col = "#36ae9eff", alpha = 0.5, linewidth = 0.5) +
  geom_point(data = df_obs, aes(x = x, y = value),
             col = "#a80de5ff", size = 3)
```