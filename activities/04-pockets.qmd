---
title: "Boosting and Pocket Measurements"
author: Kris Sankaran
date: 2026-02-18
bibliography: references.bib
format:
  html:
    embed-resources: true
    html-math-method: katex
execute:
    echo: true
    message: false
    warning: false
    cache: false
---

_[Code](https://github.com/krisrs1128/stat479_notes/blob/master/activities/04-pockets.qmd)_

## Motivation

```{r}
#| label: setup
library(tidyverse)
library(DALEX)
library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(mlr3pipelines)
library(xgboost)
set.seed(20260216)
theme_set(theme_classic())
```

This example is based on the article
["Pockets"](https://pudding.cool/2018/08/pockets/) by Jan Diehm and Amber Thomas
for the pudding.  Their question -- are pocket sizes systematically smaller in
women's pants, to the point that they wouldn't be able to hold common objects
like wallets or smartphones? They gathered
[data](https://github.com/the-pudding/data/blob/master/pockets/measurements.csv)
on pocket characteristics from major brands.

```{r}
#| label: load-data
pockets <- read_csv("https://github.com/the-pudding/data/raw/refs/heads/master/pockets/measurements.csv")
pockets
```

Our approach is to predict whether pockts are men's or women's using all the
available features. High accuracy would mean that pockets differ systematically
in style, price, or pocket dimensions. We can then inspect variable importance
to see which features drive the distinction.

![](figures/pockets.gif)

## Data Processing

We will convert character valued columns into to factors for easy encoding later
on. We omit `fabric` and `name` (ID-like variables with many levels) and
`maxHeightFront` (so straongly associated with the response that it would
dominate interpretation and make this example less representative).

```{r}
#| label: preprocess
pockets <- pockets |>
    select(!any_of(c("fabric", "name", "maxHeightFront"))) |>
    mutate(across(where(is.character), as.factor))
```

Here's the processed version of the data.

```{r}
#| label: preview-data
pockets
```

## Hyperparameter Tuning

Next, we define our hyperparameter tuning pipeline. The setup resembles the
`03-gotv_cart.qmd` analysis, but with two changes. First, we use
`classif.xgboost` to specify that we want a boosting model. Second, we add a
`po("encode")` preprocessing step to turn categorical features into numerical
encodings. Although boosting can handle categorical types in theory, the xgboost
implementation in mlr3 requires numerical predictors. We could do this manually,
but the mlr3 pipeline manages it for us.

```{r}
#| label: tuning-setup
task <- as_task_classif(
    pockets,
    target = "menWomen"
)

learner <- as_learner(
    po("encode") %>>%
    lrn(
        "classif.xgboost",
        nrounds = to_tune(100, 300),
        max_depth = to_tune(1, 3),
        eta = to_tune(1e-3, 1e-1),
        predict_type = "prob"
    )
)

instance <- ti(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 2),
  measures = msr("classif.ce"),
  terminator = trm("none")
)
```

Now that we've setup the pipeline, let's search over hyperparameters to find a
reasonable choice.

```{r}
#| label: optimize
tuner <- tnr("grid_search", resolution = 3, batch_size = 3)
tuner$optimize(instance)
```

The cross entropy shows that quick growing trees (with some interactions) did
the best.  Cross-entropy might not be as intuitive as accuracy, but values near
0 mean perfect classification and near $-\log (0.5) \approx 0.69$ mean random
guessing. Our model does much better than that baseline.

```{r}
#| label: tuning-results
as.data.table(instance$archive) |>
  rename_with(~ str_remove(., "classif.xgboost.")) |>
  arrange(classif.ce) |>
  select(1:4)
```

## Refit Model

Let's refit the model on the entire dataset (so far, we've been training on CV
split versions). This is the model that we would use in practice, and it's what
we'll interpret next.

```{r}
#| label: refit
learner$param_set$values <- instance$result_learner_param_vals
learner$train(task) # retrain on all data
fit <- learner$model
```

## Interpretation

A simple check: compare predicted probabilities against the actual class. The
probabilities barely overlap, meaning that the measured features predict men's
vs. women's pocket type quite reliably.

```{r}
#| label: predicted-probs
#| echo: false
data.frame(y_hat = learner$predict_newdata(pockets)$prob[, 1], y = pockets$menWomen) |>
  ggplot(aes(y_hat, fill = y)) +
  geom_histogram(position = "identity", alpha = 0.7) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = c("#3dafaf", "#b7552c")) +
  labs(x = "Prediction Probability", y = "Count")
```

The xgboost package gives its own variable importance measure. This looks at the
variables which were split most often during training, as well as how much they
increased node purity when they were split. We can see tha tthe top four
variables are all related to the dimensions of hte pocket.

```{r}
#| label: variable-importance
fit <- learner$model$classif.xgboost$model
fit <- attr(fit[[1]], "model") # messy, but seems to be only way to access original model....
xgb.importance(model = fit)
```

Finally, partial dependence plots. Each line represents one pocket from the
data. By passing a custom `predict_function` argument, we can study changes in
predicted probabilities rather than just class labels. Each curve shows how a
pocket's predicted probability shifts when we intervene on the plotted
variables. The effect of `minHeightFront` is especially clear: once the minimum
pocket height exceeds about 15, the model predicts men's pocket.

```{r}
#| label: partial-dependence
#| fig-width: 8
#| fig-height: 3.5
explainer <- DALEX::explain(
  learner,
  data = pockets,
  y = as.numeric(pockets$menWomen),
  predict_function = \(model, newdata) {
    model$predict_newdata(newdata)$prob[, 1]
  }
)

pdp <- model_profile(explainer, variables = c("minHeightFront", "minWidthFront", "rivetHeightFront"))
plot(pdp, geom = "profiles")
```

Indeed, if we go back to the raw data, we can see a clear difference in these
variables between mens and women's pockets.

```{r}
#| label: height-histogram
#| echo: false
#| fig-width: 8
#| fig-height: 3
pockets |>
    pivot_longer(c(minHeightFront, minWidthFront, rivetHeightFront), names_to = "variable") |>
    ggplot() +
    geom_histogram(
        aes(value, fill = menWomen),
        position = "identity", alpha = 0.7
    ) +
    scale_fill_manual(values = c("#3dafaf", "#b7552c")) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    facet_wrap(~variable, scales = "free")
```