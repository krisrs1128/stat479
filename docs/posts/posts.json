[
  {
    "path": "posts/2021-04-23-week14-4/",
    "title": "A History of Data Visualization up to 1900",
    "description": "A look at the origins of the field.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-29",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nThere could probably be an entire class taught on the history of data visualization. The reason it is worth covering in an otherwise practically-oriented class is that,\nHistorical study can illuminate the core intellectual foundation on which the entire discipline of data visualization is built.\nThe limits, biases, and trends of our current era become clear when considering it within its full historical context.\nIt’s often possible to draw inspiration from past masters.\nIt can be humbling to realize that now-commonplace ideas had to be discovered. If it took until 1833 until the first scatterplot was published, then what ideas have we yet to find?\n\nThe reading divides up the history of data visualization into 8 epochs. In these notes, we will consider the 5 epochs before 1900.\nBefore 1600: Early Maps and Diagrams\nIt might come as a surprise, but visualization has been around since the invention of writing. The Egyptians made maps, and there are examples multiple time series plots from the 10th century.\n\n\n\nFigure 1: Possibly the earliest multiple time series visualization, showing the movement of various planets, from In Somnium Scripionus.\n\n\n\nHowever, most visualizations were focused on physical geographical or astronomical quantities, and even these quantities were only imprecisely measured. Without more formal data gathering instruments, there could not be much data visualization.\n1600 - 1700: Measurement and Theory\nThe situation begins to change around 1600, at the dawn of the scientific revolution. All of a sudden, precise measurement of physical space become possible. Also, important new mathematical ideas were introduced, like probability, calculus, and reasoning about functions. This created the right environment for the design of some of the first truly sophisticated data visualizations, like\nChristopher Scheiner’s 1630 visualization of sunspots over time (a first instance of faceting),\n\n\n\nFigure 2: Scheiner’s visualization of sunspots, which played a role in his dialogues with Galileo.\n\n\n\nEdmond Halley’s plot of barometric pressure against altitude (an first instance of one feature being plotted against another), and\n\n\n\nFigure 3: One of the first bivariate plots, relating barometric pressure to altitude. Note that absence of the true observed data.\n\n\n\nEdmond Halley’s plot of wind speed over the ocean (a first visualization of a vector field).\n\n\n\nFigure 4: A plot of trade winds, appearing in An Historical Account of the Trade Winds, and Monsoons, Observable in the Seas between and near the Tropicks, with an Attempt to Assign the Phisical Cause of the Said Wind.\n\n\n\nDuring this period, the scientific value of visual thinking was more or less established. However, there were still only relatively few graphical forms available, and the focus continued to remain on visualizing physical quantities, rather than more general social, economic, biological, or ecological data.\n1700 - 1800\nDuring this period, many new graphical forms were invented, including timelines, cartograms, functional interpolations, line graphs, bar charts, and pie charts. Further, forms from earlier, like maps and function plots, became more firmly established.\nThis was also when three-color printing was invented. Before this point, color could not be used as an encoding channel.\nIn this century, governments also began large-scale data collection of social and economic statistics1. One of the most prolific inventors of data visualizations, William Playfair2, was used visualization to study a variety of economic problems. In addition to inventing line, bar, and pie charts, he experimented with original ways of composing multiple graphs to suggest the relationships between variables.\n\n\n\nFigure 5: One of William Playfair’s data visualizations, juxtaposing the price of wheat with growth in wages.\n\n\n\n1800 - 1850\nThis was a period of maturation for the field of data visualization. By this point, visualization had become standard in scientific publications. Advances in printing technology also made it also became easier to mass produce visualizations.\nIt was also around this time that the scope of problems studied through visualization expanded far beyond display of purely physical (geographical and astronomical) applications. Two areas in particular flourished, applications to social science and to engineering.\nVisualization in the social sciences began to emerged in response to government sponsored collection of social statistics – data about crime, births, and deaths, among other topics. This wasn’t a purely intellectual exercise: understanding demographic trends was important for countries that were often at war with another.\n\n\n\nFigure 6: A visualization of property crime statistics, by Andre-Michel Guerry (1829).\n\n\n\nIn engineering, the idea that visualization could serve as a computational aid become more and more common. For example, rather the chart below, by Charles Joseph Minard, displays the cost of transporting goods across different stretches of a canal. Vertical breaks correspond to cities along the canal, and the area of the square between cities encodes the cost of transportation between those cities. While the information could be stored in a table, it becomes easier to perform mental computations (and make guesstimates) using the display.\n\n\n\nFigure 7: Minard’s 1844 visualization of the transport costs across the Canal du Centre in France.\n\n\n\n1850 - 1900: The Golden Age\nIt might be counterintuitive that there was a golden age of visualization a century before the first computers were invented. However, a look at the visualizations from this period demonstrate that this was a period where visualizations inspired scientific discoveries, informed commercial decisions, and guided social reform.\nFor example, in public health, Florence Nightingale invented new visualizations to demonstrate the impact of sanitary practices in hospital-induced infections and death. Similarly, it was a visualization that guided John Snow to the source of the 1855 cholera epidemic.\n\n\n\nFigure 8: Florence Nightingale’s visualization of hospital mortality statistics from the Crimean War, used to support a campaign for sanitary reforms.\n\n\n\nSome of the graphical innovations include,\n3D function plots. The plot below, by Luigi Perozzo, shows population size broken down into age groups and traced over time.\n\n\n\nFigure 9: Luigi Perrozo’s 1879 3D visualizations of population over time.\n\n\n\nFlow diagrams. Charles-Joseph Minard, who we met before with the canal visualization, was a master of these displays. One in particular is widely considered a masterpiece, it shows the size of Napoleon’s army during it’s Russian Campaign.\n\n\n\nFigure 10: Minard’s flow display of the size of Napoleon’s army during the Russia Campaign.\n\n\n\nMultivariate visualization. Francis Galton made some of the first efforts to visualize more than 3 variables at a time. His Meteorographica, published in 1863, contained over 600 visualizations of weather data that had been collected for decades, but never visualized. One plot, shown below, led to the discovery of anticyclones.\n\n\n\nFigure 11: Galton’s display of weather patterns. Low pressure (black) areas tend to have clockwise wind patterns, while high pressure (red) tends to have anticlockwise wind patterns.\n\n\n\nThis was also an age of state-sponsored atlases. More than sponsoring the collection of data, governments assembled teams to visualize the results for official publication. From 1879 to 1897, the French Ministry of Public Works published the Albums de Statistique Graphique, which under the guidance of Émile Cheysson, developed some of the most imaginative and ambitious visualizations of the era.\n\n\n\nFigure 12: A visualization of the flow of passengers and goods through railways from Paris. Each square shows the breakdown to cities further away, and color encodes the railines.\n\n\n\n\nThe word “statistics” comes from the same root as “state”, since it originally focused on data collected for governance.↩︎\nHe has been described as an “engineer, political economist and scoundrel.”↩︎\n",
    "preview": "https://uwmadison.box.com/shared/static/c79fbmfc3mff9ota1tzxtvv4gp7e3pul.jpeg",
    "last_modified": "2021-04-26T15:06:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-23-week14-3/",
    "title": "Asking Better Questions",
    "description": "What is the purpose of data analysis?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-28",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nIn these notes, we will review “Tukey, Design Thinking, and Better Questions,” a blog post by Roger Peng. In the post, he discusses the importance of formulating good questions in data analysis. While the process of refining questions is central to any successful, real-world analysis, it is rarely discussed in formal statistics textbooks, which usually restrict themselves to developments in theory, methodology, or computation.\nThe post itself is based on a line of thinking from the statistician John Tukey’s 1962 paper, “The Future of Data Analysis.” Perhaps the most famous quote from this paper is,\n\nFar better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.\n\nand throughout the article, Tukey argues that data analysts should not (1) be called upon to provide stamps of authority and (2) be distracted by the allure of proving optimal algorithms in settings that are likely not that realistic.\nThis of course begs the question – what should data analysts be doing? Tukey never directly discusses this, but Peng’s thesis is that the purpose of data analysis is to spark the formulation of better questions. This idea is captured in the diagram below. The idea is that algorithms (visual, predictive, and inferential) can help us strengthen evidence that certain patterns exist. However, if those patterns answer irrelevant questions, then there is little point in the analysis. With so many algorithms available, it’s easy to spend substantial effort gathering evidence to answer low quality questions. Instead, a good data scientist should focus on improving the quality of the questions asked, even if that means that the answers will be less definitive.\n\n\n\nFigure 1: Peng’s summary of misplaced priorities in the data science process.\n\n\n\nWhat does it mean to ask better questions? Peng argues that better questions are “sharper,” providing more discriminating information and guiding research to more promising directions. In my view, a sharper question is one that helps inform decisions that have to be made (all data aside), either by adding to our body of beliefs or bringing new uncertainties into focus.\nThe implication is that data analysis is not about finding “the answer.” Instead, it should be about clarifying what can and cannot be answered based on the data. But more, it can help clarify what questions we really want answered – in Peng’s words, “we can learn more about ourselves by looking at the data.”\nThe final example in the reading is that the residuals in an analysis are themselves data worth examining. They inform whether a model (and the associated way of conceptualizing the problem) is really appropriate, or whether a reformulation would be beneficial.\nAt the end, the main idea is that data analysis equips us to ask better questions.\n\n\n\n",
    "preview": "https://simplystatistics.org/post/2019-04-17-tukey-design-thinking-and-better-questions_files/question_evidence.png",
    "last_modified": "2021-04-26T15:06:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-23-week14-2/",
    "title": "Design Process Case Study",
    "description": "Tracing the refinement of questions and design.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nHow does a visualization expert go about creating a data visualization? Perhaps the most useful lesson from this reading is that good visualizations don’t materialize out of thin air – there is always a creative process involved, steps where it’s unclear what the final result will be, even for a data visualization genius like Shirley Wu. We’re lucky that she has documented this process for us, so that we might be able to take away a few lessons for our own reflection.\nHer visualization, “655 Frustrations of Data Visualization”, is based on an online data visualization survey. It had 45 questions (\"How many years have you been doing data visualization? What percent of your day is focused on data prep work? …). There are 981 responses, probably mostly submitted by the survey initiator’s internet following.\n\n\n\nFigure 1: A few entries from the data visualization survey. The full data are publicly available here.\n\n\n\nQuestion Formulation\nAt the start of the project – before writing code – there is the problem of choosing a guiding question. Her initial question was “Why might people leave the field?” This was a timely question, because, after a few years of high activity and visibility in industry, data visualization seemed to be cooling down. However, this question was not directly answerable with the data at hand, so instead, she focused on the proxy question, “Do you want to spend more time or less time visualizing data in the future?”\nThis is an important lesson: there is often a distinction between what we really want to know and what the data can tell us.\nWe will revisit this theme of asking sharper questions in the next reading. Data analysis should be driven by curiosity about the world, not simply the data that happen to be conveniently accessible.\n\nTo see what within the data are relevant to this guiding question, she then conducted an exploratory analysis1, studying the marginal distributions of all the available questions. For example, visualizing the “percentage of time” questions, it became clear that most people worked on a mix of multiple data-related tasks in their work – the particular mix might help understand whether people want to stay in the field.\n\n\n\nFigure 2: Example exploratory displays of the ‘percentage of time’ questions on the data science survey.\n\n\n\nThe exploratory analysis also revealed that there are some questions that would not be useful for answering the guiding question. For example, many of the qualitative responses were not useful2. It’s easy to feel responsible for visualizing all the data that are available, but this is not necessary. It’s far more important to focus on the guiding question(s).\nInitial Design\nThe initial design answered whether there is a relationship between (a) the survey respondent wanting to do more data visualization in the future, and (b) the current fraction of time spent on design. This was visually encoded using a stacked barchart. However, the display was not that informative, because most respondents wanted to do more data visualization in the future3.\n\n\n\nFigure 3: The initial design used a bar chart to see whether experience was related to interest in further work in data visualization.\n\n\n\nIn this situation, it seemed like perhaps additional context would help. However, the resulting faceted barchart was difficult to make sense of, again because any relationships between variables were weak or nonexistent.\n\n\n\nFigure 4: The faceted barchart did not add much information relevant to the guiding question.\n\n\n\nRedesigns\nAt this point, two significant changes to the design were made, one to the question, and one to the design. The question was reframed from “do you want to continue working in data visualization” to “do you experience any frustrations with data visualization.” This question is still related to the guiding question, but shows much more variation across respondents. For the design, the encoding changed from a stacked barchart to a beeswarm plot. Unlike the barchart, which aggregates responses into bins, the beeswarm makes it possible to see every single respondent.\n\n\n\nFigure 5: A redesigned plot. The left and right panels separate respondents with and without frustrations, vertical position encodes job role, and color gives number of years of experience.\n\n\n\nA few more refinements were made. Instead of placing those with and without frustrations far apart on the page, they were rearranged to share the same \\(x\\)-axis4. Also, instead of coloring circles by years of experience, color was used to represent the percentage of the day spent on data visualization. Again, these changes reflect sharpening of both design and questions.\nIn the final version of the static display, a boxplot was introduced to summarize the most salient characteristics of each beeswarm. Then, instead of just plotting the points in two parallel regions, they were made to “rise” and “fall” off the boxplots, depending on whether the respondents experienced frustrations. This kind of visual metaphor takes the visualization to another level; it becomes more than functional, it becomes evocative.\n\n\n\nFigure 6: The final version of the static visualization.\n\n\n\nInterpretation\nOnly at this point is interactivity introduced into the visualization. The interactivity is simple – views transition into one another depending on selected questions – but provides an effective alternative to simply faceting all pairs of questions.\n\n\n\nFigure 7: The visualization above with interactivity added in.\n\n\n\nFinally, this interactive visualization is used for an extensive exploration. This is often when the effectiveness of a visualization can be evaluated. Ultimately, visualization should help inform our body of beliefs, guiding the actions we take (either in the short or long-term). If it’s hard to draw these sorts of inferences, then a visualization is not particularly functional.\n\n\n\nFigure 8: A screenshot of notes from the designer’s exploration of the resulting visualization.\n\n\n\nTo guide the reader, this investigative work was then incorporated into the visualization. These additional details allow the visualization to stand alone, it becomes a self-explanatory intellectual artifact.\nConclusion\nWrapping up, the final visualization is clearly the culmination of substantial intellectual labor over the course of weeks (if not months). The result is both beautiful and informative. This is an ideal to strive for – the crafting of data visualizations that can guide discovery and change.\nOne final note. It’s often useful to study the development of projects that you find interesting. Sometimes, authors share their code on github, or earlier versions are available through technical reports or recorded talks. This additional context can shed light on the overall inspiration and intention of the project, and especially when starting out, imitation can be an effective strategy for learning.\n\nUsing vega-lite!↩︎\nAre there really 100x more pie charts than scatterplots in industry??↩︎\nPerhaps an instance of selection bias.↩︎\nThe less our eyes have to travel across the page to make a comparison, the more efficient the visualization.↩︎\n",
    "preview": "https://uploads-ssl.webflow.com/5fa9d88da1855554db37b1e1/5fc61cc4356e0d312965c372_split_beeswarm.png",
    "last_modified": "2021-04-26T15:06:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-23-week14-1/",
    "title": "Final Takeaways",
    "description": "Some major themes from STAT 479, in a nutshell.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-26",
    "categories": [],
    "contents": "\nRecording, Rmarkdown\nWe’ve covered many practical visualization strategies in this course. However, I hope a few overarching themes have come across as well. In these notes, I will try articulating a few of these themes, and they are also the subject of the readings reviewed in the next few notes.\nFirst, creating a visualization is in many ways like writing an essay. A great visualization cannot simply be summoned on demand. Instead, visualizations go through drafts, as the ideal graphical forms and questions of interest become clearer.\nVisualization is not simply for consumption by external stakeholders. In data science, visualization can be used to support the process of exploration and discovery, before any takeaways have yet been found.\nVisualization can be used throughout the data science process, from data quality assessment to model prediction analysis. In fact, I would be wary of any data science workflow that did not make use of visualization at multiple intermediate steps.\nIt pays dividends to think carefully about data format. A design can be easy to implement when the data are in tidy, but not wide, format, and vice versa.\nStructured, nontabular data – think time series, spatial formats, networks, text, and images – are everywhere, and specific visualization idioms are available for each.\nSimilarly, high-dimensional data are commonplace, but a small catalog of techniques, like faceting, dynamic linking, PCA, and UMAP, are able to take us quite far.\nFinally, data visualization can be enriching. Visual thinking has helped me navigate and appreciate complexity in many contexts. Through this course, I have shared techniques that I’ve found useful over the years, and I hope you find them handy as you go off and solve real-world data science problems.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-26T15:06:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-12-week13-4/",
    "title": "Optimizing Feature Maps",
    "description": "Interpreting neurons by finding optimal inputs",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-22",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"purrr\")\nlibrary(\"keras\")\nlibrary(\"tensorflow\")\n\n\n\nSo far, we’ve visualized neural networks by analyzing the activations of learned features across observed samples. A complementary approach is to ask instead — is there a hypothetical image that would maximize the activation of a particular neuron? If we can construct such an image, then we might have a better sense of the types of image concepts to which a neuron is highly sensitive.\nWe will illustrate these ideas on a network that has been trained on Imagenet. This is a large image dataset with many (thousands of) class labels, and it is often used to evaluate image classification algorithms. The network is loaded below.\n\n\nmodel <- application_vgg16(weights = \"imagenet\", include_top = FALSE)\n\n\n\nThe main idea is to setup an optimization problem that searches through image space for an image that maximizes the activation for a particular neuron. The function below computes the average activation of a one of the feature maps. The goal is to find an image that maximizes this value for a given feature.\n\n\nmean_activation <- function(image, layer, ix=1) {\n  h <- layer(image)\n  k_mean(h[,,, ix])\n}\n\n\n\nTo implement this, we can compute the gradient of a neuron’s average activation with respect to input image pixel values. This is a measure of how much the activation would change when individual pixel values are perturbed. The function below moves an input image in the direction of steepest ascent for the mean_activation function above.\n\n\ngradient_step <- function(image, layer, ix=1, lr=1e-3) {\n  with(tf$GradientTape() %as% tape, {\n    tape$watch(image)\n    objective <- mean_activation(image, layer, ix)\n  })\n  grad <- tape$gradient(objective, image)\n  image <- image + lr * grad\n}\n\n\n\n\n\n\nFigure 1: Starting from a random image, we can take a gradient step in the image space to increase a given neuron’s mean activation.\n\n\n\nOnce these gradients can be computed, it’s possible to perform gradient ascent to solve the activation maximization problem. This ascent is encoded by the function below. We initialize with a random uniform image and then take n_iter gradient steps in the direction that maximizes the activation of feature ix.\n\n\nrandom_image <- function() {\n  tf$random$uniform(map(c(1, 150, 150, 3), as.integer))\n}\n\ngradient_ascent <- function(layer, ix = 1, n_iter = 100, lr = 10) {\n  im_seq <- array(0, dim = c(n_iter, 150, 150, 3))\n  image <- random_image()\n  for (i in seq_len(n_iter)) {\n    image <- gradient_step(image, layer, ix, lr)\n    im_seq[i,,,] <- as.array(image[1,,,])\n  }\n  \n  im_seq\n}\n\n\n\n\n\n\nFigure 2: Taking many gradient steps leads us towards an image that optimizes a neuron’s activation.\n\n\n\nBelow, we visualize the images that optimize the activations for a few neurons in layer 3. These neurons seem to be most responsive particular colors and edge orientations.\n\n\nsquash <- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\npar(mfrow = c(5, 8), mai = rep(0.00, 4))\nactivation_model <- keras_model(inputs = model$input, outputs = model$layers[[3]]$output)\nfor (i in seq_len(40)) {\n  im_seq <- gradient_ascent(activation_model, ix = i)\n  plot(as.raster(squash(im_seq[100,,,])))\n}\n\n\n\n\nFigure 3: The hypothetical images that maximize the activations for 40 different neurons. These neurons seem to pull out features related to color and edge orientations.\n\n\n\nWe can think of these features as analogous to a collection of basis functions. At the first layer, the network is representing each image as a combination of basis images, related to particular color or edge patterns.\nWe can compare these activation maximizing inputs with those associated with later layers. It seems that the basis images at this level are more intricate, reflecting textures and common objects across this dataset. For example, the polka dot pattern may be strongly activated by cat eyes.\n\n\npar(mfrow = c(5, 8), mai = rep(0.00, 4))\nactivation_model <- keras_model(inputs = model$input, outputs = model$layers[[8]]$output)\nfor (i in seq_len(40)) {\n  im_seq <- gradient_ascent(activation_model, ix = i)\n  plot(as.raster(squash(im_seq[100,,,])))\n}\n\n\n\n\nFigure 4: The results of the corresponding optimization for 40 neurons in layer 8.\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-12-week13-4/week13-4_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-04-16T20:56:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-12-week13-3/",
    "title": "Collections of Features",
    "description": "Analyzing feature activations across datasets",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-21",
    "categories": [],
    "contents": "\nReading (1, 2), Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"keras\")\nlibrary(\"magrittr\")\nlibrary(\"pdist\")\nlibrary(\"purrr\")\nlibrary(\"stringr\")\nlibrary(\"superheat\")\nlibrary(\"tidymodels\")\nset.seed(479)\n\n\n\nThe previous notes gave us a look into the features learned by a deep learning model. However, we could only look at one feature within one layer at a time. We also only studied an individual image. If we want to better understand the representations learned by a network, we will need ways of analyzing collections of features taken from throughout the network, across entire datasets.\nThis seems like an impossible task, but it turns out that, in real-world models, the learned features tend to be highly correlated. Certain patterns of activation tend to recur across similar images. This kind of structure makes it possible to use clustering and dimensionality-reduction to begin to make sense of the learned representations of individual networks.\nTo illustrate this idea, we will download the same model from before along with a larger subsample of images used in training.\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/dxibamcr0bcmnj7xazqxnod8wtew70m2.rda\", f)\nimages <- get(load(f))\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel <- load_model_hdf5(f)\n\n\n\nThe code block below save features from layers 6 and 8 from this model, for all the images we downloaded. The code for extracting features is the same as from the previous lecture, except instead of extracting features from all layers, we’re only considering these later ones. The reason we’ve focused on these deeper layers is that (1) they are smaller, so we will consume less memory on our computers, and (2) they correspond to the higher-level concepts which are more difficult to understand directly, unlike the simple edge detectors in the initial layers.\n\n\nl <- c(model$layers[[6]]$output, model$layers[[8]]$output)\nactivation_model <- keras_model(inputs = model$input, outputs = l)\nfeatures <- predict(activation_model, images)\n\n\n\nideally, we could work with a matrix of samples by features. the \\(ij^{th}\\) element would be the activation of feature \\(j\\) on observation \\(i\\).\nthis is unfortunately not immediately available. as we saw before, each feature map is actually a small array across spatial contexts, not a single number. there is no single way to aggregate across a feature map, and it is common to see people use the maximum, average, norm, or variance of a feature map as a summary for how strongly that feature activates on a given image. we will take the mean of activations.\n\n\nfeature_means <- function(h) {\n  apply(h, c(1, 4), mean) %>%\n    as_tibble()\n}\n\nh <- map_dfc(features, feature_means) %>%\n  set_colnames(str_c(\"feature_\", 1:ncol(.))) %>%\n  mutate(id = row_number())\n\n\n\nGiven this array, we can ask questions like, which neurons are most activated for a particular image? or, which images induce the largest activations for a particular neuron? In the block below, we find the 20 images that activate the most for the third feature map in layer 6. This neuron seems to have learned to recognize grass. Perhaps unsurprisingly, all the images are of dogs.\n\n\ntop_ims <- h %>%\n  slice_max(feature_3, n = 20) %>%\n  pull(id)\n\npar(mfrow = c(5, 4), mai = rep(0.00, 4))\nout <- images[top_ims,,,] %>% \n  array_tree(1) %>%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\n\nFigure 1: The 20 images in the training dataset with the highest activations for neuron 3 in layer 6. This neuron seems to be sensitive to the presence of grass in an image (which happens to be correlated with whether a dog is present).\n\n\n\nThis particular example should serve as a kind of warning. While it’s easy to imbue models with human-like characteristics, they often arrive at the answers they need in unexpected ways. We asked the model to distinguish between cats and dogs, but it is using whether the image has grass in it as a predictor. While for this dataset this may be accurate, I would expect this model to fail on an image of a cat in a grassy field.\nInstead of only investigating one neuron, we can consider all the images and neurons simultaneously. The code below makes a heatmap of the average feature activations from before. Each row is an image and each column is a feature from either layers 6 or 8. A similar example is given in the reading, where coordinated views reveal that certain patterns of neuron activation encode the lifts of the pen or specific curve shapes in a handwriting generation network.\n\n\nsuperheat(\n  h %>% select(-id),\n  pretty.order.rows = TRUE,\n  pretty.order.cols = TRUE,\n  legend = FALSE\n)\n\n\n\n\nFigure 2: A heatmap of feature map activations for layers 6 and 8, across the entire dataset. Each row is an image, and each column is a neuron. There is limited clustering structure, but there are substantial differences in how strongly different neurons activate on average.\n\n\n\nWe can also apply clustering and dimensionality reduction ideas to understand the collection of mean feature activations. For example, below we run \\(K\\)-means across images, with the hope of finding images that are viewed similarly according to the model. The 20 images closest to the centroid of cluster 3 are printed below. It seems like the model has learned to cluster all the orange images together1. This is a little surprising, considering that there are both cats and dogs that are orange, so this isn’t a particularly discriminating feature. It also suggests a way to improve the model – we could train it on recolorized input images, so that it is forced to discover features that are unrelated to color2.\n\n\nsub <- function(x) {\n  select(x, starts_with(\"feature\"))\n}\n\ncluster_result <- kmeans(sub(h), centers = 25, nstart = 20)\ncentroids <- tidy(cluster_result)\nD <- pdist(sub(centroids), sub(h)) %>%\n  as.matrix()\n\npar(mfrow = c(5, 4), mai = rep(0.00, 4))\nnear_centroid <- order(D[3, ])[1:20]\nout <- images[near_centroid,,, ] %>%\n  array_tree(1) %>%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\n\nFigure 3: The 20 images closest to the centroid of cluster 3 in the feature activation space. This cluster seems to include images with many orange pixels.\n\n\n\nIn general, we can treat the activations generated by a deep learning model as themselves the object of data analysis. This can help us determine whether the kinds of features that we want it to (high-level concepts, rather than just colors or textures). It can also highlight instances where the model learns features associated with concepts that we would rather it be invariant to (e.g., changes in season, variations in lighting).\n\nSometimes, it’s just looking at the color of the floor!↩︎\nIn the deep learning community, this would be called using color augmentation to enforce invariance.↩︎\n",
    "preview": "posts/2021-04-12-week13-3/week13-3_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-04-16T20:35:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-12-week13-2/",
    "title": "Visualizing Learned Features",
    "description": "A first look at activations in a deep learning model.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"keras\")\nlibrary(\"purrr\")\nlibrary(\"RColorBrewer\")\n\n\n\nIn the last lecture, we discussed the conceptual foundations of feature learning. In this lecture, we’ll see how to extract and visualize features learned by a computer vision model.\nWe will inspect a model that was trained1 to distinguish between photos of cats and dogs. We’ve included a subsample of the training dataset below – the full dataset can be downloaded here. From the printout, you can see that we have saved 20 images, each of size \\(150 \\times 150\\) pixels, and with three color channels (red, green, and blue).\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/o7t3nt77iv3twizyv7yuwqnca16f9nwi.rda\", f)\nimages <- get(load(f))\ndim(images) # 20 sample images\n\n\n[1]  20 150 150   3\n\nLet’s take a look at a few examples from the training dataset. We’ve randomly sampled 10 dogs and 10 cats. The command par allows us to plot many images side by side (in this case, in a \\(4 \\times 5\\) grid).\n\n\npar(mfrow = c(4, 5), mai = rep(0.00, 4))\nout <- images %>%\n  array_tree(1) %>%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\n\nFigure 1: A sample of 20 random images from the dog vs. cat training dataset.\n\n\n\nThe array_tree function above splits the 4D array into a collection of 3D slices. Each of these 3D slices corresponds to one image — the three channels correspond to red, green, and blue colors, respectively. The next map line plots each of the resulting 3D arrays\nNext, let’s consider what types of features the model has learned, in order to distinguish between cats and dogs. Our approach will be to compute activations on a few images and visualize them as 2D feature maps. These visualizations will help us see whether there are systematic patterns in what leads to an activation for a particular neuron.\nTo accomplish this, we will create an R object to retrieve all the intermediate feature activations associated with an input image. Every time we call this object on a new image, it will return the activations for features at all layers.\n\n\n# download model\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel <- load_model_hdf5(f)\n\nlayer_outputs <- map(model$layers, ~ .$output)\nactivation_model <- keras_model(inputs = model$input, outputs = layer_outputs)\nfeatures <- predict(activation_model, images)\n\n\n\nEach element of features corresponds to a different layer. Within a single layer, the 3D array provides the activations of each feature across different spatial windows. For example, for the first layer, there are 32 features with activations spread across a 148 x 148 grid, each grid element with its own spatial context.\n\n\ndim(features[[1]])\n\n\n[1]  20 148 148  32\n\nThe block below visualizes the first feature map in the first layer. We plot the associated input image next to it. This feature seems to be a horizontal edge detector – it activates whenever there are transitions from dark to light areas when moving vertically. For example, when the white leash goes over the shadow in the background, this feature has some of its highest activations.\n\n\nplot_feature <- function(feature) {\n  rotate <- function(x) t(apply(x, 2, rev))\n  image(rotate(feature), axes = FALSE, asp = 1, col = brewer.pal(4, \"Blues\"))\n}\n\nix <- 3\npar(mfrow = c(1, 2), mai = rep(0.00, 4))\nplot(as.raster(images[ix,,, ], max = 255))\nplot_feature(features[[1]][ix,,, 1])\n\n\n\n\nFigure 2: An image and its activations for the first neuron in layer 1.\n\n\n\nLet’s visualize a few more of these features. We see more vertical and horizontal edge detectors — features with high values at sharp changes in color in the underlying images. This is consistent with our earlier claim that the first layer of a network learns to recognize pixel-level interactions.\n\n\npar(mfrow = c(6, 7), mai = rep(0.00, 4))\nout <- features[[2]][ix,,,] %>%\n  array_branch(margin = 3) %>%\n  map(~ plot_feature(.))\n\n\n\n\nFigure 3: Activations for a collection of neurons at layer 2, for the same image as given above.\n\n\n\nNext, we visualize features at a higher level in the network. At this point, each activation corresponds to a larger spatial context in the original image, so there are fewer activations per feature. There are more feature maps total, but each is smaller. It’s not so clear what these feature maps correspond to, but there do seem to be a few that are clearly activated within the dog, and others that are sensitive to the foliage in the background.\n\n\npar(mfrow = c(6, 7), mai = rep(0.00, 4))\nout <- features[[6]][ix,,,1:40] %>%\n  array_branch(margin = 3) %>%\n  map(~ plot_feature(.))\n\n\n\n\nFigure 4: Activations for a collection of neurons at layer 6.\n\n\n\nWhile we had some interpretations for these higher-level features, it’s hard to know definitively, since we are only considering a single image. In the next set of notes, we will examine the same neuron across many dataset examples, and this will give us more confidence in our interpretations of individual neurons.\n\nFor those who are curious, the training code is here). Of course, you will not be responsible for understanding this step.↩︎\n",
    "preview": "posts/2021-04-12-week13-2/week13-2_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-04-16T20:35:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-12-week13-1/",
    "title": "Introduction to Feature Learning",
    "description": "An introduction to compositional feature learning.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-19",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"keras\")\n\n\n\nIn classical machine learning, we assume that the features most relevant to prediction are already available. E.g., when we want to predict home price, we already have features about square feet and neighborhood income, which are clearly relevant to the prediction task.\nIn many modern problems though, we have only access to data where the most relevant features have not been directly encoded.\nIn image classification, we only have raw pixel values. We may want to predict whether a pedestrian is in an image taken from a self-driving car, but we have only the pixels to work with. It would be useful to have an algorithm come up with labeled boxes like those in the examples below.\nFor sentiment analysis, we want to identify whether a piece of text is a positive or negative sentiment. However, we only have access to the raw sequence of words, without any other context. Examples from the IMDB dataset are shown below.\nIn both of these examples, this information could be encoded manually, but it would a substantial of effort, and the manual approach could not be used in applications that are generating data constantly. In a way, the goal of these algorithms is to distill the raw data down into a succinct set of descriptors that can be used for more classical machine learning or decision making.\n\n\n\nFigure 1: An example of the types of labels that would be useful to have, starting from just the raw image.\n\n\n\nExample reviews from the IMDB dataset:\n  positive,\"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"\"has got all the polari ....\"\n  positive,\"I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be ...\"\n  negative,\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to ...\"\n  positive,\"Petter Mattei's \"\"Love in the Time of Money\"\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a ...\"\nIn these problems, the relevant features only arise as complex interactions between the raw data elements.\nTo recognize a pedestrian, we need many adjacent pixels to have a particular configuration of values, leading to combinations of edges and shapes, which when viewed together, become recognizable as a person.\nTo recognize a positive or negative sentiment, we need to recognize interactions between words. “The movie was good until” clearly has bad sentiment, but you cannot tell that from the isolated word counts alone.\n\nThe main idea of deep learning is to learn these more complex features one layer at a time. For image data, the first layer recognizes interactions between individual pixels. Specifically, individual features are designed to “activate” when particular pixel interactions are present. The second layer learns to recognize interactions between features in the first layer, and so on, until the learned features correspond to more “high-level” concepts, like sidewalk or pedestrian.\nBelow is a toy example of how an image is processed into feature activations along a sequence of layers. Each pixel within the feature maps correspond to a patch of pixels in the original image – those later in the network have a larger field of view than those early on. A pixel in a feature map has a large value if any of the image features that it is sensitive to are present within its field of vision.\n\n\n\nFigure 2: A toy diagram of feature maps from the model loaded below. Early layers have fewer, but larger feature maps, while later layers have many, but small ones. The later layers typically contain higher-level concepts used in the final predictions.\n\n\n\nAt the end of the feature extraction process, all the features are passed into a final linear or logistic regression module that completes the regression or classification task, respectively.\nIt is common to refer to each feature map as a neuron. Different neurons activate when different patterns are present in the original, underlying image.\n\n\n\nFigure 3: An illustration of the different spatial contexts of feature maps at different layers. An element of a feature map has a large value (orange in the picture) if the feature that it is sensitive to is present in its spatial context. Higher-level feature maps are smaller, but each pixel within it has a larger spatial context.\n\n\n\nBelow, we load a model to illustrate the concept of multilayer networks. This model has 11 layers followed by a final logistic regression layer. There are many types of layers. Each type of layer contributes in a different way to the feature learning goal, and learning how design and compose these different types of layers is one of the central concerns of deep learning. The Output Shape column describes the number and shape of feature maps associated with each layer. For example, the first layer has 32 feature maps, each of size \\(148 \\times 148\\). Deeper parts of the network have more layers, but each is smaller. We will see how to load and inspect these features in the next lecture.\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel <- load_model_hdf5(f)\nmodel\n\n\nModel\nModel: \"sequential_1\"\n______________________________________________________________________\nLayer (type)                   Output Shape                Param #    \n======================================================================\nconv2d_7 (Conv2D)              (None, 148, 148, 32)        896        \n______________________________________________________________________\nmax_pooling2d_7 (MaxPooling2D) (None, 74, 74, 32)          0          \n______________________________________________________________________\nconv2d_6 (Conv2D)              (None, 72, 72, 64)          18496      \n______________________________________________________________________\nmax_pooling2d_6 (MaxPooling2D) (None, 36, 36, 64)          0          \n______________________________________________________________________\nconv2d_5 (Conv2D)              (None, 34, 34, 128)         73856      \n______________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D) (None, 17, 17, 128)         0          \n______________________________________________________________________\nconv2d_4 (Conv2D)              (None, 15, 15, 128)         147584     \n______________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D) (None, 7, 7, 128)           0          \n______________________________________________________________________\nflatten_1 (Flatten)            (None, 6272)                0          \n______________________________________________________________________\ndropout (Dropout)              (None, 6272)                0          \n______________________________________________________________________\ndense_3 (Dense)                (None, 512)                 3211776    \n______________________________________________________________________\ndense_2 (Dense)                (None, 1)                   513        \n======================================================================\nTotal params: 3,453,121\nTrainable params: 3,453,121\nNon-trainable params: 0\n______________________________________________________________________\n\nWhile we will only consider image data in this course, the idea of learning complex features by composing a few types of layers is a general one. For example, in sentiment analysis, the first layer learns features that activate when specific combinations of words are present in close proximity to one another. The next layer learns interactions between phrases, and later layers are responsive to more sophisticated grammar.\nDeep learning is often called a black box because these intermediate features are often complex and not directly interpretable according to human concepts. The problem is further complicated by the fact that features are “distributed” in the sense that a single human concept can be encoded by a configuration of multiple features. Conversely, the same model feature can encode several human concepts.\nFor this reason, a literature has grown around the question of interpreting neural networks. The field relies on visualization and interaction to attempt to understand the learned representations, with the goal of increasing the safety and scientific usability of deep learning models. While our class will not discuss how to design or develop deep learning models, we will get a taste of the interpretability literature in the next few lectures.\n\n\n\n",
    "preview": "https://uwmadison.box.com/shared/static/dlglc32rea1ovqrwgj9in6yxje5ml036.png",
    "last_modified": "2021-04-16T22:00:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05-week12-5/",
    "title": "Pointwise Diagnostics",
    "description": "Evaluating the fit at particular observations in Bayesian models.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-16",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"loo\")\nlibrary(\"purrr\")\nlibrary(\"rstan\")\nlibrary(\"tidyr\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\n\n\n\nAll the model visualization strategies we’ve looked at in the last few lectures have been dataset-wide. That is, we looked at properties of the dataset as a whole, and whether the model made sense globally, across the whole dataset. Individual observations might warrant special attention, though.\nThe block below loads in the fitted models from the previous set of notes.\n\n\ndownloader <- function(link) {\n  f <- tempfile()\n  download.file(link, f)\n  get(load(f))\n}\n\nmodels <- downloader(\"https://uwmadison.box.com/shared/static/x7dotair443mhx34yzie3m3lrsvhk19a.rda\")\nGM <- downloader(\"https://uwmadison.box.com/shared/static/2pzgdu7gyobhl5tezo63tns7by1aiy6d.rda\")\n\n\n\nA first diagnostic to consider is the leave-one-out predictive distribution. This is the probability \\(p\\left(y_{i} \\vert y_{-i}\\right)\\) of sample \\(i\\) after having fitted a model to all samples except \\(i\\). Ideally, most observations in the dataset to have high predictive probability.\nNote that this can be used for model comparison. Some models might have better per-sample leave-one-out predictive probabilities for almost all observations.\nThis is similar to a leave-one-out residual.\n\nIf we use rstan to fit a Bayesian model, then these leave-one-out probabilities can be estimated using the loo function in the loo package. The code below computes these probabilities for each model, storing the difference in predictive probabilities for models two and three in the diff23 variable.\n\n\nelpd <- map(models, ~ loo(., save_psis = TRUE)$pointwise[, \"elpd_loo\"])\nelpd_diffs <- GM@data %>%\n  mutate(\n    ID = row_number(),\n    diff23 = elpd[[3]] - elpd[[2]]\n  )\n\noutliers <- elpd_diffs %>%\n  filter(abs(diff23) > 6)\n\n\n\nWe plot the difference between these predictive probabilities below. The interpretation is that Ulaanbataar has much higher leave-one-out probability under the cluster-based model, perhaps because that model is able to group the countries with large deserts together with one another. On the other hand, Santo Domingo is better modeled by model 2, since it has higher leave-one-out probability in that model.\n\n\nggplot(elpd_diffs, aes(ID, diff23)) +\n  geom_point(\n    aes(col = super_region_name),\n    size = 0.9, alpha = 0.8\n    ) +\n  geom_text_repel(\n    data = outliers,\n    aes(label = City_locality),\n    size = 3 \n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    y = \"Influence (Model 2 vs. 3)\",\n    col = \"WHO Region\"\n  )\n\n\n\n\nFigure 1: The difference in leave one out predictive probabilities for each sample, according to the WHO-region and cluster based hierarchical models.\n\n\n\nAnother diagnostic is to consider the influence of an observation. Formally, the influence is a measure of how much the posterior predictive distribution changes when we leave one sample out. The idea is to measure the difference between the posterior predictives using a form of KL divergence, and note down the observations that lead to a very large difference in divergence.\n\n\n\nFigure 2: Visual intuition about the influence of observations. If the posterior predictive distributions shift substantially when an observation is included or removed, then it is an influential observation.\n\n\n\nWhen using rstan, the influence measure can be computed by the psis function. The pareto_k diagnostic summarizes how much the posterior predictive shifts when an observation is or isn’t included. For example, in the figure below, observation 2674 (Ulaanbaatar again) is highly influential.\n\n\nloglik <- map(models, ~ as.matrix(., pars = \"log_lik\"))\nkdata <- GM@data %>%\n  mutate(\n    k_hat = psis(loglik[[2]])$diagnostics$pareto_k,\n    Index = row_number()\n  )\noutliers <- kdata %>%\n  filter(k_hat > 0.25)\n\nggplot(kdata, aes(x = Index, y = k_hat)) + \n  geom_point(aes(col = super_region_name), size = 0.5, alpha = 0.9) + \n  scale_color_brewer(palette = \"Set2\") +\n  geom_text_repel(data = outliers, aes(label = Index)) +\n  labs(y = \"k-hat\")\n\n\n\n\nFigure 3: The influence of each sample on the final posterior distribution.\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-05-week12-5/week12-5_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-04-09T15:01:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05-week12-4/",
    "title": "Prior and Posterior Predictives",
    "description": "Simulating data to evaluate model quality.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-15",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"purrr\")\nlibrary(\"rstan\")\nlibrary(\"tidyr\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nFrom the previous notes, we see that an exploratory analysis can motivate few plausible models for a dataset. How should we go about choosing between them?\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/2pzgdu7gyobhl5tezo63tns7by1aiy6d.rda\", f)\nGM <- get(load(f))\n\n\n\nIn some of your other classes, you might have seen people use cross-validation / test set error. While this is useful (and relatively automatic), it can be a black box. An alternative which often brings mores insight into the structure of the problem is to use prior and posterior predictive distributions for (visual) model comparison.\nPrior predictive distributions\nThe prior predictive distribution can be used to decide whether certain model families are reasonable candidates for a problem, before formally incorporating the evidence coming from the data.\nThe idea is that if we can write down a generative model, then we can simulate different datasets with it, even before estimating the model parameters. This is often the case in Bayesian models, where we can (a) sample parameters from the prior, and (b) simulate data from the model with those parameters.\nIf the simulated datasets are plausible, then the overall model class is a reasonable one. If they are not, then the model class should be modified. Either the prior or the likelihood might need revision.\nFor example, in the example below, we simulate datasets using both a vague and an informative prior. Vague priors are often recommended because they are « more objective » in some sense. However, in this case, we see that the simulated datasets are not even remotely plausible.\n\n\n# function to simulate from vague prior\nprior1 <- function(Nsim) {\n  tau0 <- 1 / sqrt(rgamma(1, 1, rate = 100))\n  tau1 <- 1 / sqrt(rgamma(1, 1, rate = 100))\n  sigma <- 1 / sqrt(rgamma(1, 1, rate = 100))\n  beta0i <- rnorm(7, 0, tau0)\n  beta1i <- rnorm(7, 0, tau1)\n  beta0 <- rnorm(1, 0, 100)\n  beta1 <- rnorm(1, 0, 100)\n  \n  epsilon <- rnorm(Nsim, 0, sigma)\n  data.frame(\n    log_pm25 = GM$log_pm25,\n    region = GM$super_region_name,\n    sim = beta0 + beta0i[GM$super_region] + (beta1 + beta1i[GM$super_region]) * GM$log_sat + epsilon\n  )\n}\n\n\n\n\n\nprior1_data <- map_dfr(1:12, ~ prior1(Nsim = nrow(GM@data)), .id = \"replicate\")\nggplot(prior1_data, aes(x = log_pm25, y = sim)) + \n  geom_abline(slope = 1) +\n  geom_point(aes(col = region), alpha = 0.1, size = 0.4) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"True station data\", y = \"Simulated station data\") +\n  facet_wrap(~ replicate, scale = \"free_y\")\n\n\n\n\nFigure 1: Prior predictive samples from the vague prior are on a completely implausible scale.\n\n\n\nThe block below instead simulates from a subjective, informative prior. The resulting samples are much more plausible, lying in a comparable range to the true data. However, note, the samples from the prior predictive do not need to look exactly like the observed data — if they did, there would be no need to fit model parameters! Instead, they should look like plausible datasets that might have been observed.\n\n\n# function to simulate from informative prior\nprior2 <- function(Nsim) {\n  tau0 <- abs(rnorm(1, 0, 1))\n  tau1 <- abs(rnorm(1, 0, 1))\n  sigma <- abs(rnorm(1, 0, 1))\n  beta0i <- rnorm(7, 0, tau0)\n  beta1i <- rnorm(7, 0, tau1)\n  beta0 <- rnorm(1, 0, 1)\n  beta1 <- rnorm(1, 1, 1)\n  \n  epsilon <- rnorm(Nsim, 0, sigma)\n  data.frame(\n    log_pm25 = GM$log_pm25,\n    region = GM$super_region_name,\n    sim = beta0 + beta0i[GM$super_region] + (beta1 + beta1i[GM$super_region]) * GM$log_sat + epsilon\n  )\n}\n\nprior2_data <- map_dfr(1:12, ~ prior2(Nsim = nrow(GM@data)), .id = \"replicate\")\nggplot(prior2_data, aes(x = log_pm25, y = sim)) + \n  geom_abline(slope = 1) +\n  geom_point(aes(col = region), alpha = 0.1, size = 0.4) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"True station data\", y = \"Simulated station data\") +\n  facet_wrap(~ replicate, scale = \"free_y\")\n\n\n\n\nFigure 2: Prior predictive samples from the weakly informative prior seem more plausible, though they do not (and should not) exactly fit the true data.\n\n\n\nPhilosophically, this prior predictive analysis is based on the idea that, though probability is subjective, evidence can be used to update our beliefs. The idea of the prior predictive is to visually encode subjective beliefs about the problem under study before gathering new evidence.\nPosterior predictive distributions\nOnce the prior predictive is calibrated, we can fit the model. To evaluate it’s quality, we can use the posterior predictive.\nThe posterior predictive is just like the prior predictive, except that it samples model parameters from the data-informed posterior, rather than the data-ignorant prior.\nFormally, it is the distribution of new datasets when drawing parameters from the posterior. The simulation mechanism is (a) draw model parameters from the posterior and (b) simulate a dataset using parameters from (a).\nThe code below fits the three models. We are using the rstan package to fit three Bayesian models. The first model lm is a Bayesian linear regression, assuming the same slope across all regions. The two other models assume different slopes for different regions1, but use the WHO and cluster-based region definitions, respectively. You do not need to worry about how the rstan code, which is sourced into the stan_model function, is written. It is enough to be able to fit these two types of models as if they are built-in function in R.\n\n\n# Define the input datasets for the lm, region-based, and cluster-based models\ndatasets <- list(\n  \"lm\" = with(GM@data, list(N = length(log_pm25), y = log_pm25, x = log_sat)),\n  \"regions\" = with(\n    GM@data, \n    list(N = length(log_pm25), y = log_pm25, x = log_sat, group = super_region, R = n_distinct(super_region))\n  ),\n  \"clusters\" = with(\n    GM@data, \n    list(N = length(log_pm25), y = log_pm25, x = log_sat, group = as.integer(cluster_region), R = n_distinct(cluster_region))\n  )\n)\n\n# Define the two types of Bayesian models\nmodel_def <- list(\n  \"lm\" = stan_model(\"https://uwmadison.box.com/shared/static/hoq1whain301bj6gj670itxabnnhvcy7.stan\"),\n  \"hier\" = stan_model(\"https://uwmadison.box.com/shared/static/lvouz9jj4rbkmrx5osj2dtrhj2ycdll8.stan\")\n)\n\n\n\n\n\n# Fit the models above to the three datasets of interest\ncontrols <- list(max_treedepth = 15, adapt_delta = 0.99)\nmodels <- list(\n  \"lm\" = sampling(model_def$lm, data = datasets$lm, chains = 1, control = controls),\n  \"regions\" = sampling(model_def$hier, data = datasets$regions, chains = 1, control = controls),\n  \"clusters\" = sampling(model_def$hier, data = datasets$clusters, chains = 1, control = controls)\n)\n\n\n\nThe code above takes a little while to run (about 10 minutes for the last two models). To save some time, you can download the fitted models from the link below. The models object is a list whose elements are fitted STAN models for the three model definitions above. The fitted model objects include posterior samples for the region slopes as well as simulated ground station PM2.5 data, based on those posterior slopes.\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/x7dotair443mhx34yzie3m3lrsvhk19a.rda\", f)\nmodels <- get(load(f))\n\n\n\nThe block below simulates station PM2.5 data from the fitted posterior of the cluster-based model. Note that, compared to the prior predictive, the posterior predictive is much more closely related to the true underlying dataset.\n\n\n# extract 12 samples and reshape it to \"long\" format\nposterior_samples <- as.matrix(models$clusters, pars = \"y_sim\")[950:961, ] %>%\n  t() %>%\n  as_tibble() %>%\n  bind_cols(GM@data) %>%\n  pivot_longer(V1:V12, names_to = \"replicate\", values_to = \"log_pm25_sim\")\n\nggplot(posterior_samples, aes(log_pm25, log_pm25_sim)) +\n  geom_abline(slope = 1) +\n  geom_point(aes(col = cluster_region), size = 0.4, alpha = 0.1) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"True station data\", y = \"Simulated station data\") +\n  facet_wrap(~ replicate, scale = \"free_y\")\n\n\n\n\nFigure 3: Samples from the posterior predictive in the cluster-based model.\n\n\n\nWe can verify that features of the real dataset are accurately captured by features of the posterior predictive. One subtlety is that there is a danger of overfitting features in the posterior predictive. It is best to choose features of the data that are not directly modeled (e.g., if you use slope in the model estimation, then don’t evaluate the posterior predictive using the slope, since by definition this will be well-captured). In the block below, we compute the skewness for each simulated station dataset from the three different models. These skewnesses are plotted as histograms, with the true dataset’s skewness indicated by a vertical line. It seems that the model that uses clustering to define regions is able to simulate datasets with skewness similar to that in the real dataset.\n\n\napply_stat <- function(x, f) {\n  z <- as.matrix(x, pars = \"y_sim\")\n  tibble(\n    \"replicate\" = seq_len(nrow(z)),\n    \"statistic\" = apply(z, 1, f)\n  )\n}\n\nskew <- function(x) {\n  xdev <- x - mean(x)\n  n <- length(x)\n  r <- sum(xdev^3) / sum(xdev^2)^1.5\n  r * sqrt(n) * (1 - 1/n)^1.5\n}\n\nposteriors <- map_dfr(models, ~ apply_stat(., skew), .id = \"model\")\ntruth <- skew(GM@data$log_pm25)\nggplot(posteriors, aes(statistic)) +\n  geom_histogram(aes(fill = model), binwidth = 0.01) +\n  geom_vline(xintercept = truth, col = \"red\") +\n  scale_fill_brewer(palette = \"Set3\")\n\n\n\n\nFigure 4: Posterior simulated skewness according to the three different models.\n\n\n\n\nBayesian regression models that allow different slopes for different groups are called hierarchical models.↩︎\n",
    "preview": "posts/2021-04-05-week12-4/week12-4_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-04-09T15:01:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05-week12-3/",
    "title": "Visualization for Model Building",
    "description": "The relationship between exploratory analysis and model development.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"rstan\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidyr\")\nlibrary(\"purrr\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nExploratory data analysis and model building complement each other well. In practical problems, visualization can guide us towards more plausible models.\nWe rarely know the exact form of a model in advance, but usually have a few reasonable candidates. Exploratory analysis can rule out some candidates and suggest new, previously unanticipated, relationships.\nWe will illustrate these ideas using an example. A researcher is interested in monitoring the level of PM2.5, a type of small air particlute that can be bad for public health. High quality data are available from weather stations scattered around the world, but their data only apply locally. On the other hand, low quality data, available from satellites, are available everywhere. A model is desired that uses the weather station measurements to calibrate the satellite data. If it works well, it could be used to monitor PM2.5 levels at global scale.\n\n\nf <- tempfile()\ndownload.file(\"https://github.com/jgabry/bayes-vis-paper/blob/master/bayes-vis.RData?raw=true\", f)\nGM <- get(load(f))\nGM@data <- GM@data %>% \n  mutate(\n    log_pm25 = log(pm25), \n    log_sat = log(sat_2014)\n  )\n\n\n\nThe simplest model simply fits \\(\\text{station} = a + b \\times \\text{satellite}\\) at locations where they are both available. This model was used in practice by the Global Burden of Disease project until 2016.\n\n\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = super_region_name), size = 0.8, alpha = 0.7) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"WHO Region\"\n  ) +\n  coord_fixed()\n\n\n\n\nFigure 1: The relationship between satellite and ground station estimates of PM2.5.\n\n\n\nHowever, when we plot these two variables against one another, we notice that there is still quite a bit of heterogeneity. The residuals are large — what features might be correlated with these residuals, which if included, would improve the model fit?\nThe error \\(\\epsilon_{i}\\) in a model \\(y_i = f\\left(x_{i}\\right) +  \\epsilon_{i}\\) represents out our ignorance of the myriad of unmeasured factors that determine the relationship between \\(x\\) and \\(y\\).\nFor example, desert sand is known to increase PM2.5, but it is not visible from space. The residuals are probably correlated with whether the model is in a desert area (we underpredict PM2.5 in deserts), and so would be improved if we included a term with this feature.\n\nOne hypothesis is that country region is an important factor. Below, we fit regression lines separately for different country super-regions, as specified by the WHO. The fact that the slopes are not the same in each region means that we should modify our model to have a different slope in each region1.\n\n\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = super_region_name), size = 0.4, alpha = 0.7) +\n  geom_smooth(aes(col = super_region_name), method = \"lm\", se = F, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"WHO Region\"\n  ) +\n  coord_fixed()\n\n\n\n\nFigure 2: The relationship between these variables is not the same across regions.\n\n\n\nThe WHO categorizations are somewhat arbitrary. Maybe there are better country groupings, tailored specifically to the PM2.5 problem? One idea is to cluster the ground stations based on PM2.5 level and use these clusters as a different region grouping.\n\n\naverage <- GM@data %>% \n  group_by(iso3) %>% \n  summarise(pm25 = mean(pm25))\n\nclust <- dist(average) %>%\n  hclust() %>%\n  cutree(k = 6)\n\nGM@data$cluster_region <- map_chr(GM@data$iso3, ~ clust[which(average$iso3 == .)])\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = cluster_region), size = 0.4, alpha = 0.7) +\n  geom_smooth(aes(col = cluster_region), method = \"lm\", se = F, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"Cluster Region\"\n  ) +\n  coord_fixed()\n\n\n\n\nFigure 3: We can define clusters of regions on our own, using a hierarchical clustering.\n\n\n\nWe now have a « network » of models. We’re going to want more refined tools for distinguishing between them. This is the subject of the next two lectures.\n\nViewed differently, this is like adding an interaction between the satellite measurements and WHO region.↩︎\n",
    "preview": "posts/2021-04-05-week12-3/week12-3_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-09T14:59:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05-week12-2/",
    "title": "Partial Dependence Profiles II",
    "description": "Discovering richer structure in partial dependence profiles.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"caret\")\nlibrary(\"DALEX\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nPartial dependence (PD) plots help answer the question, « How is feature \\(j\\) used by my model \\(f\\)? » Slight variations on PD plots are useful for some related followup questions,\nHas my model \\(f\\) learned interactions between features \\(j\\) and \\(j^{\\prime}\\)?\nHow do the models \\(f\\) and \\(f^{\\prime}\\) differ in the way that they use feature \\(j\\)?\n\nThe variants, called Grouped and Contrastive PD plots, reduce the original CP profiles less aggressively than PD plots, but without becoming overwhelmingly complicated.\nInteractions\nWe say that there is an interaction between variables \\(j\\) and \\(j^{\\prime}\\) if the relationship between \\(x_{j}\\) and \\(y\\) is modulated by variable \\(j^{\\prime}\\). For example, in the figure below, the slope of cross-sections across \\(j\\) depends on \\(j^{\\prime}\\).\nUsing the language of CP profiles, the figure above means that the shape of the CP profile in \\(j\\) depends on the particular setting of \\(j^{\\prime}\\). This motivates the use of Grouped PD profiles — we compute several PD profiles in \\(j\\), restricting attention to CP profiles whose value \\(x_{j^{\\prime}}\\) lies within a prespecified range.\nTo illustrate, we revisit the CP profiles for age from the Titanic dataset. Below, the profiles are grouped according to the class of the ticket holder. The result shows that the relationship between age and survival was not the same across all passengers. For all classes, there was a decrease in survival probability for adults, but the dropoff was most severe for crew members.\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/nau695mppsoxx0f6bns1ieo7kh1bje0j.rda\", f)\nfit <- get(load(f))\n\ndata(titanic)\ntitanic <- titanic %>%\n  select(-country) %>%\n  na.omit()\nx <- select(titanic, -survived)\n\nexplanation <- explain(model = fit, data = x, y = titanic$survived)\n\n\nPreparation of a new explainer is initiated\n  -> model label       :  train  ( [33m default [39m )\n  -> data              :  2179  rows  7  cols \n  -> target variable   :  2179  values \n  -> predict function  :  yhat.train  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package caret , ver. 6.0.86 , task classification ( [33m default [39m ) \n  -> model_info        :  Model info detected classification task but 'y' is a factor .  ( [31m WARNING [39m )\n  -> model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -> model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -> model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -> predicted values  :  numerical, min =  0.007271972 , mean =  0.3239071 , max =  0.9885397  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n [32m A new explainer has been created! [39m \n\nprofiles <- model_profile(explainer = explanation, groups = \"class\")\nplot(profiles, geom = \"profiles\", variables = \"age\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\n\nFigure 1: Grouping the CP profiles by ticket class reveals an interaction effect with age in the Titanic dataset.\n\n\n\nWhat should we do if there are many input variables and we don’t have a priori knowledge about which variables \\(j^{\\prime}\\) might be interacting with \\(j\\)? One idea is to try to discover relevant interactions by clustering the original set of CP profiles.\nIn more detail, we can compute the CP profiles for all the samples, and then see whether there are subsets of profiles that all look similar. If we find features \\(j^{\\prime}\\) that characterize these groupings, then we have found features that interact with \\(j\\) (with respect to the fitted model \\(f\\)). The plot below shows the same profiles as above, but clustering directly. It seems to recover the interaction between age and class, even though we have not explicitly provided this grouping variable.\n\n\nprofiles <- model_profile(explainer = explanation, variables = \"age\", k = 3)\nplot(profiles, geom = \"profiles\", variables = \"age\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\n\nFigure 2: Discovered groupings in the CP profiles for age reveals an interaction effect.\n\n\n\nModel Comparison\nThe comparison of different models’ PD profiles can be used to,\nValidate a simple model\nGuide the design of new features, and\nCharacterizing overfitting PD profiles that are used to compare different models are sometimes called « Contrastive » PD profiles.\n\nTo validate a simple model, we can compare its PD profiles with those of a more sophisticated model. We will illustrate this by fitting linear and random forest models to a dataset of apartment prices1. Given various properties of an apartment, the goal is to determine its price. The code below fits the two models and extracts their CP and PD profiles.\n\n\ndata(apartments)\nx <- select(apartments, -m2.price)\nprofiles_lm <- train(x, apartments$m2.price, method = \"lm\") %>%\n  explain(x, apartments$m2.price, label = \"LM\") %>%\n  model_profile()\n\n\nPreparation of a new explainer is initiated\n  -> model label       :  LM \n  -> data              :  1000  rows  5  cols \n  -> target variable   :  1000  values \n  -> predict function  :  yhat.train  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package caret , ver. 6.0.86 , task regression ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  1781.848 , mean =  3487.019 , max =  6176.032  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -247.4728 , mean =  2.093656e-14 , max =  469.0023  \n [32m A new explainer has been created! [39m \n\nprofiles_rf <- train(x, apartments$m2.price, method = \"rf\", tuneGrid = data.frame(mtry = 10)) %>%\n  explain(x, apartments$m2.price, label = \"RF\") %>%\n  model_profile()\n\n\nPreparation of a new explainer is initiated\n  -> model label       :  RF \n  -> data              :  1000  rows  5  cols \n  -> target variable   :  1000  values \n  -> predict function  :  yhat.train  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package caret , ver. 6.0.86 , task regression ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  1658.777 , mean =  3486.158 , max =  6375.731  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -228.5476 , mean =  0.8606697 , max =  268.4641  \n [32m A new explainer has been created! [39m \n\nThe PD profile below shows that the random forest learns linear relationships with price for both the surface and floor variables. If all the effects were like this, then we would have a good reason for preferring the linear model.\n\n\nplot(profiles_lm, profiles_rf, variables = c(\"surface\", \"floor\")) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\n\nFigure 3: A contrastive PD display suggests that the floor and surface features are linearly related with apartment price.\n\n\n\n11.. When making the comparison between a simple and a complex model, certain discrepancies might become apparent. For example, important nonlinearities or interactions might be visible from the PD profiles of the complex model. This information can guide the design of new features in the simpler model, so that it can continue to be used. This is exactly the case in the apartments dataset above – there is a strong nonlinear relationship for the construction year variables. This suggests that, if a linear model is still desired, then a new feature should be defined that identifies whether the apartment was built between 1935 and 1990.\n\n\nplot(profiles_lm, profiles_rf, variables = \"construction.year\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\n\nFigure 4: The random forest learns a nonlinear relationship between construction year and apartment price. This suggests designing new features to include in the linear model.\n\n\n\nSuppose you have found that a model is overfitting (e.g., by finding that it’s training error is much lower than its test error). One way to address this overfitting is to compare the PD profiles between the simple and complex models. If the profiles are very different for one of the features, then that feature may be the source of overfitting.\n\nIt is a simulated dataset, but designed to reflect properties of a real dataset.↩︎\n",
    "preview": "posts/2021-04-05-week12-2/week12-2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-04-13T10:17:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05-week12-1/",
    "title": "Partial Dependence Profiles I",
    "description": "An introduction to partial dependence profiles.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-12",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"caret\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"DALEX\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nAs more complex models become more common in practice, visualization has emerged as a key way for (a) summarizing their essential structure and (b) motivating further modeling refinements.\nIn modern machine learning, it’s common to use a function \\(f\\) to approximate the relationship between a \\(D\\)-dimensional input \\(\\mathbf{x}\\) and a univariate response \\(y\\). We are given a sample of \\(n\\) pairs \\(\\left(\\mathbf{x}_{i}, y_{i}\\right)\\) with which to learn this relationship, and we hope that the function we learn will generalize to future observations.\nSome further notation: We will write \\(x_{j}\\) for the \\(j^{th}\\) coordinate of \\(\\mathbf{x}\\). We will write \\(\\mathbf{x}^{j\\vert = z}\\) to denote the observation \\(\\mathbf{x}\\) with the \\(j^{th}\\) coordinate set to \\(z\\).\n\n\n\nFigure 1: Illustration of the \\(\\mathbf{x}^{j \\vert = z}\\) operation. The \\(j^{th}\\) coordinate (1 in this case) for a selected observation is set equal to \\(z\\).\n\n\n\nLinear models are simple enough that they don’t require any follow-up visual inspection. Since they assume \\(f\\left(\\mathbf{x}\\right) = \\hat{\\beta}^{T}\\mathbf{x}\\), they are completely described by the vector of coefficients \\(\\hat{\\beta}\\). We can exactly describe what happens to \\(f\\) when we increase \\(x_{j}\\) by one unit: we just increase the prediction by \\(\\hat{\\beta}_{j}\\).\nMore complex models — think random forests or neural networks — don’t have this property. While these models often have superior performance, it’s hard to say how changes in particular input features will affect the prediction.\nPartial dependence plots try to address this problem. They provide a description for how changing the \\(j^{th}\\) input feature affects the predictions made by complex models.\nTo motivate the definition, consider the toy example below. The surface is the fitted function \\(f\\left(\\mathbf{x}\\right)\\), mapping a two dimensional input \\(\\mathbf{x}\\) to a real-valued response. How would you summarize the relationship between \\(x_{1}\\) and \\(y\\)? The main problem is that the shape of the relationship depends on which value of \\(x_{2}\\) we start at.\n\n\n\nFigure 2: An example of why it is difficult to summarize the relationship between an input variable and a fitted surface for nonlinear models.\n\n\n\nOne idea is to consider the values of \\(x_{2}\\) that were observed in our dataset. Then, we can evaluate our model over a range of values \\(x_{1}\\) after fixing those values of \\(x_{2}\\). These curves are called Ceteris Paribus profiles1.\nThe same principle holds in higher dimensions. We can fix \\(D - 1\\) coordinates of an observation and then evaluate what happens to a sample’s predictions when we vary coordinate \\(j\\). Mathematically, this is expressed by \\(h_{x}^{f, j}\\left(z\\right) := f\\left(\\mathbf{x}^{j\\vert= z}\\right)\\).\n\n\n\nFigure 3: Visual intuition behind the CP profile. Varying the \\(j^{th}\\) coordinate for an observation traces out a curve in the prediction surface.\n\n\n\nFor example, let’s consider how CP can be used to understand a model fitted to the Titanic dataset. This is a dataset that was used to understand what characteristics survivors of the Titanic disaster had in common. It’s not obvious in advance which characteristics of passengers made them more likely to survive, so a model is fitted to predict survival.\n\n\ndata(titanic)\ntitanic <- select(titanic, -country) %>%\n  na.omit()\n\nx <- select(titanic, -survived)\nhyper <- data.frame(n.trees = 100, interaction.depth = 8, shrinkage = 0.1, n.minobsinnode = 10)\nfit <- train(x = x, y = titanic$survived, method = \"gbm\", tuneGrid = hyper, verbose = F)\n\n\n\nNext, we can compute the CP profiles. We are showing the relationship between age and survival, though any subset of variables could be requested. The bold curve is a Partial Dependence (PD) profile, which we will discuss below. Each of the other curves corresponds to a passenger, though only a subsample is shown. The curves are obtained by fixing all the characteristics of the passanger except for age, and then seeing what happens to the prediction when the age variable is increased or decreased.\n\n\nexplanation <- explain(model = fit, data = x, y = titanic$survived)\n\n\nPreparation of a new explainer is initiated\n  -> model label       :  train  ( [33m default [39m )\n  -> data              :  2179  rows  7  cols \n  -> target variable   :  2179  values \n  -> predict function  :  yhat.train  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package caret , ver. 6.0.86 , task classification ( [33m default [39m ) \n  -> model_info        :  Model info detected classification task but 'y' is a factor .  ( [31m WARNING [39m )\n  -> model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -> model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -> model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -> predicted values  :  numerical, min =  0.01116753 , mean =  0.3242713 , max =  0.9890543  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n [32m A new explainer has been created! [39m \n\nprofile <- model_profile(explainer = explanation)\nplot(profile, geom = \"profiles\", variables = \"age\") +\n  theme479\n\n\n\n\nFigure 4: CP and PDP profiles for age, for a GBM fitted to the Titanic dataset.\n\n\n\nIt seems that children had the highest probability2 of survival. The relationship is far from linear, with those between 40 and 60 all having about the same probabilities. Notice that the profiles are vertically offset from one passenger to another. This is because, aside from age, each passenger had characteristics that made them more or less likely to survive.\nWe used the DALEX package to produce these curves. The explain function takes the fitted model and original dataset as input. It returns an object with many kinds of model summaries. To extract the CP profiles from these summaries, we use model_profile. The output of this function has been designed so that calling plot with geom = \"profiles\" will show the CP profiles.\nThe PD profile is computed by averaging across all the CP profiles. It is a more concise alternative to CP profiles, showing one curve per features, rather than one curve per sample.\n\n\nplot(profile, geom = \"aggregates\") +\n  theme479\n\n\n\n\nNot only are the PD plots simpler to read than the full collection of CP profiles — by performing this aggregation step, subtle patterns may become more salient, for the same reason that an average carries more information than any subset of observations.\n\nCeteris Paribus means « All Else Held Equal. »↩︎\nTechnically, these are all predicted probabilities from the model.↩︎\n",
    "preview": "https://uwmadison.box.com/shared/static/mpe45nor6xm4gt1idhedayw9ik754c2k.png",
    "last_modified": "2021-04-13T10:16:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-30-week11-4/",
    "title": "Topic Modeling Case Study",
    "description": "An application to a gene expression dataset.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-08",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"forcats\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"superheat\")\nlibrary(\"tibble\")\nlibrary(\"tidyr\")\nlibrary(\"tidytext\")\nlibrary(\"topicmodels\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nWe have used text data analysis to motivate and illustrate the use of topic models. However, these models can be used whenever we have high-dimensional count data1. To illustrate this broad applicability, this lecture will consider an example from gene expression analysis.\nThe dataset we consider comes from the GTEX consortium. A variety of tissue samples have been subject to RNA-seq analysis, which measures how much of each type of gene is expressed within each sample. Intuitively, we relate,\nDocuments → Tissue samples\nWords → Genes\nWord Counts → Gene expression levels\n\n\n\nx <- read_csv(\"https://uwmadison.box.com/shared/static/fd437om519i5mrnur14xy6dq3ls0yqt2.csv\")\nx\n\n\n# A tibble: 2,000,000 x 6\n   sample         gene      tissue tissue_detail     Description value\n   <chr>          <chr>     <chr>  <chr>             <chr>       <dbl>\n 1 GTEX-NFK9-092… ENSG0000… Heart  Heart - Left Ven… FGR           368\n 2 GTEX-OXRO-001… ENSG0000… Brain  Brain - Frontal … FGR           593\n 3 GTEX-QLQ7-052… ENSG0000… Heart  Heart - Left Ven… FGR           773\n 4 GTEX-POMQ-032… ENSG0000… Heart  Heart - Left Ven… FGR           330\n 5 GTEX-QESD-052… ENSG0000… Heart  Heart - Left Ven… FGR           357\n 6 GTEX-OHPN-001… ENSG0000… Brain  Brain - Amygdala  FGR           571\n 7 GTEX-OHPK-032… ENSG0000… Heart  Heart - Left Ven… FGR           391\n 8 GTEX-OIZG-112… ENSG0000… Heart  Heart - Left Ven… FGR           425\n 9 GTEX-O5YW-032… ENSG0000… Heart  Heart - Left Ven… FGR           172\n10 GTEX-REY6-102… ENSG0000… Heart  Heart - Left Ven… FGR           875\n# … with 1,999,990 more rows\n\nThe goal here is to find sets of genes that tend to be expressed together, because these co-expression patterns might be indications of shared biological processes. Unlike clustering, which assumes that each sample is described by one gene expression profile, a topic model will be able to model each tissue sample as a mixture of profiles (i.e., a mixture of underlying biological processes).\nAs a first step in our analysis, we need to prepare a DocumentTermMatrix for use by the topicmodels package. Since the data were in tidy format, we can use the cast_dtm function to spreaed genes across columns. From there, we can fit an LDA model. However, we’ve commented out the code (it takes a while to run) and instead just download the results that we’ve hosted on Box.\n\n\nx_dtm <- cast_dtm(x, sample, gene, value)\n#fit <- LDA(x_dtm, k = 10, control = list(seed = 479))\n#save(fit, file = \"lda_gtex.rda\")\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/ifgo6fbvm8bdlshzegb5ty8xif5istn8.rda\", f)\nfit <- get(load(f))\n\n\n\nLet’s extract the tidy topic and memberships data. For the memberships, we will also join in the tissue from which each biological sample belonged.\n\n\ntissue_info <- x %>%\n  select(sample, starts_with(\"tissue\")) %>%\n  unique()\n\ntopics <- tidy(fit, matrix = \"beta\") %>%\n  mutate(topic = factor(topic))\nmemberships <- tidy(fit, matrix = \"gamma\") %>%\n  mutate(topic = factor(topic)) %>%\n  left_join(tissue_info, by = c(\"document\" = \"sample\"))\n\n\n\nWe can now visualize the topics. Let’s consider the genes with the highest discrimination between topics, using the same discrimination score as in the previous notes. Each row in the heatmap below is a gene, and each column is a topic. The intensity of color represents the gene’s probability within the corresponding topic. Since only discriminative genes are shown, it’s not surprising that most genes are only active within a subset of topics.\n\n\n\n\n\ndiscriminative_genes <- topics %>%\n  group_by(term) %>%\n  mutate(D = discrepancy(beta)) %>%\n  ungroup() %>%\n  slice_max(D, n = 400) %>%\n  mutate(term = fct_reorder(term, -D))\n\ndiscriminative_genes %>%\n  pivot_wider(term, topic, values_from = \"beta\") %>%\n  column_to_rownames(\"term\") %>%\n  superheat(\n    pretty.order.rows = TRUE,\n    left.label.size = 1.5,\n    left.label.text.size = 3,\n    bottom.label.size = 0.05,\n    legend = FALSE\n  )\n\n\n\n\nFigure 1: A heatmap of the most discriminative genes across the 10 estimated topics.\n\n\n\nNow, let’s see what tissues are related to which topics. We can use a structure plot. Before making the plot, we prepare the data appropriately. First, there are some tissues with very few samples, so we will filter those tissues away. Second, we will reorder the samples so that those samples with similar topic profiles are placed next to one another. This is accomplished by running a hierarchical clustering on the topic membership vectors and extracting the order of the resulting dendrogram leaves.\n\n\nkeep_tissues <- memberships %>%\n  count(tissue) %>%\n  filter(n > 70) %>%\n  pull(tissue)\n\nhclust_result <- hclust(dist(fit@gamma))\ndocument_order <- fit@documents[hclust_result$order]\nmemberships <- memberships %>%\n  filter(tissue %in% keep_tissues) %>%\n  mutate(document = factor(document, levels = document_order))\n\n\n\nNext, we can generate the structure plot. The first three lines are the key lines: they create a stacked barchart for each sample and then facet across tissues. The remaining lines simply refine the appearance of the plot.\n\n\nggplot(memberships, aes(gamma, document, fill = topic, col = topic)) +\n  geom_col(position = position_stack()) +\n  facet_grid(tissue ~ ., scales = \"free\", space = \"free\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_color_brewer(palette = \"Set3\", guide = FALSE) +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(x = \"Topic Membership\", y = \"Sample\", fill = \"Topic\") +\n  theme(\n    panel.spacing = unit(0.5, \"lines\"),\n    strip.switch.pad.grid = unit(0, \"cm\"),\n    strip.text.y = element_text(size = 8, angle = 0),\n    axis.text.y = element_blank(),\n  )\n\n\n\n\nFigure 2: A structure plot, showing the topic memberships across all tissue samples in the dataset.\n\n\n\nFrom this plot, we can see clearly that different tissues express different combinations of topics. For example, pancreas tissue typically expresses genes with high probability in topics 3 and 8. Further, within tissues, there can be differences in the types of genes expressed – certain blood cells are almost entirely summarized by topic 1, but most require some mixture of topics 1 and 6. Finally, we see that there are some topics that are common across several tissues. For example, topic 4 is key component of thyroid, skin, muscle, lung, and some brain tissue.\n\nIn fact, topic models are an example of a larger family of models, called mixed-membership models. All of these models generalize clustering, and different variants can be applied to other data types, like continuous, categorical, and network data.↩︎\n",
    "preview": "posts/2021-03-30-week11-4/week11-4_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-04-02T21:48:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-30-week11-3/",
    "title": "Visualizing Topic Models",
    "description": "Once we've fit a topic model, how should we inspect it?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-07",
    "categories": [],
    "contents": "\nReading 1 and 2, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"forcats\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"superheat\")\nlibrary(\"tibble\")\nlibrary(\"tidyr\")\nlibrary(\"tidytext\")\nlibrary(\"topicmodels\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nIn the last set of notes, we fit a topic model to the “Great Library Heist” dataset, but we did not visualize or interpret the results. We’ll work on that here. The code below reads in the tidy topic and membership data.frames from before.\n\n\nmemberships <- read_csv(\"https://uwmadison.box.com/shared/static/c5k5iinwo9au44fb3lc00vq6isbi72c5.csv\")\ntopics <- read_csv(\"https://uwmadison.box.com/shared/static/uh34hhc1wnp072zcryisvgr3z0yh25ad.csv\")\n\n\n\nVisualizing Topics\nA topic is a probability distribution across a collection of words. If the vocabulary isn’t too large, two appropriate visualization strategies are,\nFaceted barplot: Each facet corresponds to a topic. The height of each bar corresponds to a given word’s probability within the topic. The sum of heights across all bars is 1.\nHeatmap: Each row is a topic and each column is a word. The color of the heatmap cells gives the probability of the word within the given topic.\n\nWe can construct a faceted barplot using the tidied beta matrix. We’ve filtered to only words with a probability of at least \\(0.0003\\) in at least one topic, but there are still more words than we could begin to inspect. Nonetheless, it seems that there are words that have relatively high probability in one topic, but not others.\n\n\nggplot(topics %>% filter(beta > 3e-4), aes(term, beta)) +\n  geom_col() +\n  facet_grid(topic ~ .) +\n  theme(axis.text.x = element_blank())\n\n\n\n\nFigure 1: A faceted barplot view of the original topic distributions, with only very limited filtering.\n\n\n\nFor the heatmap, we need to pivot the topics, so that words appear along columns. From there, we can use superheatmap. The advantage of the heatmap is that it takes up less space, and while it obscures comparisons between word probabilities1 the main differences of interest are between low and high probability words.\n\n\ntopics %>%\n  filter(beta > 3e-4) %>%\n  pivot_wider(topic, term, values_from = \"beta\", values_fill = 0, names_repair = \"unique\") %>%\n  select(-1) %>%\n  superheat(\n    pretty.order.cols = TRUE,\n    legend = FALSE\n  )\n\n\n\n\nFigure 2: An equivalent heatmap view of the above faceted barplot.\n\n\n\nNeither approach is very satisfactory since there are too many words for us to effectively label. A workaround is to restrict attention to a subset of “interesting” words. For example, we could filter to,\nTop words overall: We can consider only words whose probabilities are above some threshold. This is the approach used in the visualizations above, though the threshold is very low (there are still too many words to add labels).\nTop words per topic: We can sort the words within each topic in order from highest to lowest probability, and then keep only the \\(S\\) largest.\nMost discriminative words: Some words have high probability just because they are common. They have high probability within each topic but aren’t actually interesting as far as characterizing the topics is concerned. Instead, we can focus on words that are common in some topics but rare in others.\n\nWe can obtain the most probable words using the slice_max function, after first grouping by topic. Then, we use the same reorder_within function from the PCA lectures to reorder words within each topic. The resulting plot is much more interpretable. Judging from the words that are common in each topic’s distribution, we can guess that the topics approximately correspond to: 1 -> Great Expectations, 2 -> 20,000 Leagues Under the Sea, 3 -> Pride & Prejudice, 4 -> War of the Worlds.\n\n\ntop_terms <- topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>%\n  mutate(term = reorder_within(term, beta, topic))\n\nggplot(top_terms, aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_reordered()\n\n\n\n\nFigure 3: The top words associated with the four fitted topics from the Great Library Heist example.\n\n\n\nTo visualize discriminative words, we first compute a discrimination measure for each word and filter to those with the top score. The filtered results can be used in either faceted barplots or heatmaps. Specifically, to find the words that discriminate between topics \\(k\\) and \\(l\\), we compute \\[\\begin{align*}\nD\\left(k, l\\right) := \\beta_{kw}\\log\\left(\\frac{\\beta_{kw}}{\\beta_{lw}}\\right) + \\left(\\beta_{lw} - \\beta_{kw}\\right)\n\\end{align*}\\] for each word \\(w\\). By maximizing over all pairs \\(k, l\\), we can determine whether the word is discriminative between any pair of topics. This might seem like a mysterious formula, but it is just a function that is large when topic \\(k\\) has much larger probability than topic \\(l\\) (see the figure).\n\n\n\n\n\np <- seq(0.01, .99, length.out = 50)\ndf <- expand.grid(p, p) %>%\n  mutate(D = kl_div(Var1, Var2))\n\nggplot(df, aes(Var2, Var1)) +\n  geom_tile(aes(col = D, fill = D)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed() +\n  scale_color_distiller(direction = 1) +\n  scale_fill_distiller(direction = 1) +\n  labs(\n    y = expression(beta[kw]),\n    x = expression(beta[lw])\n  )\n\n\n\n\nFigure 4: An illustration of the formula used for computing a word’s discrimination between topics. The value of D is large when topic k has much larger probability than topic l.\n\n\n\nAn example heatmap of discriminative words is shown below. This backs up our interpretation from the figure above. It also has the advantage that it removes common words (e.g., hand, people, and time appeared in the plot above) and highlights rarer words that are specific to individual topics (e.g., names of characters that appear in only one of the books).\n\n\ndiscriminative_terms <- topics %>%\n  group_by(term) %>%\n  mutate(D = discrepancy(beta)) %>%\n  ungroup() %>%\n  slice_max(D, n = 200) %>%\n  mutate(term = fct_reorder(term, -D))\n\ndiscriminative_terms %>%\n  pivot_wider(term, topic, values_from = \"beta\") %>%\n  column_to_rownames(\"term\") %>%\n  superheat(\n    pretty.order.rows = TRUE,\n    left.label.size = 1.5,\n    left.label.text.size = 3,\n    bottom.label.size = 0.05,\n    legend = FALSE\n  )\n\n\n\n\nFigure 5: A heatmap of the terms that are most discriminative across the four topics.\n\n\n\nVisualizing Memberships\nBesides the topics, it is useful to study the topic proportions for each chapter. One compact approach is to use a boxplot. The result below suggest that each chapter is very definitely assigned to one of the four topics, except for chapters from Great Expectations. Therefore, while the model had the flexibility to learn more complex mixtures, it decided that a clustering structure made the most sense for Pride & Prejudice, War of the Worlds, and 20,000 Leagues Under the Sea.\n\n\nmemberships <- memberships %>%\n  mutate(\n    book = str_extract(document, \"[^_]+\"),\n    topic = factor(topic)\n  )\n\nggplot(memberships, aes(topic, gamma)) +\n  geom_boxplot() +\n  facet_wrap(~book)\n\n\n\n\nFigure 6: A boxplot of the document memberships. It seems that most documents are definitively assigned to one of the four topics.\n\n\n\nThe boxplot considers the collection of documents in aggregate. If we want to avoid aggregation and visualize individual documents, we can use a heatmap or jittered scatterplot. These approaches are useful because heatmap cells and individual points can be drawn relatively small — anything requiring more space would become unwieldy as the number of documents grows. For example, the plot below shows that chapter 119 of Great Expectations has unusually high membership in Topic 2 and low membership in topic 3.\n\n\nggplot(memberships, aes(topic, gamma, col = book)) +\n  geom_point(position = position_jitter(h = 0.05, w = 0.3)) +\n  geom_text_repel(aes(label = document), size = 3) +\n  facet_wrap(~ book) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 7: A jittered scatterplot of the topic memberships associated with each document.\n\n\n\nAlternatively, we can use a “structure” plot. This is a type of stacked barplot where the colors of each bar corresponds to a topic. We’ve sorted the documents using the result of a hierarchical clustering on their proportion vectors – this is like how superheatmap orders rows using a dendrogram when using pretty.order.rows. The takeaways here are similar to those in the scatterplot above.\n\n\ngamma <- memberships %>%\n  pivot_wider(document:book, topic, values_from = gamma)\n\n\n\n\n\nhclust_result <- hclust(dist(gamma[, 3:6]))\ndocument_order <- gamma$document[hclust_result$order]\nmemberships <- memberships %>%\n  mutate(document = factor(document, levels = document_order))\n\nggplot(memberships, aes(gamma, document, fill = topic, col = topic)) +\n  geom_col(position = position_stack()) +\n  facet_grid(book ~ ., scales = \"free\", space = \"free\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(axis.text.y = element_blank())\n\n\n\n\nFigure 8: A structure plot view of each chapter’s topic memberships.\n\n\n\n\nColor is in general harder to compare than bar height.↩︎\n",
    "preview": "posts/2021-03-30-week11-3/week11-3_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-04-02T21:48:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-30-week11-2/",
    "title": "Fitting Topic Models",
    "description": "Data preparation and model fitting code for topics.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-06",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gutenbergr\")\nlibrary(\"stringr\")\nlibrary(\"tidyr\")\nlibrary(\"tidytext\")\nlibrary(\"topicmodels\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nThere are several packages in R that can be used to fit topic models. We will use LDA as implemented in the topicmodels package, which expects input to be structured as a DocumentTermMatrix, a special type of matrix that stores the counts of words (columns) across documents (rows). In practice, most of the effort required to fit a topic model goes into transforming the raw data into a suitable DocumentTermMatrix.\nTo illustrate this process, let’s consider the “Great Library Heist” example from the reading. We imagine that a thief has taken four books — Great Expectations, Twenty Thousand Leagues Under The Sea, War of the Worlds, and Pride & Prejudice — and torn all the chapters out. We are left with pieces of isolated pieces of text and have to determine from which book they are from. The block below downloads all the books into an R object.\n\n\ntitles <- c(\"Twenty Thousand Leagues under the Sea\", \n            \"The War of the Worlds\",\n            \"Pride and Prejudice\", \n            \"Great Expectations\")\nbooks <- gutenberg_works(title %in% titles) %>%\n  gutenberg_download(meta_fields = \"title\")\nbooks\n\n\n# A tibble: 53,205 x 3\n   gutenberg_id text                                    title         \n          <int> <chr>                                   <chr>         \n 1           36 \"The War of the Worlds\"                 The War of th…\n 2           36 \"\"                                      The War of th…\n 3           36 \"by H. G. Wells [1898]\"                 The War of th…\n 4           36 \"\"                                      The War of th…\n 5           36 \"\"                                      The War of th…\n 6           36 \"     But who shall dwell in these wor… The War of th…\n 7           36 \"     inhabited? .  .  .  Are we or th… The War of th…\n 8           36 \"     World? .  .  .  And how are all … The War of th…\n 9           36 \"          KEPLER (quoted in The Anato… The War of th…\n10           36 \"\"                                      The War of th…\n# … with 53,195 more rows\n\nSince we imagine that the word distributions are not equal across the books, topic modeling is a reasonable approach for discovering the books associated with each chapter. Note that, in principle, other clustering and dimensionality reduction procedures could also work.\nFirst, let’s simulate the process of tearing the chapters out. We split the raw texts anytime the word “Chapter” appears. We will keep track of the book names for each chapter, but this information is not passed into the topic modeling algorithm.\n\n\nby_chapter <- books %>%\n  group_by(title) %>%\n  mutate(\n    chapter = cumsum(str_detect(text, regex(\"chapter\", ignore_case = TRUE)))\n  ) %>%\n  group_by(title, chapter) %>%\n  mutate(n = n()) %>%\n  filter(n > 5) %>%\n  ungroup() %>%\n  unite(document, title, chapter)\n\n\n\nAs it is, the text data are long character strings, giving actual text from the novels. To fit LDA, we only need counts of each word within each chapter – the algorithm throws away information related to word order. To derive word counts, we first split the raw text into separate words using the unest_tokens function in the tidytext package. Then, we can count the number of times each word appeared in each document using count, a shortcut for the usual group_by and summarize(n = n()) pattern.\n\n\nword_counts <- by_chapter %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words) %>%\n  count(document, word) # shortcut for group_by(document, word) %>% summarise(n = n())\n\nword_counts\n\n\n# A tibble: 51,361 x 3\n   document             word       n\n   <chr>                <chr>  <int>\n 1 Great Expectations_0 1          1\n 2 Great Expectations_0 1867       1\n 3 Great Expectations_0 2          3\n 4 Great Expectations_0 4          1\n 5 Great Expectations_0 _also      1\n 6 Great Expectations_0 _am_       2\n 7 Great Expectations_0 _and_      1\n 8 Great Expectations_0 _are_      2\n 9 Great Expectations_0 _betsy     1\n10 Great Expectations_0 _both_     1\n# … with 51,351 more rows\n\nThese words counts are still not in a format compatible with conversion to a DocumentTermMatrix. The issue is that the DocumentTermMatrix expects words to be arranged along columns, but currently they are stored across rows. The line below converts the original “long” word counts into a “wide” DocumentTermMatrix in one step. Across these 4 books, we have 65 chapters and a vocabulary of size 18325.\n\n\nchapters_dtm <- word_counts %>%\n  cast_dtm(document, word, n)\nchapters_dtm\n\n\n<<DocumentTermMatrix (documents: 65, terms: 18312)>>\nNon-/sparse entries: 51361/1138919\nSparsity           : 96%\nMaximal term length: 19\nWeighting          : term frequency (tf)\n\nOnce the data are in this format, we can use the LDA function to fit a topic model. We choose \\(K = 4\\) topics because we expect that each topic will match a book. Different hyperparameters can be set using the control argument.\n\n\nchapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))\nchapters_lda\n\n\nA LDA_VEM topic model with 4 topics.\n\nThere are two types of outputs produced by the LDA model: the topic word distributions (for each topic, which words are common?) and the document-topic memberships (from which topics does a document come from?). For visualization, it will be easiest to extract these parameters using the tidy function, specifying whether we want the topics (beta) or memberships (gamma).\n\n\ntopics <- tidy(chapters_lda, matrix = \"beta\")\nmemberships <- tidy(chapters_lda, matrix = \"gamma\")\n\n\n\nThis tidy approach is preferable to extracting the parameters directly from the fitted model (e.g., using chapters_lda@gamma) because it ensures the output is a tidy data.frame, rather than a matrix. Tidy data.frames are easier to visualize using ggplot2.\n\n\n# highest weight words per topic\ntopics %>%\n  arrange(topic, -beta)\n\n\n# A tibble: 73,248 x 3\n   topic term        beta\n   <int> <chr>      <dbl>\n 1     1 captain  0.0154 \n 2     1 nautilus 0.0131 \n 3     1 sea      0.00882\n 4     1 nemo     0.00875\n 5     1 ned      0.00807\n 6     1 conseil  0.00684\n 7     1 land     0.00605\n 8     1 water    0.00595\n 9     1 sir      0.00495\n10     1 day      0.00371\n# … with 73,238 more rows\n\n# topic memberships per document\nmemberships %>%\n  arrange(document, topic)\n\n\n# A tibble: 260 x 3\n   document                topic       gamma\n   <chr>                   <int>       <dbl>\n 1 Great Expectations_0        1 0.000000336\n 2 Great Expectations_0        2 0.000000336\n 3 Great Expectations_0        3 0.996      \n 4 Great Expectations_0        4 0.00440    \n 5 Pride and Prejudice_0       1 0.00159    \n 6 Pride and Prejudice_0       2 0.662      \n 7 Pride and Prejudice_0       3 0.00159    \n 8 Pride and Prejudice_0       4 0.335      \n 9 Pride and Prejudice_100     1 0.0000394  \n10 Pride and Prejudice_100     2 0.0000394  \n# … with 250 more rows\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-02T21:48:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-30-week11-1/",
    "title": "Introduction to Topic Models",
    "description": "An overview of dimensionality reduction via topics.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-05",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nTopic modeling is a type of dimensionality reduction method that is especially useful for high-dimensional count matrices. For example, it can be applied to,\nText data analysis, where each row is a document and each column is a word. The \\(ij^{th}\\) entry contains the count of the \\(j^{th}\\) word in the \\(i^{th}\\) document.\nGene expression analysis, where each row is a biological sample and each column is a gene. The \\(ij^{th}\\) entry measures the amount of gene \\(j\\) expressed in sample \\(i\\).\n\nFor clarity, we will refer to samples as documents and features as words. However, keep in mind that these methods can be used more generally – we will see a biological application three lectures from now.\nThese models are useful to know about because they provide a compromise between clustering and PCA.\nIn clustering, each document would have to be assigned to a single topic. In contrast, topic models allow each document to partially belong to several topics simultaneously. In this sense, they are more suitable when data do not belong to distinct, clearly-defined clusters.\nPCA is also appropriate when the data vary continuously, but it does not provide any notion of clusters. In contrast, topic models estimate \\(K\\) topics, which are analogous to a cluster centroids (though documents are typically a mix of several centroids).\n\nWithout going into mathematical detail, topic models perform dimensionality reduction by supposing,\nEach document is a mixture of topics.\nEach topic is a mixture of words.\n\n\n\n\nFigure 1: An overview of the topic modeling process. Topics are distributions over words, and the word counts of new documents are determined by their degree of membership over a set of underlying topics. In an ordinary clustering model, the bars for the memberships would have to be either pure purple or orange. Here, each document is a mixture.\n\n\n\nTo illustrate the first point, consider modeling a collection of newspaper articles. A set of articles might belong primarily to the “politics” topic, and others to the “business” topic. Articles that describe a monetary policy in the federal reserve might belong partially to both the “politics” and the “business” topic.\nFor the second point, consider the difference in words that would appear in politics and business articles. Articles about politics might frequently include words like “congress” and “law,” but only rarely words like “stock” and “trade.”\nGeometrically, LDA can be represented by the following picture. The corners of the simplex1 represent different words (in reality, there would be \\(V\\) different corners to this simplex, one for each word). A topic is a point on this simplex. The closer the topic is to one of the corners, the more frequently that word appears in the topic.\n\n\n\nFigure 2: A geometric interpretation of LDA, from the original paper by Blei, Ng, and Jordan.\n\n\n\nA document is a mixture of topics, with more words coming from the topics that it is close to. More precisely, a document that is very close to a particular topic has a word distribution just like that topic. A document that is intermediate between two topics has a word distribution that mixes between both topics. Note that this is different from a clustering model, where all documents would lie at exactly one of the corners of the topic simplex. Finally, note that the topics form their own simplex, since each document can be described as a mixture of topics, with mixture weights summing up to 1.\n\nA simplex is the geometric object describing the set of probability vectors over \\(V\\) elements. For example, if \\(V = 3\\), then \\(\\left(0.1, 0, 0.9\\right)\\) and \\(\\left(0.2, 0.3, 0.5\\right)\\) belong to the simplex, but not \\(\\left(0.3, 0.1, 9\\right)\\), since it sums to a number larger than 1.↩︎\n",
    "preview": "https://uwmadison.box.com/shared/static/3shdh2f5vqarkwjucmmebigj2rwm4wyh.png",
    "last_modified": "2021-04-02T21:48:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-25-week10-5/",
    "title": "PCA and UMAP Examples",
    "description": "More examples of dimensionality reduction using PCA and UMAP.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-02",
    "categories": [],
    "contents": "\nReading 1 and 2, Recording, Rmarkdown\n\n\nlibrary(\"embed\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"tidymodels\")\nlibrary(\"tidytext\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\nset.seed(479)\n\n\n\nThese notes don’t introduce any new conceptual material. Instead, they give a few examples of how PCA and UMAP can be used.\nFrom two dimensions to one\nWe presented the two moons dataset earlier as an example where UMAP, but not PCA, would be able to discover a one-dimensional representation that separates the groups. The implication is that, if we anticipate some sort of nonlinearity in higher-dimensions (which we can’t directly visualize), then UMAP would be a more suitable choice.\n\n\nmoons <- read_csv(\"https://uwmadison.box.com/shared/static/kdt9qqvonhcz2ssb599p1nqganrg1w6k.csv\")\nggplot(moons, aes(X, Y, col = Class)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nFigure 1: The original two moons dataset. We will ask both PCA and UMAP to recover a 1D reduction of these 2D data.\n\n\n\nThe code block below defines both the PCA and UMAP recipes. There is no need to normalize the data, since the two dimensions are already on the same scale.\n\n\nmoons_ <- recipe(~ ., data = moons) %>%\n  update_role(Class, new_role = \"id\")\n\npca_rec <- step_pca(moons_, all_predictors(), num_comp = 1)\numap_rec <- step_umap(moons_, all_predictors(), num_comp = 1)\n\n\n\nThe block below shows both the UMAP and PCA representations. The PCA representation seems to mostly reflect the variation on the \\(x\\)-axis of the original data, and the two classes mix together. On the other hand, the UMAP clearly separates the groups. This is expected, since the nearest neighborhood graph that defines UMAP is likely separated into two major components, one for each moon.\n\n\nscores <- bind_cols(\n  prep(umap_rec) %>% juice() %>% mutate(umap_1 = scale(umap_1)),\n  prep(pca_rec) %>% juice() %>% select(-Class)\n) %>%\n  pivot_longer(-Class, names_to = \"method\")\n\nggplot(scores, aes(value, method, col = Class)) +\n  geom_point(position = position_jitter(h = 0.1), alpha = 0.8) +\n  scale_color_brewer(palette = \"Set2\") \n\n\n\n\nFigure 2: 1D PCA and UMAP representations of the 2D two moons dataset.\n\n\n\nAntibiotics Dataset\nWe can apply dimensionality reduction to the antibiotics dataset described in lecture 2 - 1. There, we had filtered down to the 6 most abundant bacteria. Now we will consider 147 most abundant, which means that each sample can be imagined as a vector in a 147-dimensional space. Ideally, a dimensionality-reduction procedure should be able to place samples close to one another when they have similar species profiles. In addition to loading species counts, we load taxonomic information about each species, in the taxa variable.\n\n\nantibiotic <- read_csv(\"https://uwmadison.box.com/shared/static/t1lifegdz8s0a8lgckber32ytyh9hu4r.csv\")\ntaxa <- read_csv(\"https://uwmadison.box.com/shared/static/ng6y6etk79lrm0gtsgw2u0yq6gqcozze.csv\")\n\n\n\nWe now define a PCA recipe. Since the counts are relatively skewed, we log-transform1, using step_log. The rest of the definition is like the 2D example above.\n\n\nantibiotic_ <- recipe(~ ., data = antibiotic) %>%\n  update_role(sample:antibiotic, new_role = \"id\") %>%\n  step_log(all_predictors(), offset = 1) %>%\n  step_normalize(all_predictors())\n\npca_rec <- step_pca(antibiotic_, all_predictors())\npca_prep <- prep(pca_rec)\n\n\n\nWe generate a map of the PCA scores below. The primary difference is between the three study participants – D, E, and F. Within each person, there is some variation between the antibiotic periods, as indicated by the points’ colors.\n\n\nscores <- juice(pca_prep) \nvariances <- tidy(pca_prep, 2, type = \"variance\")\nggplot(scores, aes(PC1, PC2, col = antibiotic)) +\n  geom_point(aes(shape = ind), size = 1.5) +\n  geom_text_repel(aes(label = sample), check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) + \n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nFigure 3: PCA scores for the antibiotics dataset. The main difference is between study participants, with some secondary variation related to whether the participant was taking the antibiotic at that timepoint.\n\n\n\nWhich species contribute the most to the principal components? We can analyze this like we did with the cocktails dataset. In addition to plotting the raw component value, we also join in the taxa information. This allows us to color in each bar by the species group that each bacteria belongs to. For example, we see that samples on the right side of the plot above (i.e., high PC1) likely have more Firmicutes than Bacteroidetes. PC2 seems to pick up on two species that have higher abundance when the rest drop-off.\n\n\ncomponents_ <- tidy(pca_prep, 3) %>%\n  filter(component %in% str_c(\"PC\", 1:6)) %>%\n  mutate(terms_ = reorder_within(terms, abs(value), component)) %>%\n  group_by(component) %>%\n  top_n(20, abs(value)) %>%\n  left_join(taxa)\n\nggplot(components_, aes(value, terms_, fill = Phylum)) +\n  geom_col() +\n  facet_wrap(~ component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(y = NULL) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(axis.text = element_text(size = 5))\n\n\n\n\nFigure 4: The first six principal components associated with the antibiotics dataset.\n\n\n\nWe can use similar code to compute a UMAP embedding. The UMAP seems to separate the different timepoints more clearly. However, there is no analog of components with which to interpret the different axes. Instead, a typical approach to interpret the representation is to find points that are close together (e.g., using \\(K\\)-means) and take their average species profile.\n\n\numap_rec <- step_umap(antibiotic_, all_predictors(), min_dist = 1.5)\numap_prep <- prep(umap_rec)\n\nscores <- juice(umap_prep) \nggplot(scores, aes(umap_1, umap_2, col = antibiotic)) +\n  geom_point(aes(shape = ind), size = 1.5) +\n  geom_text_repel(aes(label = sample), max.overlaps = 10) +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nFigure 5: The UMAP representation associated with the antibiotics dataset.\n\n\n\nImage Data\nBoth PCA and UMAP can be used on image data. Here, each pixel in an image is considered a different feature. For example, the FashionMNIST dataset includes 60,000 28 x 28 images of fashion objects. We can think of image as a vector2 \\(x_{i} \\in \\mathbb{R}^{784}\\). The goal of dimensionality reduction in this context is to build an atlas of images, where images with similar overall pixel values should be located next to one another. First, we read in the data and subsample it, so the code doesn’t take so long to run.\n\n\nfashion <- read_csv(\"https://uwmadison.box.com/shared/static/aur84ttkwa2rqvzo99qo7yhxemoc6om0.csv\") %>%\n  sample_frac(0.2) %>%\n  mutate(\n    image = row_number(),\n    label = as.factor(label)\n  )\n\n\n\nEach row of the matrix above is a separate image. We can prepare a PCA recipe just like in the two examples above. Note that we are not normalizing the features – the pixels are already on a common scale.\n\n\nfashion_ <- recipe(~ ., data = fashion) %>%\n  update_role(label, image, new_role = \"id\")\n\npca_rec <- step_pca(fashion_, all_predictors())\npca_prep <- prep(pca_rec)\n\n\n\n\n\n\nThe code below extracts the PCA scores and visualizes them as a cloud of points. Each point corresponds to an image, and the different types of fashion items are indicated by color. It seems that the types are well separated, but the labels are not informative… to understand what the colors mean, we need to look at the images.\n\n\nscores <- juice(pca_prep) %>%\n  rename(x = PC1, y = PC2)\nggplot(scores, aes(x, y, col = label)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set3\") +\n  coord_fixed()\n\n\n\n\nFigure 6: Principal component scores from the fashion dataset.\n\n\n\nThe block below overlays the first 300 images from the dataset at the locations from the previous plot. We have prepared a function to generate this sort of image using ggplot2; however, we have hidden it to avoid cluttering these notes. You can view the function in the rmarkdown link at the top of this document.\n\n\npivot_scores(scores, fashion) %>%\n  overlay_images()\n\n\n\n\nFigure 7: A subset of principal component scores expressed as the corresponding images.\n\n\n\nFinally, we can repeat the exercise above with UMAP. The first plot shows the UMAP scores and the second overlays the same set of 300 images onto these new coordinates. It seems that UMAP can more clearly separate shoes and pants from shirts and sweaters.\n\n\numap_rec <- step_umap(fashion_, all_predictors(), num_comp = 2, min_dist = 0.5)\numap_prep <- prep(umap_rec)\nscores <- juice(umap_prep) %>%\n  rename(x = umap_1, y = umap_2)\nggplot(scores, aes(x, y, col = label)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set3\") +\n  coord_fixed()\n\n\n\n\nFigure 8: The locations of all the images according to UMAP. Each color is a different class.\n\n\n\n\n\npivot_scores(scores, fashion, scale_factor = 0.05) %>%\n  overlay_images(scale_factor = 0.05)\n\n\n\n\nFigure 9: A sample of the images at the locations determined by UMAP.\n\n\n\n\nSpecifically, we use a \\(\\log\\left(1 + x\\right)\\) transform, since there are many 0 counts.↩︎\n28 * 28 = 784↩︎\n",
    "preview": "posts/2021-03-25-week10-5/week10-5_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2021-03-27T11:11:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-24-week10-4/",
    "title": "Uniform Manifold Approximation and Projection",
    "description": "An overview of the UMAP algorithm.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-04-01",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"tidymodels\")\nlibrary(\"embed\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nNonlinear dimension reduction methods can give a more faithful representation than PCA when the data don’t lie on a low-dimensional linear subspace.\nFor example, suppose the data were shaped like this. There is no one-dimensional line through these data that separate the groups well. We will need an alternative approach to reducing dimensionality if we want to preserve nonlinear structure.\n\n\nmoons <- read_csv(\"https://uwmadison.box.com/shared/static/kdt9qqvonhcz2ssb599p1nqganrg1w6k.csv\")\nggplot(moons, aes(X, Y, col = Class)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nFigure 1: An example nonlinear dataset where projections onto any straight line will necessarily cause the classes to bleed together.\n\n\n\nFrom a high-level, the intuition behind UMAP is to (a) build a graph joining nearby neighbors in the original high-dimensional space, and then (b) layout the graph in a lower-dimensional space.\nFor example, consider the 2-dimensional sine wave below. If we build a graph, we can try to layout the resulting nodes and edges on a 1-dimensional line in a way that approximately preserves the ordering.\n\n\n\nFigure 2: UMAP (and many other nonlinear methods) begins by constructing a graph in the high-dimensional space, whose layout in the lower dimensional space will ideally preserve the essential relationships between samples.\n\n\n\nA natural way to build a graph is to join each node to its \\(K\\) closest neighbors. The choice of \\(K\\) will influence the final reduction, and it is treated as a hyperparameter of UMAP.\n\n\n\nFigure 3: When using fewer nearest neighbors, the final dimensionality reduction will place more emphasis on effectively preserving the relationships between points in local neighborhoods.\n\n\n\nLarger values of \\(K\\) prioritize preservation of global structure, while smaller \\(K\\) will better reflect local differences. This property is not obvious a priori, but is suggested by the simulations described in the reading.\n\n\n\nFigure 4: When using larger neighborhoods, UMAP will place more emphasis on preserving global structure, sometimes at the cost of local relationships between points.\n\n\n\nOne detail in the graph construction: In UMAP, the edges are assigned weights depending on the distance they span, normalized by the distance to the closest neighbor. Neighbors that are close, relative to the nearest neighbors, are assigned higher weights than those that are far away, and points that are linked by high weight edges are pulled together with larger force in the final graph layout. This is what the authors mean by using a “fuzzy” nearest neighbor graph. The fuzziness allows the algorithm to distinguish neighbors that are very close from those that are far, even though they all lie within a \\(K\\)-nearest-neighborhood.\nOnce the graph is constructed, there is the question of how the graph layout should proceed. UMAP uses a variant of force-directed layout, and the global strength of the springs is another hyperparameter. Lower tension on the springs allow the points to spread out more loosely, higher tension forces points closer together. This is a second hyperparameter of UMAP.\n\n\n\nThese two hyperparameters — the number of nearest neighbors \\(K\\) and the layout tension — are the only two hyperparameters of UMAP.\nYou can see more examples of what this algorithm does to toy datasets in the reading. Note in particular the properties that the algorithm does not preserve. The distance between clusters should not be interpreted, since it just means that the graph components were not connected. Similarly, the density of points is not preserved.\nIn R, we can implement this using almost the same code as we used for PCA. The step_umap command is available through the embed package.\n\n\ncocktails_df <- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\numap_rec <- recipe(~., data = cocktails_df) %>%\n  update_role(name, category, new_role = \"id\") %>%\n  step_normalize(all_predictors()) %>%\n  step_umap(all_predictors(), neighbors = 20, min_dist = 0.1)\numap_prep <- prep(umap_rec)\n\n\n\nUMAP returns a low-dimensional atlas relating the points, but it does not provide any notion of derived features.\n\n\nggplot(juice(umap_prep), aes(umap_1, umap_2)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 0.8) +\n  geom_text(aes(label = name), check_overlap = TRUE, size = 3, hjust = \"inward\")\n\n\n\n\nFigure 5: The learned UMAP representation of the cocktails dataset.\n\n\n\nWe can summarize the properties of UMAP,\nGlobal or local structure: The number of nearest neighbors \\(K\\) used during graph construction can be used modulate the emphasis of global vs. local structure.\nNonlinear: UMAP can reflect nonlinear structure in high-dimensions.\nNo interpretable features: UMAP only returns the map between points, and there is no analog of components to describe how the original features were used to construct the map.\nSlower: While UMAP is much faster than comparable nonlinear dimensionality reduction algorithms, it is still slower than linear approaches.\nNondeterministic: The output from UMAP can change from run to run, due to randomness in the graph layout step. If exact reproducibility is required, a random seed should be set.\n\n\n\n",
    "preview": "posts/2021-03-24-week10-4/week10-4_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-27T11:08:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-24-week10-3/",
    "title": "Principal Components Analysis II",
    "description": "Visualizing and interpreting PCA.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-31",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"tidymodels\")\nlibrary(\"tidytext\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nHow should we visualize the results from PCA? There are three artifacts produced by the procedure worth considering — components, scores, and variances. The components describe derived features, the scores lay samples out on a map, and the variances summarize how much information was preserved by each dimension.\n\n\n# produced by code in previous notes\ncomponents <- read_csv(\"https://uwmadison.box.com/shared/static/dituepd0751qqsims22v2liukuk0v4bf.csv\") %>%\n  filter(component %in% str_c(\"PC\", 1:5))\nscores <- read_csv(\"https://uwmadison.box.com/shared/static/qisbw1an4lo8naifoxyu4dqv4bfsotcu.csv\")\nvariances <- read_csv(\"https://uwmadison.box.com/shared/static/ye125xf8800zc5eh3rfeyzszagqkaswf.csv\") %>%\n  filter(terms == \"percent variance\")\n\n\n\nFirst, let’s see how much variance is explained by each dimension of the PCA. Without detouring into the mathematical explanation, the main idea is that, the more rapid the dropoff in variance explained, the better the low-dimensional approximation. For example, if the data actually lie on a 2D plane in a high-dimensional space, then the first two bars would contain all the variance (the rest would be zero).\n\n\nggplot(variances) +\n  geom_col(aes(component, value))\n\n\n\n\nFigure 1: Proportion of variance explained by each component in the PCA.\n\n\n\nWe can interpret components by looking at the linear coefficients of the variables used to define them. From the plot below, we see that the first PC mostly captures variation related to whether the drink is made with powdered sugar or simple syrup. Drinks with high values of PC1 are usually to be made from simple syrup, those with low values of PC1 are usually made from powdered sugar. From the two largest bars in PC2, we can see that it highlights the vermouth vs. non-vermouth distinction.\n\n\nggplot(components, aes(value, terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~component, nrow = 1) +\n  labs(y = NULL) +\n  theme(axis.text = element_text(size = 7))\n\n\n\n\nFigure 2: The top 5 principal components associated with the cocktails dataset.\n\n\n\nIt is often easier read the components when the bars are sorted according to their magnitude. The usual ggplot approach to reordering axes labels, using either reorder() or releveling the associated factor, will reorder all the facets in the same way. If we want to reorder each facet on its own, we can use the reorder_within function coupled with scale_*_reordered, both from the tidytext package.\n\n\ncomponents_ <- components %>%\n  filter(component %in% str_c(\"PC\", 1:3)) %>%\n  mutate(terms = reorder_within(terms, abs(value), component))\nggplot(components_, aes(value, terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(y = NULL) +\n  theme(axis.text = element_text(size = 7))\n\n\n\n\nFigure 3: The top 3 principal components, with defining variables sorted by the magnitude of their coefficient.\n\n\n\nNext, we can visualize the scores of each sample with respect to these components. The plot below shows \\(\\left(z_{i1}, z_{i2}\\right)\\). Suppose that the columns of \\(\\Phi\\) are \\(\\varphi_{1}, \\dots, \\varphi_{K}\\). Then, since \\(x_{i} \\approx \\varphi_{1}z_{i1} + \\varphi_{2} z_{i2}\\), the samples have large values for variables with large component values in the coordinate directions where \\(z_{i}\\) is farther along.\n\n\nggplot(scores, aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 1.5) +\n  geom_text_repel(check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) # rescale axes to reflect variance\n\n\n\n\nFigure 4: The scores associated with the cocktails dataset.\n\n\n\nFor example, El Nino has high value for PC1, which means it has a high value of variables that are positive for PC1 (like simple syrup) and low value for those variables that are negative (like powdered sugar). Similarly, since \\(\\varphi_{2}\\) puts high positive weight on vermouth-related variables, so H. P. W. Cocktail has many vermouth-related ingredients.\nIn practice, it will often be important to visualize several pairs of PC dimensions against one another, not just the top 2.\nLet’s examine the original code in a little more detail. We are using tidymodels, which is a package for decoupling the definition and execution of a data pipeline. This compartmentalization makes it easier to design and reuse across settings.\n\n\npca_rec <- recipe(~., data = cocktails_df) %>%\n  update_role(name, category, new_role = \"id\") %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors())\n\npca_prep <- prep(pca_rec)\n\n\n\nHere is how you would apply PCA without the tidymodels package. You have to first split the data into the “metadata” that is used to interpret the scores and the numerical variables used as input to PCA. Then at the end, you have to join the metadata back in. It’s not impossible, but the code is not as readable.\n\n\n# split name and category out of the data frame\npca_result <- cocktails_df %>%\n  select(-name, -category) %>%\n  scale() %>%\n  princomp()\n\n# join them back into the PCA result\nmetadata <- cocktails_df %>%\n  select(name, category)\nscores_direct <- cbind(metadata, pca_result$scores)\n\nggplot(scores_direct, aes(Comp.1, Comp.2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 1.5) +\n  geom_text_repel(check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) # rescale axes to reflect variance\n\n\n\n\nFigure 5: A plot of the PCA scores made without using tidymodels.\n\n\n\nThe equivalent tidymodels implementation handles the difference between supplementary and modeling data less bluntly, setting the name and category variables to id roles, so that all_predictors() knows to skip them.\nWe conclude with some characteristics of PCA, which can guide the choice between alternative dimensionality reduction methods.\nGlobal structure: Since PCA is looking for high-variance overall, it tends to focus on global structure.\nLinear: PCA can only consider linear combinations of the original features. If we expect nonlinear features to be more meaningful, then another approach should be considered.\nInterpretable features: The PCA components exactly specify how to construct each of the derived features.\nFast: Compared to most dimensionality reduction methods, PCA is quite fast. Further, it is easy to implement approximate versions of PCA that scale to very large datasets.\nDeterministic: Some embedding algorithms perform an optimization process, which means there might be some variation in the results due to randomness in the optimization. In contrast, PCA is deterministic, with the components being unique up to sign (i.e., you could reflect the components across an axis, but that is the most the results might change).\n\n\n\n",
    "preview": "posts/2021-03-24-week10-3/week10-3_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-03-27T11:08:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-24-week10-2/",
    "title": "Principal Components Analysis I",
    "description": "Linear dimensionality reduction using PCA.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-30",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"tidymodels\")\nlibrary(\"readr\")\n\n\n\nIn our last notes, we saw how we could organize a collection of images based on average pixel brightness. We can think of average pixel brightness as a derived feature that can be used to build a low-dimensional map.\nWe can partially automate the process of deriving new features. Though, in general, finding the best way to combine raw features into derived ones is a complicated problem, we can simplify things by restricting attention to,\nFeatures that are linear combinations of the raw input columns.\nFeatures that are orthogonal to one another.\nFeatures that have high variance.\nRestricting to linear combinations allows for an analytical solution. We will relax this requirement when discussing UMAP.\nOrthogonality means that the derived features will be uncorrelated with one another. This is a nice property, because it would be wasteful if features were redundant.\nHigh variance is desirable because it means we preserve more of the essential structure of the underlying data. For example, if you look at this 2D representation of a 3D object, it’s hard to tell what it is,\n\n\n\nFigure 1: What is this object?\n\n\n\nBut when viewing an alternative reduction which has higher variance…\n\n\n\nFigure 2: Not so complicated now. Credit for this example goes to Professor Julie Josse, at Ecole Polytechnique.\n\n\n\nPrincipal Components Analysis (PCA) is the optimal dimensionality reduction under these three restrictions, in the sense that it finds derived features with the highest variance. Formally, PCA finds a matrix \\(\\Phi \\in \\mathbb{R}^{D \\times K}\\) and a set of vector \\(z_{i} \\in \\mathbb{R}^{K}\\) such that \\(x_{i} \\approx \\Phi z_{i}\\) for all \\(i\\). The columns of \\(\\Phi\\) are called principal components, and they specify the structure of the derived linear features. The vector \\(z_{i}\\) is called the score of \\(x_{i}\\) with respect to these components. The top component explains the most variance, the second captures the next most, and so on.\nFor example, if one of the columns of \\(\\Phi\\) was equal to \\(\\left(\\frac{1}{D}, \\dots, \\frac{1}{D}\\right)\\), then that feature computes the average of all coordinates (e.g., to get average brightness), and the corresponding \\(z_{i}\\) would be a measure of the average brightness of sample \\(i\\).\nGeometrically, the columns of \\(\\Phi\\) span a plane that approximates the data. The \\(z_{i}\\) provide coordinates of points projected onto this plane.\n\n\n\nFigure 3: PCA finds a low-dimensional linear subspace that closely approximates the high-dimensional data.\n\n\n\nIn R, PCA can be conveniently implemented using the tidymodels package. We will see a base R implementation in the next lecture. The dataset below contains properties of a variety of cocktails, from the Boston Bartender’s guide. The first two columns are qualitative descriptors, while the rest give numerical ingredient information.\n\n\ncocktails_df <- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\ncocktails_df[, 1:6]\n\n\n# A tibble: 937 x 6\n   name     category   light_rum lemon_juice lime_juice sweet_vermouth\n   <chr>    <chr>          <dbl>       <dbl>      <dbl>          <dbl>\n 1 Gauguin  Cocktail …      2           1          1               0  \n 2 Fort La… Cocktail …      1.5         0          0.25            0.5\n 3 Cuban C… Cocktail …      2           0          0.5             0  \n 4 Cool Ca… Cocktail …      0           0          0               0  \n 5 John Co… Whiskies        0           1          0               0  \n 6 Cherry … Cocktail …      1.25        0          0               0  \n 7 Casa Bl… Cocktail …      2           0          1.5             0  \n 8 Caribbe… Cocktail …      0.5         0          0               0  \n 9 Amber A… Cordials …      0           0.25       0               0  \n10 The Joe… Whiskies        0           0.5        0               0  \n# … with 927 more rows\n\nThe pca_rec object below defines a tidymodels recipe for performing PCA. Computation of the lower-dimensional representation is deferred until prep() is called. This delineation between workflow definition and execution helps clarify the overall workflow, and it is typical of the tidymodels package.\n\n\npca_rec <- recipe(~., data = cocktails_df) %>%\n  update_role(name, category, new_role = \"id\") %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors())\n\npca_prep <- prep(pca_rec)\n\n\n\nThe step_normalize call is used to center and scale all the columns. This is needed because otherwise columns with larger variance will have more weight in the final dimensionality reduction, but this is not conceptually meaningful. For example, if one of the columns in a dataset were measuring length in kilometers, then we could artificially increase its influence in a PCA by expressing the same value in meters. To achieve invariance to this change in units, it would be important to normalize first.\nWe can tidy each element of the workflow object. Since PCA was the second step in the workflow, the PCA components can be obtained by calling tidy with the argument “2.” The scores of each sample with respect to these components can be extracted using juice. The amount of variance explained by each dimension is also given by tidy, but with the argument type = \"variance\". We’ll see how to visualize and interpret these results in the next lecture.\n\n\ntidy(pca_prep, 2)\n\n\n# A tibble: 1,600 x 4\n   terms             value component id       \n   <chr>             <dbl> <chr>     <chr>    \n 1 light_rum        0.163  PC1       pca_pxTD1\n 2 lemon_juice     -0.0140 PC1       pca_pxTD1\n 3 lime_juice       0.224  PC1       pca_pxTD1\n 4 sweet_vermouth  -0.0661 PC1       pca_pxTD1\n 5 orange_juice     0.0308 PC1       pca_pxTD1\n 6 powdered_sugar  -0.476  PC1       pca_pxTD1\n 7 dark_rum         0.124  PC1       pca_pxTD1\n 8 cranberry_juice  0.0954 PC1       pca_pxTD1\n 9 pineapple_juice  0.119  PC1       pca_pxTD1\n10 bourbon_whiskey  0.0963 PC1       pca_pxTD1\n# … with 1,590 more rows\n\n\n\njuice(pca_prep)\n\n\n# A tibble: 937 x 7\n   name           category          PC1     PC2     PC3     PC4    PC5\n   <fct>          <fct>           <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 Gauguin        Cocktail Clas…  1.38  -1.15    1.34   -1.12    1.52 \n 2 Fort Lauderda… Cocktail Clas…  0.684  0.548   0.0308 -0.370   1.41 \n 3 Cuban Cocktai… Cocktail Clas…  0.285 -0.967   0.454  -0.931   2.02 \n 4 Cool Carlos    Cocktail Clas…  2.19  -0.935  -1.21    2.47    1.80 \n 5 John Collins   Whiskies        1.28  -1.07    0.403  -1.09   -2.21 \n 6 Cherry Rum     Cocktail Clas… -0.757 -0.460   0.909   0.0154 -0.748\n 7 Casa Blanca    Cocktail Clas…  1.53  -0.392   3.29   -3.39    3.87 \n 8 Caribbean Cha… Cocktail Clas…  0.324  0.137  -0.134  -0.147   0.303\n 9 Amber Amour    Cordials and …  1.31  -0.234  -1.55    0.839  -1.19 \n10 The Joe Lewis  Whiskies        0.138 -0.0401 -0.0365 -0.100  -0.531\n# … with 927 more rows\n\n\n\ntidy(pca_prep, 2, type = \"variance\")\n\n\n# A tibble: 160 x 4\n   terms    value component id       \n   <chr>    <dbl>     <int> <chr>    \n 1 variance  2.00         1 pca_pxTD1\n 2 variance  1.71         2 pca_pxTD1\n 3 variance  1.50         3 pca_pxTD1\n 4 variance  1.48         4 pca_pxTD1\n 5 variance  1.37         5 pca_pxTD1\n 6 variance  1.32         6 pca_pxTD1\n 7 variance  1.30         7 pca_pxTD1\n 8 variance  1.20         8 pca_pxTD1\n 9 variance  1.19         9 pca_pxTD1\n10 variance  1.18        10 pca_pxTD1\n# … with 150 more rows\n\n\n\n\n",
    "preview": "https://www.huber.embl.de/msmb/images/CAM4.png",
    "last_modified": "2021-03-27T11:08:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-24-week10-1/",
    "title": "Introduction to Dimensionality Reduction",
    "description": "Examples of high-dimensional data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-29",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nHigh-dimensional data are data where many features are collected for each observation. These tend to be wide datasets with many columns. The name comes from the fact that each row of the dataset can be viewed as a vector in a high-dimensional space (one dimension for each feature). These data are common in modern applications,\nEach cell in a genomics dataset might have measurements for hundreds of molecules.\nEach survey respondent might provide answers to dozens of questions.\nEach image might have several thousand pixels.\nEach document might have counts across several thousand relevant words.\nFor low-dimensional data, we could visually encode all the features in our data directly, either using properties of marks or through faceting. In high-dimensional data, this is no longer possible.\nHowever, though there are many features associated with each observation, it may still be possible to organize samples across a smaller number of meaningful, derived features.\nFor example, consider the Metropolitan Museum of Art dataset, which contains images of many artworks. Abstractly, each artwork is a high-dimensional object, containing pixel intensities across many pixels. But it is reasonable to derive a feature based on the average brightness.\n\n\n\nFigure 1: An arrangement of artworks according to their average pixel brightness, as given in the reading.\n\n\n\nIn general, manual feature construction can be difficult. Algorithmic approaches try streamline the process of generating these maps by optimizing some more generic criterion. Different algorithms use different criteria, which we will review in the next couple of lectures.\n\n\n\nFigure 2: The dimensionality reduction algorithm in this animation converts a large number of raw features into a position on a one-dimensional axis defined by average pixel brightness. In general, we might reduce to dimensions other than 1D, and we will often want to define features tailored to the dataset at hand.\n\n\n\nInformally, the goal of dimensionality reduction techniques is to produce a low-dimensional “atlas” relating members of a collection of complex objects. Samples that are similar to one another in the high-dimensional space should be placed near one another in the low-dimensional view. For example, we might want to make an atlas of artworks, with similar styles and historical periods being placed near to one another.\n\n\n\n",
    "preview": "posts/2021-03-24-week10-1/images/brightness.png",
    "last_modified": "2021-03-27T11:08:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-17-week9-5/",
    "title": "Cluster Stability",
    "description": "How reliable are the results of a clustering?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-25",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"MASS\")\nlibrary(\"Matrix\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"pdist\")\nlibrary(\"superheat\")\nlibrary(\"tidyr\")\ntheme_set(theme_minimal())\nset.seed(1234)\n\n\n\nOne of the fundamental principles in statistics is that, no matter how the experiment / study was conducted, if we ran it again, we would get different results. More formally, sampling variability creates uncertainty in our inferences.\nHow should we think about sampling variability in the context of clustering? This is a tricky problem, because you can permute the labels of the clusters without changing the meaning of the clustering. However, it is possible to measure and visualize the stability of a point’s cluster assignment.\nTo make this less abstract, consider an example. A study has found a collection of genes that are differentially expressed between patients with two different subtypes of a disease. There is an interest in clustering genes that have similar expression profiles across all patients — these genes probably belong to similar biological processes.\nOnce you run the clustering, how sure can you be that, if the study would run again, you would recover a similar clustering? Are there some genes that you are sure belong to a particular cluster? Are there some that lie between two clusters?\nTo illustrate, consider the simulated dataset below. Imagine that the rows are patients, the column are genes, and the colors are the expression levels of genes within patients. There are 5 clusters of genes here (columns 1 - 20 are cluster 1, 21 - 41 are cluster 2, …). The first two clusters are only weakly visible, while the last three stand out strongly.\n\n\nn_per <- 20\np <- n_per * 5\nSigma1 <- diag(2) %x% matrix(rep(0.3, n_per ** 2), nrow = n_per)\nSigma2 <- diag(3) %x% matrix(rep(0.6, n_per ** 2), nrow = n_per)\nSigma <- bdiag(Sigma1, Sigma2)\ndiag(Sigma) <- 1\nmu <- rep(0, 100)\nx <- mvrnorm(25, mu, Sigma)\n\ncols <- c('#f6eff7','#bdc9e1','#67a9cf','#1c9099','#016c59')\nsuperheat(\n  x, \n  pretty.order.rows = TRUE, \n  bottom.label = \"none\", \n  heat.pal = cols,\n  left.label.text.size = 3,\n  legend = FALSE\n)\n\n\n\n\nFigure 1: A simulated clustering of genes (columns) across rows (patients).\n\n\n\nThe main idea for how to compute cluster stability is to bootstrap (i.e., randomly resample) the patients and see whether the cluster assignments for each gene change. More precisely, we use the following strategy,\nUsing all the patients, \\(X\\), estimate the cluster centroids \\(c_{1}, \\dots, c_{K}\\).\nFor \\(B\\) bootstrap iterations, perform the following.\nSample the patients with replacement, generating a bootstrap resampled version of the dataset \\(X_{b}^{\\ast}\\).\nPermute the original cluster centroids to reflect the order of patients in \\(X_{b}^{\\ast}\\). Call the permuted centroids \\(c_{1b}^{\\ast}, \\dots, c_{Kb}^{\\ast}\\).\nAssign genes in \\(X_{b}^{\\ast}\\) to the cluster \\(k\\) of the closest \\(c_{bk}^{\\ast}\\).\n\nWe quantify our certainty that gene \\(j\\) belongs to cluster \\(k\\) by counting the number of times that gene \\(j\\) was assigned to cluster \\(k\\).\nThe picture below describes the bootstrapping process for a gene. The two rows correspond to the original and bootstrapped representations a specific gene, respectively. Each bar gives the expression level of the gene for one individual. Due to the random sampling in the bootstrapped dataset, some individuals become overrepresented and some are removed. If we also permute the centroids in the same way, we get a new distance between genes and their centroids. Since the patients who are included changes, the distances between each gene and each centroid changes, so the genes might be assigned to different clusters.\n\n\n\n\n\nK <- 5\nB <- 1000\ncluster_profiles <- kmeans(t(x), centers = K)$centers\ncluster_probs <- matrix(nrow = ncol(x), ncol = B)\n\nfor (b in seq_len(B)) {\n  b_ix <- sample(nrow(x), replace = TRUE)\n  dists <- as.matrix(pdist(t(x[b_ix, ]), cluster_profiles[, b_ix]))\n  cluster_probs[, b] <- apply(dists, 1, which.min)\n}\n\ncluster_probs <- as_tibble(cluster_probs) %>%\n  mutate(gene = row_number()) %>%\n  pivot_longer(-gene, names_to = \"b\", values_to = \"cluster\")\n\n\n\nThe table below shows the result of this procedure. In each bootstrap iteration, gene 1 was assigned to cluster 4, so we can rely on that assignment. On the other hand, gene 3 is assigned to cluster 4 75% of the time, but occasionally appears in clusters 1, 2, and 5.\n\n\ncluster_probs <- cluster_probs %>%\n  mutate(cluster = as.factor(cluster)) %>%\n  group_by(gene, cluster) %>%\n  summarise(prob = n() / B)\n\ncluster_probs\n\n\n# A tibble: 267 x 3\n# Groups:   gene [100]\n    gene cluster  prob\n   <int> <fct>   <dbl>\n 1     1 2       0.956\n 2     1 3       0.041\n 3     1 5       0.003\n 4     2 2       0.778\n 5     2 5       0.222\n 6     3 1       0.001\n 7     3 2       0.978\n 8     3 5       0.021\n 9     4 1       0.001\n10     4 2       0.689\n# … with 257 more rows\n\nThese fractions for all genes are summarized by the plot below. Each row is a gene. The length of each color gives the number of times that gene was assigned to that cluster. The genes from rows 41 - 100 are all clearly distinguished, which is in line with what we saw visually in the heatmap above. The first two clusters are somewhat recovered, but since they were often assigned to alternative clusters, we can conclude that they were harder to demarcate out than the others.\n\n\nggplot(cluster_probs) +\n  geom_bar(aes(y = as.factor(gene), x = prob, col = cluster, fill = cluster), stat = \"identity\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = \"Gene\", x = \"Proportion\") +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_text(size = 7),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-17-week9-5/week9-5_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-23T08:14:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-17-week9-4/",
    "title": "Silhouette Statistics",
    "description": "Diagnostics for the quality of a clustering.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-24",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"cluster\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\nlibrary(\"tidymodels\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\ntheme_set(theme_bw())\nset.seed(123)\n\n\n\nClustering algorithms usually require the number of clusters \\(K\\) as an argument. How should it be chosen?\nThere are many possible criteria, but one common approach is to compute the silhouette statistic. It is a statistic that can be computed for each observation in a dataset, measuring how strongly it is tied to its assigned cluster. If a whole cluster has large silhouette statistics, then that cluster is well-defined and clearly isolated other clusters.\nThe plots below illustrate the computation of silhouette statistics for a clustering of the penguins dataset that used \\(K = 3\\). To set up, we first need to cluster the penguins dataset. The idea is the same as in the \\(K\\)-means notes, but we encapsulate the code in a function, so that we can easily extract data for different values of \\(K\\).\n\n\npenguins <- read_csv(\"https://uwmadison.box.com/shared/static/ijh7iipc9ect1jf0z8qa2n3j7dgem1gh.csv\") %>%\n  na.omit() %>%\n  mutate(id = row_number())\n\ncluster_penguins <- function(penguins, K) {\n  x <- penguins %>%\n    select(matches(\"length|depth|mass\")) %>%\n    scale()\n    \n  kmeans(x, center = K) %>%\n    augment(penguins) %>% # creates column \".cluster\" with cluster label\n    mutate(silhouette = silhouette(as.integer(.cluster), dist(x))[, \"sil_width\"])\n}\n\n\n\nDenote the silhouette statistic of observation \\(i\\) by \\(s_{i}\\). We will compute \\(s_i\\) for the observation with the black highlight below1.\n\n\ncur_id <- 2\npenguins3 <- cluster_penguins(penguins, K = 3)\nobs_i <- penguins3 %>%\n  filter(id == cur_id)\n\n\n\n\n\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\n\nFigure 1: The observation on which we will compute the silhouette statistic.\n\n\n\nThe first step in the calculation of the silhouette statistic is to measure the pairwise distances between the observation \\(i\\) and all observations in the same cluster. These distances are the lengths of the small lines below. Call average of these lengths \\(a_{i}\\).\n\n\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_segment(\n    data = penguins3 %>% filter(.cluster == obs_i$.cluster), \n    aes(xend = obs_i$bill_length_mm, yend = obs_i$bill_depth_mm),\n    size = 0.6, alpha = 0.3\n  ) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1)) +\n  labs(title = expression(paste(\"Distances used for \", a[i])))\n\n\n\n\nFigure 2: The average distance between the target observation and all others in the same cluster.\n\n\n\nNext, we compute pairwise distances to all observations in clusters 2 and 3. The average of these pairwise distances are called \\(b_{i2}\\) and \\(b_{i3}\\). Choose the smaller of \\(b_{i2}\\) and \\(b_{i3}\\), and call it \\(b_{i}\\). In a sense, this is the “next best” cluster to put observation \\(i\\). For a general \\(K\\), you would compute \\(b_{ik}\\) for all \\(k\\) (other than observation \\(i\\)’s cluster) and take the minimum across all of them. In this case, the orange segments are on average smaller than the blue segments, so \\(b_i\\) is defined as the average length of the orange segments.\n\n\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_segment(\n    data = penguins3 %>% filter(.cluster != obs_i$.cluster), \n    aes(xend = obs_i$bill_length_mm, yend = obs_i$bill_depth_mm, col = .cluster),\n    size = 0.5, alpha = 0.3\n  ) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1)) +\n  labs(title = expression(paste(\"Distances used for \", b[i][1], \" and \", b[i][2])))\n\n\n\n\nFigure 3: The average distance between the target observation and all others in different clusters.\n\n\n\nThe silhouette statistic for observation \\(i\\) is derived from the relative lengths of the orange vs. green segments. Formally, the silhouette statistic for observation \\(i\\) is \\(s_{i}:= \\frac{b_{i} - a_{i}}{\\max\\left({a_{i}, b_{i}}\\right)}\\). This number is close to 1 if the orange segments are much longer than the green segments, close to 0 if the segments are about the same size, and close to -1 if the the orange segments are much shorter than the green segments2.\nThe median of these \\(s_{i}\\) for all observations within cluster \\(k\\) is a measure of how well-defined cluster \\(k\\) is overall. The higher this number, the more well-defined the cluster.\nDenote the median of the silhouette statistics within cluster \\(k\\) by \\(SS_{k}\\). A measure how good a choice of \\(K\\) is can be determined by the median of these medians: \\(\\text{Quality}(K) := \\text{median}_{k = 1 \\dots, K} SS_{k}\\).\nIn particular, this can be used to define (a) a good cut point in a hierarchical clustering or (b) a point at which a cluster should no longer be split into subgroups.\nIn R, we can use the silhouette function from the cluster package to compute the silhouette statistic. The syntax is silhouette(cluster_labels, pairwise_distances) where cluster_labels is a vector of (integer) cluster ID’s for each observation and pairwise_distances gives the lengths of the segments between all pairs of observations. An example of this function’s usage is given in the function at the start of the illustration.\nThis is what the silhouette statistic looks like in the penguins dataset when we choose 3 clusters. The larger points have lower silhouette statistics. This points between clusters 2 and 3 have large silhouette statistics because those two clusters blend into one another.\n\n\nggplot(penguins3) +\n  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster, size = silhouette)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\n\nFigure 4: The silhouette statistics on the Palmers Penguins dataset, when using \\(K\\)-means with \\(K = 3\\).\n\n\n\nWe can also visualize the histogram of silhouette statistics within each cluster. Since the silhouette statistics for cluster 2 are generally lower than those for the other two clusters (in particular, its median is lower), we can conclude that it is less well-defined.\n\n\nggplot(penguins3) +\n  geom_histogram(aes(x = silhouette), binwidth = 0.05) +\n  facet_grid(~ .cluster)\n\n\n\n\nFigure 5: The per-cluster histograms of silhouette statistics summarize how well-defined each cluster is.\n\n\n\nIf we choose even more clusters, then there are more points lying along the boundaries of poorly defined clusters. Their associated silhouette statistics end up becoming larger. From the histogram, we can also see a deterioration in the median silhouette scores across all clusters.\n\n\npenguins4 <- cluster_penguins(penguins, K = 4)\nggplot(penguins4) +\n  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster, size = silhouette)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\n\nFigure 6: We can repeat the same exercise, but with \\(K = 4\\) clusters instead.\n\n\n\n\n\nggplot(penguins4) +\n  geom_histogram(aes(x = silhouette), binwidth = 0.05) +\n  facet_grid(~ .cluster)\n\n\n\n\n\nYou can change cur_id to try different observations.↩︎\nThis last case likely indicates a misclustering.↩︎\n",
    "preview": "posts/2021-03-17-week9-4/week9-4_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-03-20T11:16:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-17-week9-3/",
    "title": "Heatmaps",
    "description": "Visualizing table values, ordered by clustering results.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-23",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"superheat\")\nlibrary(\"tibble\")\ntheme_set(theme_minimal())\n\n\n\nThe direct outputs of a standard clustering algorithim are (a) cluster assignments for each sample, (b) the centroids associated with each cluster. A hierarchical clustering algorithm enriches this output with a tree, which provide (a) and (b) at multiple levels of resolution.\nThese outputs can be used to improve visualizations. For example, they can be used to define small multiples, faceting across clusters. One especially common idea is to reorder the rows of a heatmap using the results of a clustering, and this is the subject of these notes.\nIn a heatmap, each mark (usually a small tile) corresponds to an entry of a matrix. The \\(x\\)-coordinate of the mark encodes the index of the observation, while the \\(y\\)-coordinate encodes the index of the feature. The color of each tile represents the value of that entry. For example, here are the first few rows of the movies data, along with the corresponding heatmap, made using the superheat package.\n\n\nmovies_mat <- read_csv(\"https://uwmadison.box.com/shared/static/wj1ln9xtigaoubbxow86y2gqmqcsu2jk.csv\") %>%\n  column_to_rownames(var = \"title\")\n\n\n\n\n\ncols <- c('#f6eff7','#bdc9e1','#67a9cf','#1c9099','#016c59')\nsuperheat(movies_mat, left.label.text.size = 4, heat.pal = cols, heat.lim = c(0, 5))\n\n\n\n\nJust like in adjacency matrix visualizations, the effectiveness of a heatmap can depend dramatically on the way in which rows and columns are ordered. To provide a more coherent view, we cluster both rows and columns, placing rows / columns belonging to the same cluster next to one another.\n\n\nmovies_clust <- movies_mat %>%\n  kmeans(centers = 10)\n\nusers_clust <- movies_mat %>%\n  t() %>%\n  kmeans(centers = 10)\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  order.rows = order(movies_clust$cluster),\n  order.cols = order(users_clust$cluster),\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\n\nsuperheat also makes it easy to visualize plot statistics adjacent ot the adjacent to the main heatmap. These statistics can be plotted as points, lines, or bars. Points are useful when we want to highlight the raw value, lines are effective for showing change, and bars give a sense of the area below a set of observations. In this example, we use an added panel on the right hand side (yr) to encode the total number of ratings given to that movie. The yr.obs.cols allows us to change the color of each point in the adjacent plot. In this example, we change color depending on which cluster the movie was found to belong to.\n\n\ncluster_cols <- c('#8dd3c7','#ccebc5','#bebada','#fb8072','#80b1d3','#fdb462','#b3de69','#fccde5','#d9d9d9','#bc80bd')\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  order.rows = order(movies_clust$cluster),\n  order.cols = order(users_clust$cluster),\n  heat.pal = cols,\n  heat.lim = c(0, 5),\n  yr = rowSums(movies_mat > 0),\n  yr.axis.name = \"Number of Ratings\",\n  yr.obs.col = cluster_cols[movies_clust$cluster],\n  yr.plot.type = \"bar\"\n)\n\n\n\n\nIt also makes sense to order the rows / columns using hierarchical clustering. This approach is especially useful when the samples fall along a continuous gradient, rather than belonging to clearly delineated groups. The pretty.order.rows and pretty.order.cols arguments use hierarchical clustering to reorder the heatmap.\n\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  pretty.order.cols = TRUE,  \n  pretty.order.rows = TRUE,\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\n\nThe hierarchical clustering trees estimated by pretty.order.rows and pretty.order.cols can be also visualized.\n\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  pretty.order.cols = TRUE,  \n  pretty.order.rows = TRUE, \n  row.dendrogram = TRUE,\n  col.dendrogram = TRUE,\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-17-week9-3/week9-3_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-20T11:16:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-17-week9-2/",
    "title": "Hierarchical Clustering",
    "description": "Clustering data at multiple scales using trees.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-22",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"ggraph\")\nlibrary(\"knitr\")\nlibrary(\"readr\")\nlibrary(\"robservable\")\nlibrary(\"tidygraph\")\ntheme_set(theme_graph())\n\n\n\nIn reality, data are rarely separated into a clear number of homogeneous clusters. More often, even once a cluster formed, it’s possible to identify a few subclusters. For example, if you initially clustered movies into “drama” and “scifi”, you might be able to further refine the scifi cluster into “time travel” and “aliens.”\n\\(K\\)-means only allows clustering at a single level of magnification. To instead simultaneously cluster across scales, you can use an approach called hierarchical clustering. As a first observation, note that a tree can be used to implicitly store many clusterings at once. You can get a standard clustering by cutting the tree at some level.\n\n\n\nFigure 1: We can recover clusters at different levels of granularity, by cutting a hierarchical clustering tree.\n\n\n\nThese hierarchical clustering trees can be thought of abstract versions of the taxonomic trees. Instead of relating species, they relate observations in a dataset.\n\n\nrobservable(\"@mbostock/tree-of-life\", height = 1150)\n\n\n\n{\"x\":{\"notebook\":\"@mbostock/tree-of-life\",\"include\":null,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nElaborating on this analogy, the leaves of a hierarchical clustering tree are the original observations. The more recently two nodes share a common ancestor, the more similar those observations are.\nThe specific algorithm proceeds as follows,\nInitialize: Associate each point with a cluster \\(C_i := \\{x_i\\}\\).\nIterate until only one cluster: Look at all pairs of clusters. Merge the pair \\(C_k, C_{k^{\\prime}}\\) which are the most similar.\n\n\n\n\nFigure 2: At initialization, the hierarchical clustering routine has a cluster for each observation.\n\n\n\n\n\n\nFigure 3: Next, the two closest observations are merged into one cluster. This is the first merge point on the tree.\n\n\n\n\n\n\nFigure 4: We continue this at the next iteration, though this time we have compute the pairwise distance between all clusters, not observations (technically, all the observations were their own cluster at the first step, and in both cases, we compare the pairwise distances between clusters).\n\n\n\n\n\n\nFigure 5: We can continue this process…\n\n\n\n\n\n\nFigure 6: … and eventually we will construct the entire tree.\n\n\n\n\n\n\nIn R, this can be accomplished by using the hclust function. First, we compute the distances between all pairs of observations (this provides the similarities used in the algorithm). Then, we apply hclust to the matrix of pairwise distances.\nWe apply this to a movie ratings dataset. Movies are considered similar if they tend to receive similar ratings across all audience members. The result is visualized below.\n\n\nmovies_mat <- read_csv(\"https://uwmadison.box.com/shared/static/wj1ln9xtigaoubbxow86y2gqmqcsu2jk.csv\")\n\nD <- movies_mat %>%\n  column_to_rownames(var = \"title\") %>%\n  dist()\n\nhclust_result <- hclust(D)\nplot(hclust_result, cex = 0.5)\n\n\n\n\nWe can customize our tree visualization using the ggraph package. We can convert the hclust object into a ggraph, using the same as_tbl_graph function from the network and trees lectures.\n\n\nhclust_graph <- as_tbl_graph(hclust_result, height = height)\nhclust_graph <- hclust_graph %>%\n  mutate(height = ifelse(height == 0, 27, height)) # shorten the final edge\nhclust_graph\n\n\n# A tbl_graph: 99 nodes and 98 edges\n#\n# A rooted tree\n#\n# Node Data: 99 x 4 (active)\n  height leaf  label                       members\n   <dbl> <lgl> <chr>                         <int>\n1   27   TRUE  \"Schindler's List\"                1\n2   27   TRUE  \"Forrest Gump\"                    1\n3   27   TRUE  \"Shawshank Redemption, The\"       1\n4   27   TRUE  \"Pulp Fiction\"                    1\n5   27   TRUE  \"Silence of the Lambs, The\"       1\n6   58.7 FALSE \"\"                                2\n# … with 93 more rows\n#\n# Edge Data: 98 x 2\n   from    to\n  <int> <int>\n1     6     4\n2     6     5\n3     7     3\n# … with 95 more rows\n\n\n\nggraph(hclust_graph, \"dendrogram\", height = height, circular = TRUE) +\n  geom_edge_elbow() +\n  geom_node_text(aes(label = label), size = 4) +\n  coord_fixed()\n\n\n\n\nWe can cut the tree to recover a standard clustering. This is where the grammar-of-graphics approach from ggraph becomes useful – we can encode the cluster membership of a movie using color, for example.\n\n\ncluster_df <- cutree(hclust_result, k = 10) %>% # try changing K and regenerating the graph below\n  tibble(label = names(.), cluster = as.factor(.))\ncluster_df\n\n\n# A tibble: 50 x 3\n       . label                cluster\n   <int> <chr>                <fct>  \n 1     1 Seven (a.k.a. Se7en) 1      \n 2     1 Usual Suspects, The  1      \n 3     2 Braveheart           2      \n 4     2 Apollo 13            2      \n 5     3 Pulp Fiction         3      \n 6     4 Forrest Gump         4      \n 7     2 Lion King, The       2      \n 8     2 Mask, The            2      \n 9     2 Speed                2      \n10     2 Fugitive, The        2      \n# … with 40 more rows\n\n\n\n# colors chosen using https://medialab.github.io/iwanthue/\ncols <- c(\"#51b48c\", \"#cf3d6e\", \"#7ab743\", \"#7b62cb\", \"#c49644\", \"#c364b9\", \"#6a803a\", \"#688dcd\", \"#c95a38\", \"#c26b7e\")\nhclust_graph %>%\n  left_join(cluster_df) %>%\n  ggraph(\"dendrogram\", height = height, circular = TRUE) +\n  geom_edge_elbow() +\n  geom_node_text(aes(label = label, col = cluster), size = 4) +\n  coord_fixed() +\n  scale_color_manual(values = cols) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-17-week9-2/week9-2_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-03-20T11:16:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-17-week9-1/",
    "title": "K-means",
    "description": "An introduction to clustering and how to manage its output.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-21",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dslabs\")\nlibrary(\"ggplot2\")\nlibrary(\"knitr\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyr\")\nlibrary(\"dplyr\")\ntheme_set(theme_minimal())\n\n\n\nThe goal of clustering is to discover distinct groups within a dataset. In an ideal clustering, samples are very different between groups, but relatively similar within groups. At the end of a clustering routine, \\(K\\) clusters have been identified, and each sample is assigned to one of these \\(K\\) clusters. \\(K\\) must be chosen by the user.\nClustering gives a compressed representation of the dataset. Therefore, clustering is useful for getting a quick overview of the high-level structure in a dataset.\nFor example, clustering can be used in the following applications,\nUser segmentation: A marketing study might try to summarize different user behaviors by clustering into a few key “segments.” Each segment might motivate a different type of marketing campaign.\nGene expression profiling: A genomics study might try to identify genes whose expression levels are similar across different experimental conditions. These gene clusters might be responsible for a shared biological function.\n\n\\(K\\)-means is a particular algorithm for finding clusters. First, it randomly initializes \\(K\\) cluster centroids. Then, it alternates the following two steps until convergence,\nAssign points to their nearest cluster centroid.\nUpdate the \\(K\\) centroids to be the averages of points within their cluster.\n\nHere is an animation from the tidymodels page on \\(K\\)-means,\n\n\n\nNote that, since we have to take an average for each coordinate, we require that our data be quantitative, not categorical1.\nWe illustrate this idea using the movielens dataset from the reading. This dataset has ratings (0.5 to 5) given by 671 users across 9066 movies. We can think of this as a matrix of movies vs. users, with ratings within the entries. For simplicity, we filter down to only the 50 most frequently rated movies. We will assume that if a user never rated a movie, they would have given that movie a zero2. We’ve skipped a few steps used in the reading (subtracting movie / user averages and filtering to only active users), but the overall results are comparable.\n\n\ndata(\"movielens\")\nfrequently_rated <- movielens %>%\n  group_by(movieId) %>%\n  summarize(n=n()) %>%\n  top_n(50, n) %>%\n  pull(movieId)\n\nmovie_mat <- movielens %>% \n  filter(movieId %in% frequently_rated) %>%\n  select(title, userId, rating) %>%\n  pivot_wider(title, names_from = userId, values_from = rating, values_fill = 0)\n\nmovie_mat[1:10, 1:20]\n\n\n# A tibble: 10 x 20\n   title     `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Seven …     4   0       0     0     0     0   5       3     0     0\n 2 Usual …     4   0       0     0     0     0   5       0     5     5\n 3 Braveh…     4   4       0     0     0     5   4       0     0     0\n 4 Apollo…     5   0       0     4     0     0   0       0     0     0\n 5 Pulp F…     4   4.5     5     0     0     0   4       0     0     5\n 6 Forres…     3   5       5     4     0     3   4       0     0     0\n 7 Lion K…     3   0       5     4     0     3   0       0     0     0\n 8 Mask, …     3   0       4     4     0     3   0       0     0     0\n 9 Speed       3   2.5     0     4     0     3   0       0     0     0\n10 Fugiti…     3   0       0     0     0     0   4.5     0     0     0\n# … with 9 more variables: 12 <dbl>, 13 <dbl>, 14 <dbl>, 15 <dbl>,\n#   16 <dbl>, 17 <dbl>, 18 <dbl>, 19 <dbl>, 20 <dbl>\n\nNext, we run kmeans on this dataset. I’ve used the dplyr pipe notation to run kmeans on the data above with “title” removed. augment is a function from the tidymodels package that adds the cluster labels identified by kmeans to the rows in the original dataset.\n\n\nkclust <- movie_mat %>%\n  select(-title) %>%\n  kmeans(centers = 10)\n\nmovie_mat <- augment(kclust, movie_mat) # creates column \".cluster\" with cluster label\nkclust <- tidy(kclust)\n\nmovie_mat %>%\n  select(title, .cluster) %>%\n  arrange(.cluster)\n\n\n# A tibble: 50 x 2\n   title                .cluster\n   <chr>                <fct>   \n 1 American Beauty      1       \n 2 Godfather, The       1       \n 3 Seven (a.k.a. Se7en) 2       \n 4 Usual Suspects, The  2       \n 5 Pulp Fiction         2       \n 6 Braveheart           3       \n 7 Apollo 13            3       \n 8 Speed                3       \n 9 Fugitive, The        3       \n10 Jurassic Park        3       \n# … with 40 more rows\n\nThere are two pieces of derived data generated by this routine,\nThe cluster assignments\nThe cluster centroids and both can be the subjects of visualization.\n\nIn our movie example, the cluster centroids are imaginary pseudo-movies that are representative of their cluster. They are represented by the scores they would have received by each of the users in the dataset. This is visualized below. In a more realistic application, we would also want to display some information about each user; e.g., maybe some movies are more popular among certain age groups or in certain regions.\n\n\nkclust_long <- kclust %>%\n  pivot_longer(`2`:`671`, names_to = \"userId\", values_to = \"rating\")\n\nggplot(kclust_long) +\n  geom_bar(\n    aes(x = reorder(userId, rating), y = rating),\n    stat = \"identity\"\n  ) +\n  facet_grid(cluster ~ .) +\n  labs(x = \"Users (sorted)\", y = \"Rating\") +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 5),\n    strip.text.y = element_text(angle = 0)\n  )\n\n\n\n\nFigure 1: We can visualize each cluster by seeing the average ratings each user gave to the movies in that cluster (this is the definition of the centroid). An alternative visualization strategy would be to show a heatmap – we’ll discuss this soon in the superheat lecture.\n\n\n\nIt’s often of interest to relate the cluster assignments to complementary data, to see whether the clustering reflects any previously known differences between the observations, which weren’t directly used in the clustering algorithm.\nBe cautious: Outliers, nonspherical shapes, and variations in density can throw off \\(K\\)-means.\n\n\n\nFigure 2: The difficulty that variations in density poses to k-means, from Cluster Analysis using K-Means Explained.\n\n\n\nThe goals of clustering are highly problem dependent, and different goals might call for alternative algorithms. For example, consider the ways clustering might be used to understand disease transmission. One problem might be to cluster the DNA sequences of the pathogenic agent, to recover its evolutionary history. This could be done using hierarchical clustering (next lecture). A second problem might be to determine whether patient outcomes might be driven by one of a few environmental factors, in which case a \\(K\\)-means clustering across the typical environmental factors would be reasonable. A third use would be to perform contact tracing, based on a network clustering algorithm. The point is that no clustering algorithm is uniformly better than any other in all situations, and the choice of which one to use should be guided by the problem requirements.\n\nTo work with data of different types, it’s possible to use a variant called \\(K\\)-medoids, but this is beyond the scope of this class.↩︎\nThis is an approximation – there are probably many movies that the users would have enjoyed had they had a chance to watch them.↩︎\n",
    "preview": "posts/2021-03-17-week9-1/week9-1_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-03-20T11:16:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-06-week8-4/",
    "title": "Enclosure",
    "description": "Visualization of hierarchical structure using containment.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-16",
    "categories": [],
    "contents": "\nReading (Chapter 9), Recording, Rmarkdown\n\n\nlibrary(\"ggplot2\")\nlibrary(\"ggraph\")\nlibrary(\"dplyr\")\nlibrary(\"tidygraph\")\nlibrary(\"gridExtra\")\ntheme_set(theme_graph())\n\n\n\nIf nodes can be conceptually organized into a hierarchy, then it’s possible to use enclosure (i.e., the containment of some visual marks within others) to encode those relationships.\n\n\ngraph <- tbl_graph(flare$vertices, flare$edges)\n\np1 <- ggraph(graph, \"tree\") +\n  geom_edge_link() +\n  geom_node_point(aes(size = size)) +\n  scale_size(range = c(0.1, 5))\np2 <- ggraph(graph, \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = depth)) +\n  scale_fill_distiller(direction = 1) +\n  coord_fixed()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nFigure 1: A tree and the equivalent representation using containment. The outer circle corresponds to the root node in the tree, and paths down the tree are associated with sequences of nested circles.\n\n\n\nHierarchy most obviously occurs in trees, but it can also be present in networks with clustering structure at several levels. (see point 6 below).\nEnclosure is used in treemaps. In this visualization, each node is allocated an area, and all its children are drawn within that area (and so on, recursively, down to the leaves).\n\n\nggraph(graph, \"treemap\", weight = size) +\n  geom_node_tile(aes(fill = depth, size = depth), size = 0.25) +\n  scale_fill_distiller(direction = 1) +\n  coord_fixed()\n\n\n\n\nFigure 2: A treemap representation associated with the tree from above.\n\n\n\nThis is a particularly useful visualization when it’s important to visualize a continuous attribute associated with each node. For example, a large node might correspond to a large part of a budget or a large directory in a filesystem. Here is an example visualization of Obama’s budget proposal in 2013.\nA caveat: treemaps are not so useful for making topological comparisons, like the distance between two nodes in the tree.\nIn situations where network nodes can be organized hierarchically, containment marks can directly represent the these relationships. For example, in the network below, the red and yellow clusters are contained in a green supercluster. The combination of node-link diagram and containment sets makes this structure clear.\n\n\n\nFigure 3: An example of how hierarchy across groups of nodes can be encoded within a network.\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-06-week8-4/week8-4_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-11T13:00:01-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-06-week8-3/",
    "title": "Adjacency Matrix Views",
    "description": "A scalable network visualization strategy.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\nReading (Chapter 9), Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"knitr\")\nlibrary(\"ggplot2\")\nlibrary(\"gridExtra\")\nlibrary(\"igraph\")\nlibrary(\"readr\")\nlibrary(\"ggraph\")\nlibrary(\"tidygraph\")\ntheme_set(theme_graph())\n\n\n\nThe adjacency matrix of an undirected graph is the matrix with a 1 in entry \\(ij\\) if nodes \\(i\\) and \\(j\\) are linked by an edge and 0 otherwise. It has one row and one column for every node in the graph. Visually, these 1’s and 0’s can be encoded as a black and white squares.\nThe example below shows the node-link and adajcency matrix plots associated with the toy example from the first lecture.\n\n\nE <- matrix(c(1, 3, 2, 3, 3, 4, 4, 5, 5, 6),\n  byrow = TRUE, ncol = 2) %>%\n  as_tbl_graph() %>%\n  mutate(\n    id = row_number(),\n    group = id < 4\n  ) \n\np1 <- ggraph(E, layout = 'kk') + \n    geom_edge_fan() +\n    geom_node_label(aes(label = id))\n\np2 <- ggraph(E, \"matrix\") +\n    geom_edge_tile(mirror = TRUE, show.legend = TRUE) +\n    geom_node_text(aes(label = id), x = -.5, nudge_y = 0.5) +\n    geom_node_text(aes(label = id), y = 0.5, nudge_x = -0.5) +\n    scale_y_reverse(expand = c(0, 0, 0, 1.5)) + # make sure the labels aren't hidden\n    scale_x_discrete(expand = c(0, 1.5, 0, 0)) +\n    coord_fixed() # make sure squares, not rectangles\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nIn a directed graph, the \\(ij^{th}\\) entry is set to 1 if node \\(i\\) leads directly to node \\(j\\). Unlike the undirected graph, it is not necessarily symmetric.\nThe example below shows the adjacency matrix associated with the high-school student friendship network from last lecture. This is a directed graph, because the students were surveyed about their closest friends, and this was not always symmetric.\n\n\nG_school <- as_tbl_graph(highschool) %>%\n  activate(edges) %>%\n  mutate(year = factor(year))\n\nggraph(G_school, \"matrix\") +\n  geom_edge_tile() +\n  coord_fixed() +\n  facet_wrap(~ year)\n\n\n\n\nNode properties can be encoded along the rows and columns of the matrix. Edge properties can be encoded by the color or opacity of cells in the matrix. The example below simulates clustered data and draws the cluster label for each node along rows / columns.\n\n\nG <- sample_islands(4, 40, 0.4, 15) %>%\n  as_tbl_graph() %>%\n  mutate(\n    group = rep(seq_len(4), each = 40),\n    group = as.factor(group)\n  )\n\nggraph(G, \"matrix\") +\n  geom_node_point(aes(col = group), x = -1) +\n  geom_node_point(aes(col = group), y = 1) +\n  geom_edge_tile(mirror = TRUE) +\n  scale_y_reverse() +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_fixed()\n\n\n\n\nThe example below takes a protein network and colors each edge according to the weight of experimental evidence behind that interaction.\n\n\nproteins <- read_tsv(\"https://uwmadison.box.com/shared/static/t97v5315isog0wr3idwf5z067h4j9o7m.tsv\") %>%\n  as_tbl_graph(from = node1, to = node2)\n\nggraph(proteins, \"matrix\") +\n  geom_edge_tile(aes(fill = combined_score), mirror = TRUE) +\n  coord_fixed() +\n  geom_node_text(aes(label = name), size = 3, x = -2.5, nudge_y = 0.5, hjust = 0) +\n  geom_node_text(aes(label = name), size = 3, angle = 90, y = 0.5, nudge_x = -0.5, hjust = 0) +\n  scale_y_reverse(expand = c(0, 0, 0, 2.7)) + # make sure the labels aren't hidden\n  scale_x_discrete(expand = c(0, 3, 0, 0)) +\n  scale_edge_fill_distiller() +\n  labs(edge_fill = \"Edge Confidence\")\n\n\n\n\nThe key advantage of visualization using adjacency matrices is that they can scale to large and dense networks. It’s possible to perceive structure even when the squares are quite small, and there is no risk of edges overlapping with one another.\nAdjacency matrices can be as useful as node-link diagrams for the task of finding group structure. The example below shows how cliques (fully connected subsets), bicliques (sets of nodes that are fully connected between one another), and clusters (densely, but not completely, connected subsets) are clearly visible in both node-link and adjacency matrix visualizations.\n\n\n\nFigure 1: Cliques, bicliques, and clusters are visually salient using either node-link or adjacency matrix representations of a matrix.\n\n\n\nThe key disadvantage of adjacency matrix visualization is that it’s challenging to make sense of the local topology around a node. Finding the « friends of friends » of a node requires effort.\n\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nAnother issue is that different orderings of rows and columns can have a dramatic effect on the structure that’s visible1. For example, here is the same clustered network from 5., but with a random reordering of rows and columns. This is also beautifully illustrated at this page (change the “order” dropdown menu).\n\n\nggraph(G, \"matrix\", sort.by = sample(1:160)) +\n  geom_node_point(aes(col = group), x = -1) +\n  geom_node_point(aes(col = group), y = 1) +\n  geom_edge_tile(mirror = TRUE) +\n  scale_y_reverse() +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_fixed()\n\n\n\n\nThese visualizations also tend to be less intuitive for audiences that haven’t viewed them before.\nIn a sense, adjacency matrix and node-link visualizations are complementary to one another. Some approaches try to use the adjacency matrix to give a global view of a network, and dynamic queries on the adjacency matrix can reveal the associated, local node-link diagrams.\n\nThough viewed differently, this additional degree of freedom can facilitate richer comparisons.↩︎\n",
    "preview": "posts/2021-03-06-week8-3/week8-3_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-11T13:00:10-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-06-week8-2/",
    "title": "Node - Link Diagrams",
    "description": "The most common network visualization strategy.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-14",
    "categories": [],
    "contents": "\nReading (Chapter 9), Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"knitr\")\nlibrary(\"ggplot2\")\nlibrary(\"ggraph\")\nlibrary(\"gridExtra\")\nlibrary(\"networkD3\")\nlibrary(\"tidygraph\")\ntheme_set(theme_graph())\n\n\n\nA node-link diagram is a visual encoding strategy for network data, where nodes are drawn as points and links between nodes are drawn as lines between them. The dataset below is a friendship network derived from a survey of high schoolers in 1957 and 1958, available in the tidygraph package.\n\n\nG_school <- as_tbl_graph(highschool) %>%\n  activate(edges) %>%\n  mutate(year = factor(year))\n\nggraph(G_school) +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\n\n\n\nFor trees, the vertical or radial position can further encode the depth of a node in the tree. The data below represent the directory structure from a widely used web package called flare.\n\n\nG_flare <- tbl_graph(flare$vertices, flare$edges)\np1 <- ggraph(G_flare, 'tree') + \n  geom_edge_link() +\n  geom_node_label(aes(label = shortName), size = 3)\n\np2 <- ggraph(G_flare, 'tree', circular = TRUE) + \n  geom_edge_link() +\n  geom_node_label(aes(label = shortName), size = 3)\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nFigure 1: The same node-link diagram, with either height or radius encoding depth in the tree.\n\n\n\nIn either trees or networks, attributes of nodes and edges can be encoded using size (node radius or edge width) or color.\nThe node-link representation is especially effective for the task of following paths. It’s an intuitive visualization for examining the local neighborhood of one node or describing the shortest path between two nodes.\nIn node-link diagrams, spatial position is subtle. It does not directly encode any attribute in the dataset, but layout algorithms (i.e., algorithms that try to determine the spatial positions of nodes in a node-link diagram) try to ensure that nodes that are close to one another in the shortest-path-sense also appear close to one another on the page.\n\n\np1 <- ggraph(G_school, layout = \"kk\") +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\np2 <- ggraph(G_school, layout = \"fr\") +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nFigure 2: A comparison of two layout algorithms for the same network.\n\n\n\nOne common layout algorithm uses force-directed placement. The edges here are interpreted as physical springs, and the node positions are iteratively updated according to the forces induced by the springs.\nHere is an interactive force-directed layout of the network from above, generated using the networkD3 package.\n\n\nschool_edges <- G_school %>%\n  activate(edges) %>%\n  as.data.frame()\nsimpleNetwork(school_edges)\n\n\n\n{\"x\":{\"links\":{\"source\":[0,0,0,0,0,11,11,22,22,33,33,33,33,44,44,55,55,55,66,68,68,69,69,69,69,69,1,1,1,1,2,2,2,3,3,3,3,4,4,5,6,6,6,7,7,8,8,8,9,9,9,9,9,10,10,10,10,10,12,12,12,12,13,13,13,13,14,14,14,14,14,14,14,14,14,15,16,16,16,16,16,16,17,17,17,18,18,19,19,20,20,21,21,21,23,23,23,23,24,24,24,24,25,25,26,26,27,27,27,27,28,28,29,29,29,29,30,30,30,30,30,31,31,32,32,32,32,34,34,34,34,35,35,35,36,37,37,37,37,37,38,38,38,38,39,39,39,39,40,40,40,40,41,41,41,41,42,42,42,42,43,43,43,45,45,45,45,45,46,46,47,47,47,47,47,47,48,48,48,49,49,49,50,50,50,50,50,50,51,51,52,52,52,53,53,53,53,54,54,56,56,57,57,57,57,57,58,58,58,58,59,59,59,60,60,60,60,60,61,61,61,61,61,62,62,62,62,63,63,63,63,63,64,64,64,64,64,65,65,65,65,65,65,0,0,0,11,11,33,33,33,33,33,44,44,44,55,55,55,55,55,66,66,66,68,68,68,69,69,69,69,67,67,1,1,1,1,1,2,2,2,3,3,3,3,3,4,4,5,5,5,5,6,6,6,7,7,7,7,7,8,9,9,9,9,9,10,10,10,10,10,12,12,12,12,12,13,13,13,13,13,14,14,14,14,14,15,16,16,16,16,16,17,17,17,17,17,18,18,18,19,19,19,20,20,21,21,21,21,23,23,23,23,23,23,24,24,24,24,25,27,27,27,27,27,28,28,28,29,29,29,29,30,30,30,30,30,31,31,31,32,32,32,32,32,34,34,34,35,35,35,35,36,36,37,37,37,37,37,38,38,38,38,38,39,40,40,40,40,40,41,41,41,41,41,42,42,42,43,43,43,43,43,45,45,45,46,46,46,46,47,47,47,47,47,48,48,48,48,48,49,49,49,49,49,50,50,50,50,51,51,52,52,52,52,52,53,53,53,53,53,54,54,54,56,56,57,57,57,57,57,58,58,58,59,59,59,59,60,60,60,60,60,61,61,61,61,61,62,62,62,63,63,63,63,63,64,64,64,64,64,65,65,65,65,65],\"target\":[4,5,12,47,48,12,13,69,5,44,8,9,35,9,35,3,10,13,7,4,7,2,10,12,13,43,9,42,45,46,10,12,13,7,10,12,13,12,13,10,8,32,35,66,68,1,6,9,33,1,6,8,17,55,2,12,13,29,13,43,47,48,10,12,29,43,31,35,42,45,46,53,56,59,62,43,23,26,27,31,32,34,8,28,31,29,30,3,29,26,40,67,28,31,16,24,35,56,27,32,34,35,30,38,20,40,23,24,34,35,21,31,12,13,43,48,18,19,25,38,52,26,28,6,34,35,62,27,32,35,62,32,34,62,39,41,47,48,50,64,25,30,47,52,36,40,45,46,20,42,45,46,47,48,64,65,39,45,46,62,12,13,64,39,40,42,46,62,42,45,37,41,48,57,63,65,37,41,47,58,64,65,37,41,48,60,64,65,54,59,30,47,48,51,54,56,59,51,59,53,59,60,61,63,64,65,60,61,63,65,53,56,62,58,61,63,64,65,57,58,60,63,65,21,54,56,59,57,58,60,61,65,57,60,61,63,65,57,58,60,61,63,64,5,12,13,69,13,44,1,6,9,35,33,9,35,68,3,7,10,19,3,7,12,3,4,18,10,12,13,64,8,9,33,6,9,24,34,10,12,13,44,55,7,10,13,3,13,0,2,10,12,32,56,62,0,66,68,3,13,9,33,44,69,1,2,69,2,9,12,13,10,13,29,43,48,10,12,18,29,43,45,46,57,63,65,18,20,23,25,27,30,44,26,28,31,35,13,29,30,24,27,34,26,28,14,28,31,42,19,24,27,29,30,35,27,32,34,35,38,19,24,32,34,35,17,20,31,12,13,18,30,18,29,38,52,64,17,20,28,6,24,34,35,62,19,32,35,24,32,34,62,39,42,41,43,47,48,50,30,41,43,47,48,45,37,41,43,47,48,39,43,45,46,48,14,45,46,12,37,40,47,48,41,46,48,45,48,57,64,37,41,43,48,64,37,41,45,46,47,56,61,62,64,65,41,43,60,64,54,59,30,40,41,49,62,51,54,56,64,65,51,59,60,53,64,60,61,63,64,65,60,64,65,61,62,64,65,57,58,61,64,65,53,57,60,64,65,56,64,65,57,58,60,61,65,57,60,61,63,65,49,56,60,61,64],\"value\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"colour\":[\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\"]},\"nodes\":{\"name\":[\"1\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"2\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"3\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"4\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"5\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"6\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"7\",\"70\",\"8\",\"9\"],\"group\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"nodesize\":[8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8]},\"options\":{\"NodeID\":\"name\",\"Group\":\"group\",\"colourScale\":\"d3.scaleOrdinal(['#3182bd'])\",\"fontSize\":7,\"fontFamily\":\"serif\",\"clickTextSize\":17.5,\"linkDistance\":50,\"linkWidth\":\"'1.5px'.toString()\",\"charge\":-30,\"opacity\":0.6,\"zoom\":false,\"legend\":false,\"arrows\":false,\"nodesize\":true,\"radiusCalculation\":\"d.nodesize\",\"bounded\":false,\"opacityNoHover\":1,\"clickAction\":null}},\"evals\":[],\"jsHooks\":[]}\nThe key drawback of node-link diagrams is that they do not scale well to networks with a large number of nodes or with a large number of edges per node. The nodes and edges begin to overlap too much, and the result looks like a « hairball. »\nIn this situation, it is possible to use additional structure in the data to salvage the node-link display. For example, in a large tree, a rectangular or BubbleTree layout can be used.\n\n\n\nFigure 3: Example rectangular and BubbleTree layouts, for very large trees, as shown in Visualization Analysis and Design.\n\n\n\nIf a large network has a modular structure, then it is possible to first lay out the separate clusters far apart from one another, before running force directed placement.\n\n\n\nFigure 4: A hierarchical force directed layout algorithm, as shown in Visualization Analysis and Design.\n\n\n\nIf many edges go through a few shared paths, it may be possible to bundle them.\n\n\n\nFigure 5: In edge bundling, similar paths are placed close to one another.\n\n\n\nBundled connections can be visualized using the geom_conn_bundle geometry in ggraph. Before using this layout, it is necessary to have a hierarchy over all the nodes, since shared ancestry among connected nodes is how the proximity of paths is determined.\n\n\nfrom <- match(flare$imports$from, flare$vertices$name)\nto <- match(flare$imports$to, flare$vertices$name)\nggraph(G_flare, layout = 'dendrogram', circular = TRUE) + \n  geom_conn_bundle(data = get_con(from = from, to = to), alpha = 0.1) + \n  geom_node_label(aes(label = shortName), size = 2) +\n  coord_fixed()\n\n\n\n\nFigure 6: An example of hierarchical edge bundling in R.\n\n\n\nTo summarize, node-link diagrams are very good for characterizing local structure, but struggle with large networks.\n\n\n\n",
    "preview": "posts/2021-03-06-week8-2/week8-2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-11T13:00:27-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-06-week8-1/",
    "title": "Introduction to Networks and Trees",
    "description": "Typical tasks and example network datasets.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-13",
    "categories": [],
    "contents": "\nReading 1 (Chapter 9), Reading 2, Recording, Rmarkdown\n\n\nlibrary(\"ggplot2\")\nlibrary(\"ggraph\")\nlibrary(\"dplyr\")\nlibrary(\"tidygraph\")\ntheme_set(theme_graph())\n\n\n\nNetworks and trees can be used to represent information in a variety of contexts. Abstractly, networks and trees are types of graphs, which are defined by (a) a set \\(V\\) of vertices and (b) a set \\(E\\) of edges between pairs of vertices.\nIt is helpful to have a few specific examples in mind,\nThe Internet: \\(V = \\{\\text{All Webpages}\\}, \\left(v, v^{\\prime}\\right) \\in E\\) if there is a hyperlink between pages \\(v\\) and \\(v^{\\prime}\\).\nEvolutionary Tree: \\(V = \\{\\text{All past and present species}\\}, \\left(v,  v^{\\prime}\\right) \\in E\\) if one of the species \\(v\\) or \\(v^{\\prime}\\) is a descendant of the other.\nDisease Transmission: \\(V = \\{\\text{Community Members}\\}, \\left(v,  v^{\\prime}\\right) \\in E\\) if the two community members have come in close contact.\nDirectory Tree: \\(V = \\{\\text{All directories in a computer}\\}, \\left(v,  v^{\\prime}\\right) \\in E\\) if one directory is contained in the other.\n\n\n\n\nFigure 1: A visualization of the internet, from the Opte Project.\n\n\n\n\n\n\nFigure 2: An evolutionary tree, from the Interactive Tree of Life.\n\n\n\n\n\n\nFigure 3: A COVID-19 transmission network, from Clustering and superspreading potential of SARS-CoV-2 infections in Hong Kong.\n\n\n\n\n\n\nFigure 4: Directories in a file system can be organized into a tree, with parent and child directories.\n\n\n\nEither vertices or edges might have attributes. For example, in the directory tree, we might know the sizes of the files (vertex attribute), and in the disease transmission network we might know the duration of contact between individuals (edge attribute).\nAn edge may be either undirected or directed. In a directed edge, one vertex leads to the other, while in an undirected edge, there is no sense of ordering.\nIn R, the tidygraph package can be used to manipulate graph data. It’s tbl_graph class stores node and edge attributes in a single data structure. and ggraph extends the usual ggplot2 syntax to graphs.\n\n\nE <- data.frame(\n  source = c(1, 2, 3, 4, 5),\n  target = c(3, 3, 4, 5, 6)\n)\n\nG <- tbl_graph(edges = E)\nG\n\n\n# A tbl_graph: 6 nodes and 5 edges\n#\n# A rooted tree\n#\n# Node Data: 6 x 0 (active)\n#\n# Edge Data: 5 x 2\n   from    to\n  <int> <int>\n1     1     3\n2     2     3\n3     3     4\n# … with 2 more rows\n\nThis tbl_graph can be plotted using the code below. There are different geoms available for nodes and edges – for example, what happens if you replace geom_edge_link() with geom_edge_arc()?\n\n\nggraph(G, layout = 'kk') + \n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\nWe can mutate node and edge attributes using dplyr-like syntax. Before mutating edges, it’s necessary to call activate(edges).\n\n\nG <- G %>%\n  mutate(\n    id = row_number(),\n    group = id < 4\n  ) %>%\n  activate(edges) %>%\n  mutate(width = runif(n()))\nG\n\n\n# A tbl_graph: 6 nodes and 5 edges\n#\n# A rooted tree\n#\n# Edge Data: 5 x 3 (active)\n   from    to width\n  <int> <int> <dbl>\n1     1     3 0.609\n2     2     3 0.218\n3     3     4 0.667\n4     4     5 0.449\n5     5     6 0.379\n#\n# Node Data: 6 x 2\n     id group\n  <int> <lgl>\n1     1 TRUE \n2     2 TRUE \n3     3 TRUE \n# … with 3 more rows\n\nNow we can visualize these derived attributes using an aesthetic mapping within the geom_edge_link and geom_node_point geoms.\n\n\nggraph(G, layout = \"kk\") +\n  geom_edge_link(aes(width = width)) +\n  geom_node_label(aes(label = id))\n\n\n\n\nFigure 5: The same network as above, but with edge size encoding the weight attribute.\n\n\n\nExample Tasks\nWhat types of data that are amenable to representation by networks or trees? What visual comparisons do networks and trees facilitate?\nOur initial examples suggest that trees and networks can be used to represent either physical interactions or conceptual relationships. Typical tasks include,\nSearching for groupings\nFollowing paths\nIsolating key nodes\n\nBy “searching for groupings,” we mean finding clusters of nodes that are highly interconnected, but which have few links outside the cluster. This kind of modular structure might lend itself to deeper investigation within each of the clusters.\nClusters in a network of political blogs might suggest an echo chamber effect.\nGene clusters in a differential expression study might suggest pathways needed for the production of an important protein.\nClusters in a recipe network could be used identify different culinary techniques or cuisines.\n\n\n\n\nFigure 6: A representation of 1200 blogs before the 2004 election, from The political blogosphere and the 2004 US election: divided they blog.\n\n\n\nBy “following paths,” we mean tracing the paths out from a particular node, to see which other nodes it is close to.\nFollowing paths in a citation network might reveal the chain of publications that led to an important discovery.\nFollowing paths in a recommendation network might suggest other users who might be interested in watching a certain movie.\n\n\n\n\nFigure 7: A recommendation network, linking individuals and the movies that they viewed.\n\n\n\n“Isolating key nodes” is a more fuzzy concept, usually referring to the task of finding nodes that are exceptional in some way. For example, it’s often interesting to find nodes with many more connections than others, or which link otherwise isolated clusters.\nA node with many edges in a disease transmission network is a superspreader.\nA node that links two clusters in a citation network might be especially interdisciplinary.\nA node with large size in a directory tree might be a good target for reducing disk usage.\n\n\n\n\nFigure 8: The scientific journal, Social Networks, links several publication communities, as found by Betweenness Centrality as an Indicator of the Interdisciplinarity of Scientific Journals.\n\n\n\nIf you find these questions interesting, you might enjoy the catalog of examples on the website VisualComplexity.\n\n\n\n",
    "preview": "posts/2021-03-06-week8-1/week8-1_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-03-11T13:00:43-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-02-week7-5/",
    "title": "Spatial Data Interaction",
    "description": "Some strategies for interactively visualizing spatial data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-12",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"robservable\")\nlibrary(\"leaflet\")\nlibrary(\"readr\")\n\n\n\nThese are optional notes. The reading does not cover interactivity in spatial data contexts, and there will be no problems in this class related to interactive spatial data visualization. However, these notes may be useful if your project involves interaction with spatial data.\nIn vega-lite, vector data can be visualized using vl.markGeoshape. Any fields contained within the vector dataset can be visually encoded in the marks. This is analogous to geom_sf in ggplot2.\nvl.markGeoshape({fill: \"#e0e0e0\", stroke: \"#f7f7f7\", strokeWidth: 1})\n    .data(vector_data) // contains the vector data boundaries\n    .project(vl.projection('albersUsa'))\n    .render()\n\n\nrobservable(\"@krisrs1128/minimal-geoshape\", 3, height = 300)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/minimal-geoshape\",\"include\":3,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nSpatial data are often amenable to dynamic linking. For example, selections in a complementary figure can be used to query points on a map. Here are two examples from public observable notebooks. The first, by Chanwut Kittivorawong, updates the cities displayed on a map using a brush applied to histogram data. Conceptually, it is like the NYC house prices homework problem, except it includes several histogram selectors and draws a background map for context.\n\n\nrobservable(\"@chanwutk/happiness-of-each-country\", 6, height = 750)\n\n\n\n{\"x\":{\"notebook\":\"@chanwutk/happiness-of-each-country\",\"include\":6,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nThe second, by Alissa Chen, shows how the amount of daylight varies across cities, depending on time of the year. Brushing over the time series filters the data that are used to determine the circle widths (the little suns) on top of the map.\n\n\nrobservable(\"@alchan/whats-the-average-amount-of-sunlight-in-a-city-over-a-short-te\", 3, height = 730)\n\n\n\n{\"x\":{\"notebook\":\"@alchan/whats-the-average-amount-of-sunlight-in-a-city-over-a-short-te\",\"include\":3,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nAlternatively, selections on the map can highlight elements in a complementary figure. For example, hovering over geographic elements might highlight series in a time series plot or update a barchart placed on the side. Vega-lite doesn’t support selections based on vector data elements, only points that are overlaid on the map. However, these sorts of selections can be made with d3.\n\n\nrobservable(\"@krisrs1128/geojson-mouseover\", 7)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/geojson-mouseover\",\"include\":7,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nSometimes it’s useful to add an interactive map background to a vector dataset. In this case, the leaflet package in R can be useful. In the code block below, addTiles fetches the background map. addCircles overlays the new vector features on top of the map. It’s worth noting that the vector features were created automatically – there was no need to create or read in any type of sf object.\n\n\ncities <- read_csv(\"https://uwmadison.box.com/shared/static/j98anvdoasfb1h651qxzrow2ua45oap1.csv\")\nleaflet(cities) %>% \n  addTiles() %>% \n  addCircles(\n    lng = ~Long, \n    lat = ~Lat, \n    radius = ~sqrt(Pop) * 30\n  )\n\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addCircles\",\"args\":[[42.3601,41.7627,40.7127,39.95,40.4397,41.8236],[-71.0589,-72.6743,-74.0059,-75.1667,-79.9764,-71.4222],[24111.6030159755,10607.3229421942,86979.3078841169,37385.8261912185,16590.8679700611,12656.8005435813],null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2},null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null,null]}],\"limits\":{\"lat\":[39.95,42.3601],\"lng\":[-79.9764,-71.0589]}},\"evals\":[],\"jsHooks\":[]}\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-04T15:53:35-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-02-week7-4/",
    "title": "Coordinate Reference Systems",
    "description": "The projection problem, and how to check your CRS.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"ggplot2\")\nlibrary(\"sf\")\ntheme_set(theme_minimal())\n\n\n\nAn important subtlety of geographic data visualization is that all our maps are in 2D, but earth is 3D. The process of associated points on the earth with 2D coordinates is called « projection. » All projections introduce some level of distortion, and there is no universal, ideal projection.\nHere are a few examples of famous global projections. There are also many projections designed to give optimal representations locally within a particular geographic area.\nThis means that there is no universal standard for how to represent coordinates on the earth, and it’s common to find different projections in practice. For example, the block below shows how North America gets projected according to two different CRSs.\n\n\n# some test projections, don't worry about this syntax\nmiller <- \"+proj=mill +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\nlambert <- \"+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n\nnorth_america <- world %>%\n  filter(continent == \"North America\")\n\nggplot(st_transform(north_america, miller)) + geom_sf() \n\n\n\nggplot(st_transform(north_america, lambert)) + geom_sf()\n\n\n\n\nBoth vector and raster spatial data will be associated with CRS’s. In either sf or raster objects, they can be accessed using the crs function.\nA common source of bugs is to use two different projections for the same analysis. In this class, we will always use the EPSG:4326 projection, which is what is used in most online maps. But in your own projects, you should always check that the projections are consistent across data sources. If you find an inconsistency, it will be important to « reproject » the data into the same CRS.\n\n\n\n",
    "preview": "posts/2021-03-02-week7-4/week7-4_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-04T15:53:32-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-02-week7-3/",
    "title": "Raster Data",
    "description": "Storing spatially gridded information in rasters.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"raster\")\nlibrary(\"RStoolbox\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\ntheme_set(theme_bw())\n\n\n\nThe raster data format is used to store spatial data that lie along regular grids. The values along the grid are stored as entries in the matrix. The raster object contains metadata that associates each entry in the matrix with a geographic coordinate.\nSince the data they must lie along a regular grid, rasters are most often used for continuously measured data, like elevation, temperature, population density, or landcover class.\nWe can create a single layer raster using the raster command. The code block below loads an elevation map measured by the space shuttle.\n\n\nf <- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nzion <- raster(f)\n\n\n\nTyping the name of the object shows the metadata associated with it (but not the actual grid values). We can see that the grid has 457 rows and 465 columns. We also see its spatial extent: The minimum and maximum longitude are both close to -113 and the latitudes are between 37.1 and 37.5. A quick google map search shows that this is located in Zion national park.\n\n\nzion\n\n\nclass      : RasterLayer \ndimensions : 457, 465, 212505  (nrow, ncol, ncell)\nresolution : 0.0008333333, 0.0008333333  (x, y)\nextent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : /Library/Frameworks/R.framework/Versions/4.0/Resources/library/spDataLarge/raster/srtm.tif \nnames      : srtm \nvalues     : 1024, 2892  (min, max)\n\n\n\nplot(zion)\n\n\n\n\nThe raster command also lets us create raster objects from scratch. For example, the code below makes a raster with increasing values in a 6 x 6 grid. Notice that we had to give a fake spatial extent.\n\n\ntest <- raster(\n  nrows = 6, ncols = 6, res = 0.5, \n  xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5,\n  vals = 1:36\n)\n\nplot(test)\n\n\n\n\nReal-world rasters typically have more than one layer of data. For example, you might measure both elevation and slope along the same spatial grid, which would lead to a 2 layer raster. Or, for satellite images, you might measure light at multiple wavelengths (usual RGB, plus infrared or thermal for example).\nMulti-layer raster data can be read in using brick. You can refer to particular layers in a multi-layer raster by using subset.\n\n\nf <- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nsatellite <- brick(f)\nsatellite\n\n\nclass      : RasterBrick \ndimensions : 1428, 1128, 1610784, 4  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs \nsource     : /Library/Frameworks/R.framework/Versions/4.0/Resources/library/spDataLarge/raster/landsat.tif \nnames      : landsat.1, landsat.2, landsat.3, landsat.4 \nmin values :      7550,      6404,      5678,      5252 \nmax values :     19071,     22051,     25780,     31961 \n\nBase R’s plot function supports plotting one layer of a raster at a time. To plot more than one layer in a multichannel image (like ordinary RGB images) you can use the plotRGB function.\n\n\nplotRGB(satellite)\n\n\n\n\nSometimes, it’s useful to overlay several visual marks on top of a raster image. In this case, the ggRGB function from the RStoolbox package can be used.\n\n\nsatellite <- projectRaster(satellite, crs = 4326)\nggRGB(satellite) +\n  geom_point(\n    data = data.frame(x = -113, y = 37.3), \n    aes(x = x, y = y), \n    col = \"red\", size = 10\n  )\n\n\n\n\nIf we want to visualize just a single layer using ggplot2, we can use geom_raster. This assumes that the data are in “tall” format, which can be accomplished by (1) subsetting to the relevant layer and (2) converting it to a data.frame. We use xy = TRUE to remember the original locations of each pixel.\n\n\nsat_df <- subset(satellite, 1) %>%\n  as.data.frame(xy = TRUE)\nggplot(sat_df) +\n  geom_raster(aes(x = x, y = y, fill = landsat.1)) +\n  scale_fill_gradient(low = \"white\", high = \"black\") +\n  coord_fixed()\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-02-week7-3/week7-3_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-03-04T15:53:30-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-02-week7-2/",
    "title": "Vector Data",
    "description": "Manipulating and visualizing spatial vector data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggmap\")\nlibrary(\"ggplot2\")\nlibrary(\"knitr\")\nlibrary(\"sf\")\nlibrary(\"spData\")\ntheme_set(theme_minimal())\n\n\n\nAs mentioned previously, vector data are used to store geometric spatial data. Specifically, there are 7 types of geometric information that are commonly used, as given in the figure below.\n\n\ninclude_graphics(\"sf-classes.png\")\n\n\n\n\nWe can construct these geometric objects from scratch. For example, starting from the defining coordinates, we can use st_point to create a point object,\n\n\n# make a point\np <- st_point(c(5, 2))\nplot(p)\n\n\n\n\nst_linestring to create a linestring,\n\n\n# make a line\nlinestring_matrix <- rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2))\np <- st_linestring(linestring_matrix)\nplot(p)\n\n\n\n\nand st_polygon to create a polygon.\n\n\n# make a polygon\npolygon_list <- list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\np <- st_polygon(polygon_list)\nplot(p)\n\n\n\n\nDifferent geometries can be combined into a geometry collection, using sfc.\n\n\npoint1 <- st_point(c(5, 2))\npoint2 <- st_point(c(1, 3))\npoints_sfc <- st_sfc(point1, point2)\nplot(points_sfc)\n\n\n\n\nReal-world vector datasets are more than just these geometries — they also associate each geometry with some additional information about each feature. We can add this information to the geometries above by associating each element with a row of a data.frame. This merging is accomplished by st_sf, using geometry to associate a raw st_geom each row of a data.frame.\n\n\nlnd_point <- st_point(c(0.1, 51.5))                \nlnd_geom <- st_sfc(lnd_point, crs = 4326)         \nlnd_attrib = data.frame(                        \n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n  )\n\nlnd_sf = st_sf(lnd_attrib, geometry = lnd_geom)\n\n\n\nVisualization\nVector data can be directly plotted using base R. For example, suppose we want to plot the boundaries of India, within it’s local context. We can use the world dataset, provided by the spData package. Each row of the world object contains both the boundary of a country (in the geom column) and information about its location and population characteristics.\n\n\ndata(world)\nhead(world)\n\n\nSimple feature collection with 6 features and 10 fields\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\ngeographic CRS: WGS 84\n# A tibble: 6 x 11\n  iso_a2 name_long continent region_un subregion type  area_km2\n  <chr>  <chr>     <chr>     <chr>     <chr>     <chr>    <dbl>\n1 FJ     Fiji      Oceania   Oceania   Melanesia Sove…   1.93e4\n2 TZ     Tanzania  Africa    Africa    Eastern … Sove…   9.33e5\n3 EH     Western … Africa    Africa    Northern… Inde…   9.63e4\n4 CA     Canada    North Am… Americas  Northern… Sove…   1.00e7\n5 US     United S… North Am… Americas  Northern… Coun…   9.51e6\n6 KZ     Kazakhst… Asia      Asia      Central … Sove…   2.73e6\n# … with 4 more variables: pop <dbl>, lifeExp <dbl>, gdpPercap <dbl>,\n#   geom <MULTIPOLYGON [°]>\n\nThis makes the plot, using dplyr to filter down to just the row containing the India geometry.\n\n\nindia_geom <- world %>%\n  filter(name_long == \"India\") %>%\n  st_geometry()\n\nplot(india_geom)\n\n\n\n\nUsing base R, we can also layer on several vector objects, using add = TRUE.\n\n\nworld_asia <- world %>%\n  filter(continent == \"Asia\")\n\nplot(india_geom, expandBB = c(0, 0.2, 0.1, 1), col = \"gray\", lwd = 3)\nplot(st_union(world_asia), add = TRUE)\n\n\n\n\nWe can also use geom_sf in ggplot2. To change the coordinates of the viewing box, we can use coord_sf.\n\n\nggplot() +\n  geom_sf(data = world_asia, fill = \"white\") +\n  geom_sf(data = india_geom) +\n  coord_sf(xlim = c(60, 110), ylim = c(5, 40))\n\n\n\n\nWe can also encode data that’s contained in the vector dataset, using the aes.\n\n\nggplot() +\n  geom_sf(data = world_asia, aes(fill = lifeExp)) +\n  coord_sf(xlim = c(60, 110), ylim = c(5, 40)) +\n  scale_fill_viridis_b()\n\n\n\n\nIf you want to overlay spatial features onto a publicly available map, you can find one using the ggmap library.\n\n\nsource(\"https://uwmadison.box.com/shared/static/c1mnfdl292fcot0jo58g5wnshquaa7zx.r\")\nregister_google(key = \"AIzaSyBW7PU2Ne1CM33WMaPreTbAImzGB22Pc6s\")\nwatercolor <- get_map(c(79.5937, 22.92501), maptype = \"watercolor\", zoom = 3)\nwatercolor <- ggmap_bbox(watercolor)\nggmap(watercolor) +\n  geom_sf(data = world_asia, aes(fill = lifeExp), inherit.aes = FALSE) +\n  coord_sf(xlim = c(5.5e6, 11e6), ylim = c(5e5, 5e6), crs = st_crs(3857)) +\n  scale_fill_viridis_b()\n\n\n\n\nThe code from the previous lecture gives more examples of how to use ggplot2 and ggmap to visualize vector data over public map backgrounds\nEven in this more complex setup, where we work with background images and vector data rather than standard data.frames, most of the usual ggplot2 functions still apply. For example, we can still color code or facet by fields in the vector dataset. To illustrate, we revisit the bus route data from the last lecture and distinguish between buses operated by the cities of Madison vs. Monona. Before plotting, we fetch the underlying data.\n\n\nbus <- read_sf(\"https://uwmadison.box.com/shared/static/5neu1mpuh8esmb1q3j9celu73jy1rj2i.geojson\")\nsatellite <- get_map(c(-89.37974, 43.07140), maptype = \"satellite\", zoom = 12)\nsatellite <- ggmap_bbox(satellite)\n\nhead(bus)\n\n\nSimple feature collection with 6 features and 13 fields\ngeometry type:  GEOMETRY\ndimension:      XY\nbbox:           xmin: -89.53446 ymin: 43.0501 xmax: -89.35841 ymax: 43.11426\ngeographic CRS: WGS 84\n# A tibble: 6 x 14\n  id    X.id  fixme name  network operator public_transpor… ref  \n  <chr> <chr> <chr> <chr> <chr>   <chr>    <chr>            <chr>\n1 rela… rela… how … Metr… Metro … City of… 1                73   \n2 rela… rela… <NA>  Metr… Metro … City of… 1                84   \n3 rela… rela… <NA>  Metr… Metro … City of… 1                80   \n4 rela… rela… <NA>  Metr… Metro … City of… 1                67   \n5 rela… rela… veri… Metr… Metro … City of… 1                2    \n6 rela… rela… <NA>  Metr… Metro … City of… 1                70   \n# … with 6 more variables: route <chr>, type <chr>, website <chr>,\n#   FIXME <chr>, X.relations <chr>, geometry <GEOMETRY [°]>\n\nNote that operator is the field containing information about which city is operating the buses. We can color code the routes by this attribute.\n\n\nggmap(satellite) +\n  geom_sf(data = bus, aes(color = operator), size = .5, inherit.aes = FALSE) +\n  coord_sf(crs = st_crs(3857))\n\n\n\n\nAlternatively, we can facet.\n\n\nggmap(satellite) +\n  geom_sf(data = bus, col = \"#bc7ab3\", size = .5, inherit.aes = FALSE) +\n  coord_sf(crs = st_crs(3857)) +\n  facet_wrap(~ operator)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-02-week7-2/sf-classes.png",
    "last_modified": "2021-03-04T15:53:12-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-02-week7-1/",
    "title": "Spatial Data Formats",
    "description": "An overview of common formats, with illustrative examples.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-08",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"ggmap\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"raster\")\nlibrary(\"RStoolbox\")\ntheme_set(theme_minimal())\n\n\n\nSpatial data come in two main formats: vector and raster. We’ll examine them in detail in the next few lectures, but this lecture motivates the high-level distinction and gives a few examples. It also shows how to read and write data to and from these formats.\nVector Data\nVector data formats are used to store geometric information, like the locations of hospitals (points), trajectories of bus routes (lines), or boundaries of counties (polygons). It’s useful to think of the associated data as being spatially enriched data frames, with each row corresponding to one of these geometric features.\nVector data are usually stored in .geojson, .wkt, .shp, or .topojson formats. Standard data.frames cannot be used because then important spatial metadata would be lost, like the Coordinate Reference System (to be explained in the fourth lecture this week).\nIn R, these formats can be read using read_sf in the sf package. They can be written using the write_sf function. Here, we’ll read in a vector dataset containing the boundaries of lakes in Madison.\n\n\nlakes <- read_sf(\"https://uwmadison.box.com/shared/static/duqpj0dl3miltku1676es64d5zmygy92.geojson\")\n\nlakes %>%\n  dplyr::select(id, name, geometry)\n\n\nSimple feature collection with 10 features and 2 fields\ngeometry type:  POLYGON\ndimension:      XY\nbbox:           xmin: -89.54084 ymin: 42.94762 xmax: -89.17699 ymax: 43.2051\ngeographic CRS: WGS 84\n# A tibble: 10 x 3\n   id          name                                           geometry\n   <chr>       <chr>                                     <POLYGON [°]>\n 1 relation/1… Lake Mon… ((-89.37974 43.0714, -89.37984 43.07132, -89…\n 2 relation/3… Lake Men… ((-89.46885 43.08266, -89.46864 43.08258, -8…\n 3 relation/3… Upper Mu… ((-89.31364 43.04483, -89.31361 43.04464, -8…\n 4 relation/4… Hook Lake ((-89.33198 42.94909, -89.33161 42.94866, -8…\n 5 relation/4… Lake Win… ((-89.4265 43.05514, -89.4266 43.05511, -89.…\n 6 relation/6… Lake Wau… ((-89.32949 42.99166, -89.32908 42.99187, -8…\n 7 relation/7… Lake Keg… ((-89.2648 42.9818, -89.26399 42.98126, -89.…\n 8 relation/9… Lower Mu… ((-89.28011 42.98486, -89.2794 42.98481, -89…\n 9 way/214157… Goose La… ((-89.53957 42.97967, -89.53947 42.97955, -8…\n10 way/287217… Brazee L… ((-89.18562 43.19529, -89.18533 43.19531, -8…\n\n#write_sf(lakes, \"output.geojson\", driver = \"GeoJSON\")\n\n\n\nWe’ll discuss plotting in the next lecture, but for a preview, this is how you can visualize the lakes using ggplot2.\n\n\n\nWith a little extra effort, we can overlay the features onto public map backgrounds (these are often called “basemaps”).\n\n\n\n\n\n\nThere is a surprising amount of public vector data available online. Using this query1, I’ve downloaded locations of all hospital clinics in Madison.\n\n\nclinics <- read_sf(\"https://uwmadison.box.com/shared/static/896jdml9mfnmza3vf8bh221h9hlvh70v.geojson\")\n\n# how would you overlay the names of the clinics, using geom_text?\nggmap(satellite) +\n  geom_sf(data = clinics, col = \"red\", size = 2, inherit.aes = FALSE) +\n  coord_sf(crs = st_crs(3857))\n\n\n\n\nUsing this query, I’ve downloaded all the bus routes.\n\n\nbus <- read_sf(\"https://uwmadison.box.com/shared/static/5neu1mpuh8esmb1q3j9celu73jy1rj2i.geojson\")\n\nggmap(satellite) +\n  geom_sf(data = bus, col = \"#bc7ab3\", size = .5, inherit.aes = FALSE) +\n  coord_sf(crs = st_crs(3857))\n\n\n\n\nFor the boundaries of the lakes above, I used this query.\nMany organizations prepare geojson data themselves and make it publicly available; e.g., the boundaries of rivers or glaciers. Don’t worry about how to visualize these data at this point — I just want to give some motivating examples.\nRaster Data\nRaster data give a measurement along a spatial grid. You can think of them as spatially enriched matrices, where the metadata says where on the earth each entry of the matrix is associated with.\nRaster data are often stored in tiff format. They can be read in using the brick function in the raster library, and can be written using writeRaster.\n\n\nshanghai <- brick(\"https://uwmadison.box.com/shared/static/u4na56w3r4eqg232k2ma3eqbvehfiaoq.tif\")\nshanghai\n\n\nclass      : RasterBrick \ndimensions : 8192, 8192, 67108864, 3  (nrow, ncol, ncell, nlayers)\nresolution : 2.7e-06, 2.7e-06  (x, y)\nextent     : 121.6565, 121.6786, 30.96283, 30.98494  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : https://uwmadison.box.com/shared/static/u4na56w3r4eqg232k2ma3eqbvehfiaoq.tif \nnames      : u4na56w3r4eqg232k2ma3eqbvehfiaoq.1, u4na56w3r4eqg232k2ma3eqbvehfiaoq.2, u4na56w3r4eqg232k2ma3eqbvehfiaoq.3 \nmin values :                                  0,                                  0,                                  0 \nmax values :                              65535,                              65535,                              65535 \n\n#writeRaster(shanghai, \"output.tiff\", driver = \"GeoTIFF\")\n\n\n\nSome of the most common types of public raster data are satellite images or derived measurements, like elevation maps. For example, the code below shows an image of a neighborhood outside Shanghai.\n\n\nggRGB(shanghai, stretch=\"lin\")\n\n\n\n\nThere’s actually quite a bit of information in this image. We can zoom in…\n\n\nggRGB(shanghai, stretch=\"lin\") +\n  coord_fixed(xlim = c(121.66, 121.665), ylim = c(30.963, 30.968))\n\n\n\n\nHere are is data on elevation in Zion national park.\n\n\nf <- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nzion <- raster(f) %>%\n  as.data.frame(xy = TRUE)\nggplot(zion) +\n  geom_raster(aes(x = x, y = y, fill = srtm)) +\n  scale_fill_gradient(low = \"white\", high = \"black\") +\n  coord_fixed()\n\n\n\n\nInstallation\nA note about R packages: for historical reasons, spatial data libraries in R reference a few command line programs, like gdal and proj. Since these command line programs are not themselves a part of R, they need to be installed before the corresponding R packages. The process will differ from operating system to operating system, and the experience can be frustrating, especially when the R packages don’t recognize the underlying system installation. I recommend following the instructions on this page and reaching out early if you have any issues.\n\nIt can be constructed easily using the wizard↩︎\n",
    "preview": "posts/2021-03-02-week7-1/week7-1_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-04T15:52:52-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-23-week6-5/",
    "title": "Collections of Time Series",
    "description": "Navigating across related time series.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-05",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"broom\")\nlibrary(\"dplyr\")\nlibrary(\"feasts\")\nlibrary(\"fpp2\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"lubridate\")\nlibrary(\"stringr\")\nlibrary(\"tidyr\")\nlibrary(\"tsibble\")\nlibrary(\"tsibbledata\")\ntheme_set(theme_minimal())\n\n\n\nWe have seen ways of visualizing a single time series (seasonal plots, ACF) and small numbers of time series (Cross Correlation). In practice, it’s also common to encounter large collections of time series. These datasets tend to require more sophisticated analysis techniques, but we will review one useful approach, based on extracted features.\nThe high-level idea is to represent each time series by a vector of summary statistics, like the maximum value, the slope, and so on. These vector summaries can then be used to create an overview of variation seen across all time series. For example, just looking at the first few regions in the Australian tourism dataset, we can see that there might be useful features related to the overall level (Coral Coast is larger than Barkly), recent trends (increased business in North West), and seasonality (South West is especially seasonal).\n\n\ntourism <- as_tsibble(tourism, index = Quarter) %>%\n  mutate(key = str_c(Region, Purpose, sep=\"-\")) %>%\n  update_tsibble(key = c(\"Region\", \"State\", \"Purpose\", \"key\"))\n\nregions <- tourism %>%\n  distinct(Region) %>%\n  pull(Region)\n\nggplot(tourism %>% filter(Region %in% regions[1:9])) +\n  geom_line(aes(x = time(Quarter), y = Trips, col = Purpose)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(~Region, scale = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nComputing these kinds of summary statistics by hand would be tedious. Fortunately, the feasts package makes it easy to extract a variety of statistics for tsibble objects.\n\n\ntourism_features <- tourism %>%\n  features(Trips, feature_set(pkgs = \"feasts\"))\n\ntourism_features\n\n\n# A tibble: 304 x 52\n   Region State Purpose key   trend_strength seasonal_streng…\n   <chr>  <chr> <chr>   <chr>          <dbl>            <dbl>\n 1 Adela… Sout… Busine… Adel…          0.451            0.380\n 2 Adela… Sout… Holiday Adel…          0.541            0.601\n 3 Adela… Sout… Other   Adel…          0.743            0.189\n 4 Adela… Sout… Visiti… Adel…          0.433            0.446\n 5 Adela… Sout… Busine… Adel…          0.453            0.140\n 6 Adela… Sout… Holiday Adel…          0.512            0.244\n 7 Adela… Sout… Other   Adel…          0.584            0.374\n 8 Adela… Sout… Visiti… Adel…          0.481            0.228\n 9 Alice… Nort… Busine… Alic…          0.526            0.224\n10 Alice… Nort… Holiday Alic…          0.377            0.827\n# … with 294 more rows, and 46 more variables:\n#   seasonal_peak_year <dbl>, seasonal_trough_year <dbl>,\n#   spikiness <dbl>, linearity <dbl>, curvature <dbl>,\n#   stl_e_acf1 <dbl>, stl_e_acf10 <dbl>, acf1 <dbl>, acf10 <dbl>,\n#   diff1_acf1 <dbl>, diff1_acf10 <dbl>, diff2_acf1 <dbl>,\n#   diff2_acf10 <dbl>, season_acf1 <dbl>, pacf5 <dbl>,\n#   diff1_pacf5 <dbl>, diff2_pacf5 <dbl>, season_pacf <dbl>,\n#   zero_run_mean <dbl>, nonzero_squared_cv <dbl>,\n#   zero_start_prop <dbl>, zero_end_prop <dbl>,\n#   lambda_guerrero <dbl>, kpss_stat <dbl>, kpss_pvalue <dbl>,\n#   pp_stat <dbl>, pp_pvalue <dbl>, ndiffs <int>, nsdiffs <int>,\n#   bp_stat <dbl>, bp_pvalue <dbl>, lb_stat <dbl>, lb_pvalue <dbl>,\n#   var_tiled_var <dbl>, var_tiled_mean <dbl>, shift_level_max <dbl>,\n#   shift_level_index <dbl>, shift_var_max <dbl>,\n#   shift_var_index <dbl>, shift_kl_max <dbl>, shift_kl_index <dbl>,\n#   spectral_entropy <dbl>, n_crossing_points <int>,\n#   longest_flat_spot <int>, coef_hurst <dbl>, stat_arch_lm <dbl>\n\nOnce you have a data.frame summarizing these time series, you can run any clustering or dimensionality reduction procedure on the summary. For example, this is 2D representation from PCA. We will get into much more depth about dimensionality reduction later in this course — for now, just think of this as an abstract map relating all the time series.\n\n\npcs <- tourism_features %>%\n  select(-State, -Region, -Purpose, -key) %>%\n  prcomp(scale = TRUE) %>%\n  augment(tourism_features)\n\noutliers <- pcs %>% \n  filter(.fittedPC1 ^ 2 + .fittedPC2 ^ 2 > 120)\n\n\n\nThis PCA makes it very clear that the different travel purposes have different time series, likely due to the heavy seasonality of holiday travel (Melbourne seems to be an interesting exception).\n\n\nggplot(pcs, aes(x = .fittedPC1, y = .fittedPC2)) +\n  geom_point(aes(col = Purpose)) +\n  geom_text_repel(\n    data = outliers,\n    aes(label = Region),\n    size = 2.5 \n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"PC1\", y = \"PC2\") +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nWe can look at the series that are outlying in the PCA. The reading has some stories for why these should be considered outliers. They seem to be series with substantial increasing trends or which have exceptionally high overall counts.\n\n\noutlier_series <- tourism %>%\n  filter(key %in% outliers$key)\n\nggplot(outlier_series) +\n  geom_line(aes(x = time(Quarter), y = Trips, col = Purpose)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(~Region, scale = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nThis featurization approach is especially powerful when combined with coordinated views. It is possible to link the points in the PCA plot with the time series display, so that selecting points in the PCA shows the corresponding time series.\n\n\n\n",
    "preview": "posts/2021-02-23-week6-5/week6-5_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-24T16:45:29-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-23-week6-4/",
    "title": "Cross and Auto-Correlation",
    "description": "Summaries of relationships between and within time series.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-04",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"tsibbledata\")\nlibrary(\"feasts\")\nlibrary(\"tidyr\")\nlibrary(\"lubridate\")\nlibrary(\"ggplot2\")\nlibrary(\"stringr\")\ntheme_set(theme_minimal())\n\n\n\nThere is often interest in seeing how two time series relate to one another. A scatterplot can be useful in gauging this relationship. For example, the plot below suggests that there is a relationship between electricity demand and temperature, but it’s hard to make out exactly the nature of the relationship.\n\n\nvic_2014 <- vic_elec %>%\n  filter(year(Time) == 2014)\n\nvic_2014 %>%\n  pivot_longer(Demand:Temperature) %>%\n  ggplot(aes(x = Time, y = value)) +\n  geom_line() +\n  facet_wrap(~ name, scales = \"free_y\")\n\n\n\n\nA scatterplot clarifies that, while electricity demand generally goes up in the cooler months, the very highest demand happens during high heat days.\n\n\nggplot(vic_2014, aes(x = Temperature, y = Demand)) +\n  geom_point(alpha = 0.6, size = 0.7)\n\n\n\n\nNote that the timepoints are far from independent. Points tend to drift gradually across the scatterplot, rather than jumping to completely different regions in short time intervals. This is just the 2D consequence of the time series varying smoothly.\n\n\nlagged <- rbind(vic_2014[-1, ], NA) %>%\n  setNames(str_c(\"lagged_\", colnames(vic_2014)))\n\nggplot(bind_cols(vic_2014, lagged), aes(x = Temperature, y = Demand)) +\n  geom_point(alpha = 0.6, size = 0.7) +\n  geom_segment(\n    aes(xend = lagged_Temperature, yend = lagged_Demand),\n    size = .4, alpha = 0.5\n  )\n\n\n\n\nTo formally measure the linear relationship between two time series, we can use the cross-correlation, \\[\\begin{align}\n\\frac{\\sum_{t}\\left(x_{t} - \\hat{\\mu}_{X}\\right)\\left(y_{t} - \\hat{\\mu}_{Y}\\right)}{\\hat{\\sigma}_{X}\\hat{\\sigma}_{Y}}\n\\end{align}\\] which for the data above is,\n\n\ncor(vic_2014$Temperature, vic_2014$Demand)\n\n\n[1] 0.2797854\n\nCross-correlation can be extended to autocorrelation — the correlation between a time series and a lagged version of itself. This measure is useful for quantifying the strength of seasonality within a time series. A daily time series with strong weekly seasonality will have high autocorrelation at lag 7, for example. The example below shows the lag-plots for Australian beer production after 2000. The plot makes clear that there is high autocorrelation at lags 4 and 8, suggesting high quarterly seasonality.\n\n\nrecent_production <- aus_production %>%\n  filter(year(Quarter) > 2000)\n\ngg_lag(recent_production, Beer, geom = \"point\")\n\n\n\n\nIndeed, we can confirm this by looking at the original data.\n\n\nggplot(recent_production) +\n  geom_line(aes(x = time(Quarter), y = Beer))\n\n\n\n\nThese lag plots take up a bit of space. A more compact summary is to compute the autocorrelation function (ACF). Peaks and valleys in an ACF suggest seasonality at the frequency indicated by the lag value.\n\n\nacf_data <- ACF(recent_production, Beer)\nautoplot(acf_data)\n\n\n\n\nGradually decreasing slopes in the ACF suggest trends. This is because if there is a trend, the current value tends to be very correlated with the recent past. It’s possible to have both seasonality within a trend, in which case the ACF function has bumps where the seasonal peaks align.\n\n\nggplot(as_tsibble(a10), aes(x = time(index), y = value)) +\n  geom_line()\n\n\n\nacf_data <- ACF(as_tsibble(a10), lag_max = 100)\nautoplot(acf_data)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-23-week6-4/week6-4_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-24T16:45:00-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-23-week6-3/",
    "title": "Seasonal Plots",
    "description": "Approaches for visualizing seasonality.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-03",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"robservable\")\nlibrary(\"feasts\")\nlibrary(\"fpp2\")\nlibrary(\"tsibbledata\")\ntheme_set(theme_minimal())\n\n\n\nIf our data have seasonal structure, it’s natural to compare individual periods against one another. In contrast to plotting the data in one long time series, seasonal plots reduce the amount of distance our eyes have to travel in order to compare two periods in a seasonal pattern. This also reduces the burden on our memory.\nIn R, we can use the gg_season function to overlay all the seasons onto one another. The plot below shows antidiabetes drug sales over time. This view makes it clear that there is a spike in sales every January.\n\n\ncols <- scales::viridis_pal()(10)\ngg_season(as_tsibble(a10), pal = cols)\n\n\n\n\nIf the time series exhibit seasonal structure at multiple scales, then we can view them all using the period argument.\n\n\ngg_season(vic_elec, Demand, period = \"day\", pal = cols)\n\n\n\ngg_season(vic_elec, Demand, period = \"week\", pal = cols)\n\n\n\n\nIn vega-lite, we can use tooltips, instead of trying to encode the season’s period using color. This lets us isolate specific series that have interesting behavior (e.g., week number in the plot below). The downside is that we have to manually calculate the variable that goes on the \\(x\\)-axis, rather than relying on the period argument like above.\n{\n  let line = vl.markLine({opacity: 0.8, size: 0.9})\n    .data(data)\n    .encode(\n      vl.x().fieldQ(\"time_in_week_\").scale({domain: [0, 7]}),\n      vl.y().fieldQ(\"Demand\"), \n      vl.detail().fieldN(\"week\"),\n      vl.color().fieldN(\"year\"),\n      vl.tooltip().fieldN(\"week\")\n    )    \n    .width(600);\n  \n  return line.render()\n }\n\n\nrobservable(\"@krisrs1128/seasonality\", 4)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/seasonality\",\"include\":4,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\n\n\n\n",
    "preview": "posts/2021-02-23-week6-3/week6-3_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-24T16:44:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-23-week6-2/",
    "title": "Time Series Patterns",
    "description": "Vocabulary for describing visual structure in time series.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-02",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"tsibble\")\nlibrary(\"tsibbledata\")\nlibrary(\"fpp2\")\nlibrary(\"ggplot2\")\ntheme_set(theme_minimal())\n\n\n\nThere are a few structures that are worth keeping an eye out for whenever you plot a single time series. We’ll review the main vocabulary in these notes.\nVocabulary\nTrend: A long-run increase or decrease in a time series.\nSeasonal: A pattern that recurs over a fixed and known period.\nCyclic: A rising and falling pattern that does not occur over a fixed or known period.\n\nA few series with different combinations of these patterns are shown below.\n\n\nggplot(as_tsibble(qauselec)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Australian Quarterly Electricity Production\")\n\n\n\nggplot(as_tsibble(hsales)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Housing Sales\")\n\n\n\nggplot(as_tsibble(ustreas)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"US Treasury Bill Contracts\")\n\n\n\nggplot(as_tsibble(diff(goog))) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Google Stock Prices\")\n\n\n\n\nA series can display combinations of these patterns at once. Further, the same data can exhibit different patterns depending on the scale at which it is viewed. For example, though a dataset might seem seasonal at short time scales, a long-term trend might appear after zooming out. This is visible in the electricity production plot above.\nFinally, it’s worth keeping in mind that real-world structure can be much more complicated than any of these patterns. For example, the plot below shows the number of passengers on flights from Melbourne to Sydney between 1987 and 1992. You can see a period when no flights were made and a trial in 1992 where economy seats were switched to business seats.\n\n\nmelbourne_sydney <- ansett %>%\n  filter(Airports == \"MEL-SYD\") # challenge: facet by Airports, instead of filtering\n\nggplot(melbourne_sydney) +\n  geom_line(aes(x = Week, y = Passengers, col = Class))\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-23-week6-2/week6-2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-24T16:44:45-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-23-week6-1/",
    "title": "tsibble Objects",
    "description": "A data structure for managing time series data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-03-01",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"readr\")\nlibrary(\"dplyr\")\nlibrary(\"tsibble\")\nlibrary(\"tsibbledata\")\nlibrary(\"lubridate\")\n\n\n\nTsibbles are data structures that are designed specifically for storing time series data. They are useful because they create a unified interface to various time series visualization and modeling tasks. This removes the friction of having to transform back and forth between data.frames, lists, and matrices, depending on the particular task of interest.\nThe key difference between a tsibble and an ordinary data.frame is that it requires a temporal key variable, specifying the frequency with which observations are collected. For example, the code below generates a tsibble with yearly observations.\n\n\ntsibble(\n  Year = 2015:2019,\n  Observation = c(123, 39, 78, 52, 110),\n  index = Year\n)\n\n\n# A tsibble: 5 x 2 [1Y]\n   Year Observation\n  <int>       <dbl>\n1  2015         123\n2  2016          39\n3  2017          78\n4  2018          52\n5  2019         110\n\nWe can also create a tsibble from an ordinary data.frame by calling the as_tsibble function. The only subtlety is that we have to specify an index.\n\n\nx <- data.frame(\n  Year = 2015:2019,\n  Observation = c(123, 39, 78, 52, 110)\n)\n\nas_tsibble(x, index = Year)\n\n\n# A tsibble: 5 x 2 [1Y]\n   Year Observation\n  <int>       <dbl>\n1  2015         123\n2  2016          39\n3  2017          78\n4  2018          52\n5  2019         110\n\nThe index is useful because it creates a data consistency check. If a few days are missing from a daily dataset, the index makes it easy to detect and fill in these gaps. Notice that when we print a tsibble object, it prints the index and guessed sampling frequency on the top right corner.\n\n\ndays <- seq(as_date(\"2021-01-01\"), as_date(\"2021-01-31\"), by = \"day\")\ndays <- days[-5] # Skip January 5\n\nx <- tsibble(day = days, value = rnorm(30), index = day)\nfill_gaps(x)\n\n\n# A tsibble: 31 x 2 [1D]\n   day         value\n   <date>      <dbl>\n 1 2021-01-01 -0.754\n 2 2021-01-02 -1.42 \n 3 2021-01-03 -0.575\n 4 2021-01-04 -0.373\n 5 2021-01-05 NA    \n 6 2021-01-06 -0.935\n 7 2021-01-07 -1.06 \n 8 2021-01-08  1.07 \n 9 2021-01-09 -0.527\n10 2021-01-10  0.344\n# … with 21 more rows\n\nTsibbles can store more than one time series at a time. In this case, we have to specify key columns that distinguish between the separate time series. For example, in the olympics running times dataset,\n\n\nolympic_running\n\n\n# A tsibble: 312 x 4 [4Y]\n# Key:       Length, Sex [14]\n    Year Length Sex    Time\n   <int>  <int> <chr> <dbl>\n 1  1896    100 men    12  \n 2  1900    100 men    11  \n 3  1904    100 men    11  \n 4  1908    100 men    10.8\n 5  1912    100 men    10.8\n 6  1916    100 men    NA  \n 7  1920    100 men    10.8\n 8  1924    100 men    10.6\n 9  1928    100 men    10.8\n10  1932    100 men    10.3\n# … with 302 more rows\n\nthe keys are running distance and sex. If we were creating a tsibble from a data.frame containing these multiple time series, we would need to specify the keys. This protects against accidentally having duplicate observations at given times.\n\n\nolympic_df <- as.data.frame(olympic_running)\nas_tsibble(olympic_df, index = Year, key = c(\"Sex\", \"Length\")) # what happens if we remove key?\n\n\n# A tsibble: 312 x 4 [4Y]\n# Key:       Sex, Length [14]\n    Year Length Sex    Time\n   <int>  <int> <chr> <dbl>\n 1  1896    100 men    12  \n 2  1900    100 men    11  \n 3  1904    100 men    11  \n 4  1908    100 men    10.8\n 5  1912    100 men    10.8\n 6  1916    100 men    NA  \n 7  1920    100 men    10.8\n 8  1924    100 men    10.6\n 9  1928    100 men    10.8\n10  1932    100 men    10.3\n# … with 302 more rows\n\nThe usual data tidying functions from dplyr are implemented for tsibbles. Filtering rows, selecting columns, deriving variables using mutate, and summarizing groups using group_by and summarise all work as expected. One distinction to be careful about is that the results will be grouped by their index.\nFor example, this computes the total cost of Australian pharmaceuticals per month for a particular type of script. We simply filter to the script type and take the sum of costs.\n\n\nPBS %>%\n  filter(ATC2 == \"A10\") %>%\n  summarise(TotalC = sum(Cost))\n\n\n# A tsibble: 204 x 2 [1M]\n      Month  TotalC\n      <mth>   <dbl>\n 1 1991 Jul 3526591\n 2 1991 Aug 3180891\n 3 1991 Sep 3252221\n 4 1991 Oct 3611003\n 5 1991 Nov 3565869\n 6 1991 Dec 4306371\n 7 1992 Jan 5088335\n 8 1992 Feb 2814520\n 9 1992 Mar 2985811\n10 1992 Apr 3204780\n# … with 194 more rows\n\nIf we had wanted the total cost by year, we would have to convert to an ordinary data.frame with a year variable. We cannot use a tsibble here because we would have multiple measurements per year, and this would violate tsibble’s policy of having no duplicates.\n\n\nPBS %>%\n  filter(ATC2 == \"A10\") %>%\n  mutate(Year = year(Month)) %>%\n  as_tibble() %>%\n  group_by(Year) %>%\n  summarise(TotalC = sum(Cost))\n\n\n# A tibble: 18 x 2\n    Year     TotalC\n * <dbl>      <dbl>\n 1  1991  21442946 \n 2  1992  45686946.\n 3  1993  55532688.\n 4  1994  60816080.\n 5  1995  67326599.\n 6  1996  77397927.\n 7  1997  85131672.\n 8  1998  93310626.\n 9  1999 105959043.\n10  2000 122496586.\n11  2001 136467442.\n12  2002 149066136.\n13  2003 156464261.\n14  2004 183798935.\n15  2005 199655595 \n16  2006 220354676 \n17  2007 265718966.\n18  2008 135036513 \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-24T16:44:42-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-30-week5-5/",
    "title": "Other Tricks from Profiler",
    "description": "A crash course on entity resolution, plus some other tips.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-25",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"cluster\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\ntheme_set(theme_minimal())\n\n\n\nEntity resolution1 is the process of resolving field values that refer to the same thing, but which are stored using different names. For example, a column might include entries for “UW Madison” and “University of Wisconsin - Madison.” This often arises when a dataset is made by linking a few different sources, combining different time periods of the same source, or involved manual text entry.\nOne simple way to detect candidates for unification is to cluster all the strings using an appropriate distance. The Profiler paper uses the string edit distance to decide whether two strings are similar. For example, two insertions to the first string below would give us the second string, so the edit distance is 2.\n\n\nadist(c(\"abc\", \"abcde\"))\n\n\n     [,1] [,2]\n[1,]    0    2\n[2,]    2    0\n\nLike in the Profiler paper, we’re going to illustrate this by looking for potentially duplicated movies in the movies dataset.\n\n\nmovies <- read_csv(\"https://uwmadison.box.com/shared/static/txa56mux3ca2f8w2zmc9dq7yunljd1ak.csv\") %>%\n  filter(!is.na(Title))\n\n\n\nThe block below computes the string distance between all pairs of movie titles. We then extract all clusters of movie titles whose maximum within-cluster distance is less than or equal to 3.\n\n\nD <- adist(movies$Title)\ntree <- hclust(as.dist(D), method = \"complete\")\nmovies$cluster <- cutree(tree, h = 3)\n\n\n\nThe potential duplicates are those clusters that have more than one element. Let’s derive the size of each cluster and visualize a few of the clusters. We can see that we’ve picked out sequels, movies with short names, and a few exact matches.\n\n\nmovies <- movies %>%\n  arrange(cluster) %>%\n  group_by(cluster) %>%\n  mutate(\n    cluster_ix = seq_len(n()), # indexes movies in each cluster from 1 .. cluster_size\n    cluster_size = n()\n  )\n\n\n\n\n\nmovies %>%\n  filter(cluster_size > 1, cluster < 100) %>%\n  ggplot() +\n  geom_text(\n    aes(y = reorder(cluster, -cluster_size), x = cluster_ix, label = Title),\n    size = 4\n  ) +\n  scale_x_discrete(expand = c(0.1, 0.1))\n\n\n\n\nIt seems like many of the potential duplicates are probably not mistakes. For example, Alice in Wonderland refers to two movies, the 1951 original and a 2010 remake2.\n\n\nmovies %>%\n  filter(Title == \"Alice in Wonderland\") %>%\n  select(Title, Release_Date, IMDB_Rating, IMDB_Votes)\n\n\n# A tibble: 2 x 5\n# Groups:   cluster [1]\n  cluster Title               Release_Date IMDB_Rating IMDB_Votes\n    <int> <chr>               <chr>              <dbl>      <dbl>\n1      47 Alice in Wonderland Jul 28 1951          6.7      63458\n2      47 Alice in Wonderland Mar 05 2010          6.7      63458\n\nTo automatically remove these remakes / sequels from our candidate list, let’s filter out movies that didn’t appear in the same year. This is a special case of the general idea of “conditional deduplication” – you can simplify your entity resolution candidates if you can first match by a few other fields. In the end, it doesn’t seem like there are any true duplicates, though it is quite a coincidence that “Diary of the Dead” and “Day of the Dead” both came out in\n\n\n\nmovies <- movies %>%\n  mutate(year = str_extract(Release_Date, \"[0-9]+$\"))\n\nmovies %>%\n  filter(!is.na(year)) %>%\n  group_by(cluster, year) %>%\n  mutate(conditional_count = n()) %>%\n  filter(conditional_count > 1) %>%\n  select(Title, year, cluster)\n\n\n# A tibble: 43 x 3\n# Groups:   cluster, year [20]\n   Title             year  cluster\n   <chr>             <chr>   <int>\n 1 Ray               2004        5\n 2 Saw               2004        5\n 3 Rent              2005       84\n 4 Venom             2005       84\n 5 Bobby             2006       92\n 6 Borat             2006       92\n 7 Day of the Dead   2008      230\n 8 Diary of the Dead 2008      230\n 9 The Box           2009      324\n10 The Road          2009      324\n# … with 33 more rows\n\nMinor Tricks\nThere are two other nice tricks in the Profiler paper that are worth knowing. The first is the binning trick. If you have a very large dataset, scatterplots can be misleading, since the points overlap too much. A better alternative is to use 2D binning.\nFor example, here are two Gaussian blobs. Looking at a scatterplot, it looks like one blob.\n\n\nn <- 1e6\nx <- matrix(rnorm(n), ncol = 2)\nz <- matrix(rnorm(0.05 * n, sd = 0.2), ncol = 2)\ndf <- data.frame(rbind(x, z))\n\n\n\n\n\nggplot(df) +\n  geom_point(aes(x = X1, y = X2))\n\n\n\n\nBut if you binned the plot, the second blob appears. It’s not possible to perceive 100K points on a screen anyways, so this visualization is much more meaningful.\n\n\nggplot(df) +\n  geom_bin2d(aes(x = X1, y = X2), binwidth = 0.1) +\n  scale_fill_viridis_b()\n\n\n\n\nThe final idea is that, when 0 has a qualitatively differnet meaning than a very small, but nonzero value, then you should use a color palette that highlights that discontinuity. The small, nonzero values in the left hand plot are barely visible, but are easy to distinguish on the right hand side.\n\n\nThis is also sometimes called “Deduplication.”↩︎\nThough, notice that the IMDB data are exactly the same… this is almost certainly a mistake in how the data were merged.↩︎\n",
    "preview": "posts/2021-01-30-week5-5/week5-5_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-02-21T15:48:18-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-30-week5-4/",
    "title": "Characterizing Outliers",
    "description": "Which columns might help us understand extreme values?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-24",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"caret\")\nlibrary(\"gbm\")\nlibrary(\"ggplot2\")\nlibrary(\"ggridges\")\nlibrary(\"lubridate\")\nlibrary(\"readr\")\nlibrary(\"skimr\")\nlibrary(\"dplyr\")\ntheme_set(theme_minimal())\n\n\n\nIf we uncover extreme values, it’s natural to ask why they arise. The most reliable way to find out is to study how the data were collected. Often, though, through a little detective work based on the rest of the available data, we can form reasonable hypotheses for the source of extreme values.\nThe profiler paper approaches this by coordinating views. On the left, a binned scatterplot shows the relationship between Global (\\(x\\)-axis) and US (\\(y\\)-axis) gross product for movies in the IMDB dataset. There seem to be a few outliers along the bottom of the scatterplot. These are movies with high Global gross but which had low revenue in the US. At first, we might suspect that these are errors. However, linking the scatterplot with the map on the right gives a explanation – these outlier movies were released outside of the US. In the full system, selecting bins in the left hand plot highlights the countries that contribute to that bin in orange color.\n\nHow can we know which views to coordinate? Ideally, there would be a systematic way to search through promising views, without relying solely on intuition. The Profiler paper suggests a criterion based on mutual information. We will not discuss this specific proposal, since I do not want us to get side-tracked learning about information theory. Instead, we will use a simpler (and I think more practical) alternative based on predictability.\nWe will illustrate the idea with a dataset of rail tickets in Spain1. Each row shows a train trip, along with its date, the available seats, and an associated fare. A histogram of trip duration reveals some outliers – some trips take more than 10 hours.\n\n\ntrips <- read_csv(\"https://uwmadison.box.com/shared/static/fmag4vyhqad1sdb76aphukkumq2z2yp2.csv\") # give it a minute, it's a large file\nggplot(trips, aes(x = duration)) +\n  geom_histogram(bins = 100)\n\n\n\n\nWhat might be behind these long trips? A few hypotheses come to mind,\nThese are regularly scheduled, but infrequent, journeys between distant cities.\nThese are trains that make every stop along a route.\nThese trips were made during peak travel season, which resulted in them getting delayed.\nThe trains broke down mid-journey.\nLet’s see what additional measurements are available to us, to see whether they might help us come up with a satisfactory explanation. It seems like we have origin / destination and date variables, but not much else. Note the one variable (seats) that is entirely missing.\n\n\nskim(trips)\n\n\nTable 1: Data summary\nName\ntrips\nNumber of rows\n1000000\nNumber of columns\n14\n_______________________\n\nColumn type frequency:\n\nPOSIXct\n3\ncharacter\n7\nlogical\n1\nnumeric\n3\n________________________\n\nGroup variables\nNone\nVariable type: POSIXct\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\ndeparture\n0\n1\n2019-04-12 05:50:00\n2019-08-03 21:15:00\n2019-05-29 16:45:00\n11509\narrival\n0\n1\n2019-04-12 08:38:00\n2019-08-04 00:02:00\n2019-05-29 19:30:00\n15100\ninsert_date\n0\n1\n2019-04-11 21:49:46\n2019-06-05 15:06:25\n2019-05-08 19:34:03\n230569\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ncompany\n0\n1\n5\n5\n0\n1\n0\norigin\n0\n1\n6\n10\n0\n5\n0\ndestination\n0\n1\n6\n10\n0\n5\n0\nvehicle_type\n0\n1\n2\n9\n0\n16\n0\nvehicle_class\n3957\n1\n7\n22\n0\n8\n0\nfare\n3957\n1\n4\n23\n0\n9\n0\nmeta\n0\n1\n2\n2\n0\n1\n0\nVariable type: logical\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\nseats\n1e+06\n0\nNaN\n:\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nid\n0\n1.00\n5000665.58\n2887593.85\n19.00\n2500853.25\n5004586.00\n7504431.75\n9999990.00\n▇▇▇▇▇\nduration\n0\n1.00\n3.12\n1.58\n1.63\n2.50\n2.63\n3.17\n12.42\n▇▁▁▁▁\nprice\n87446\n0.91\n63.07\n25.88\n0.00\n43.25\n60.30\n78.80\n235.30\n▅▇▂▁▁\n\nAt this point, we could try relating trip duration to each of the available columns, evaluating a few of our initial hypothesis. While this strategy would work in this particular case, it would become untenable for datasets with many more columns. Instead, we’ll consider a more generally useful strategy, based on predictability.\nIntuitively, we should try finding features that help predict whether an observation will be an outlier. The features that are most predictive determine conditions within which the outliers are expected to arise, suggesting potential explanations. For example, in the movie revenue example, you might have found that country label is strongly predictive of whether an observation had high global, but low US, revenue.\nApplying this idea to the train trips example, let’s extract some features that might be predictive of whether a trip took more than 6 hours. We put this in the x data.frame. We put an indicator of whether the trip is 6 or more hours in the y vector.\n\n\nx <- trips %>%\n  mutate_at(\n    vars(destination, origin, fare, vehicle_type, vehicle_class), \n    as.factor\n  ) %>%\n  mutate(\n    month = month(departure),\n    hour = hour(departure),\n  ) %>%\n  select(destination, origin, month, hour, fare, vehicle_type, vehicle_class) %>%\n  as.data.frame()\n\ny <- as.factor(trips$duration > 6)\n\n\n\nNow, we fit a model that predicts y based on x. We’ll subsample rows so that it doesn’t take too long – our goal is visualization, not prediction performance. We are using the caret package to run the model, because it provides an interface to many potential prediction models. Finally, the varImp command summarizes how important each variable was for the eventual prediction.\n\n\nix <- sample(seq_len(nrow(x)), 1000)\nfit <- train(x = x[ix, ], y = y[ix], method = \"gbm\", verbose = FALSE)\nvarImp(fit)\n\n\ngbm variable importance\n\n               Overall\nvehicle_type  100.0000\norigin          2.5829\nvehicle_class   1.9751\ndestination     1.1214\nfare            0.6559\nhour            0.1539\nmonth           0.0000\n\nIt seems that vehicle_type is by far the most predictive variable. To see its relationship with trip duration, let’s use ridge lines. We create separate histograms split according vehicle_type. We’ll also split histograms within each vehicle type according to origin city, since this was the second most important variable.\n\n\n# reorder for the vehicle types\nvehicle_order <- trips %>%\n  group_by(vehicle_type) %>%\n  summarise(mduration = mean(duration)) %>%\n  arrange(mduration) %>%\n  pull(vehicle_type)\n\ntrips <- trips %>%\n  mutate(vehicle_type = factor(vehicle_type, levels = vehicle_order))\n\n# make the ridge line plot\nggplot(trips, aes(x = duration, y = vehicle_type, fill = origin)) +\n  geom_density_ridges(alpha = 0.8) +\n  labs(y = \"Vehicle Type\", x = \"Duration\", fill = \"Origin City\") +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nThis is a nice explanation. A few of the vehicle_types, like AVE and ALVIA, correspond to high speed express trains. The regional lines, like MD and R Express, tend to take longer to reach their destination. Within a particular type of train, the duration is related to the origin city, likely because that determines the distance between origin and destination.\nWhile you likely would have discovered this pattern after exploring the dataset on your own, in higher-dimensional settings, it can be useful to have mechanisms that can partially automate the process. Our strategy of using a predictive model in R to sort through variables is a practically useful one, but be aware that there is modern research (in the spirit of Profiler) that streamlines the process even further, putting all computation in the background and making use of interactivity to quickly compare views.\n\nI sampled 1 million rows from 2019, the complete dataset is quite a bit larger.↩︎\n",
    "preview": "posts/2021-01-30-week5-4/week5-4_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-22T11:11:22-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-30-week5-3/",
    "title": "Detecting Outliers",
    "description": "Techniques to identify extreme values.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"MASS\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"purrr\")\nlibrary(\"stringr\")\nlibrary(\"tidyr\")\ntheme_set(theme_minimal())\nset.seed(123)\n\n\n\nExtreme values are a common data quality issue. They can either be genuine extreme values or measurement errors — in either case, it’s important to identify them and potentially account for them.\nVisualization can support detection and characterization of anomalies. We’ll review the ideas in the Profiler paper. That work is trying to set the foundation for more complex systems for data cleaning. We’ll look at a few of the interesting, more self-contained ideas within the paper, and demonstrate them in simple examples.\nHow can we detect anomalies in numerical data? A first idea is to use \\(z\\)-scores. For column \\(j\\), estimate the mean \\(\\hat{\\mu}_{j}\\) and standard deviation \\(\\hat{\\sigma}_{j}\\). For each observation \\(i\\), it’s anomalousness can be summarized by \\(z_{ij} := \\frac{x_{ij} - \\hat{\\mu}_{j}}{\\hat{\\sigma}_{j}}\\). This is illustrated below on a normally distributed dataset that has been contaminated with a few outliers from a \\(t\\) distribution.\n\n\nn <- 1000\np <- 0.95\nx <- c(rnorm(n * p), rt(n * (1 - p), df = 3))\nz <- (x - mean(x)) / sd(x)\nx <- data.frame(x = x, z = z, z_ = abs(z))\n\nggplot(x, aes(x = x)) +\n  geom_histogram(binwidth = 0.5) +\n  geom_rug(\n    data = x %>% filter(z_ > 2.5), \n    aes(col = z_),\n    size = 1.5\n  ) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_color_gradient2(low = \"#fffbfc\", high = \"#c50f2c\")\n\n\n\n\nThere is a potential issue with only using z-scores. The presence of anomalies can distort mean and variance estimates. For example, if the standard deviation is distorted, true outliers can be hidden away. To illustrate, consider the means and standard deviations estimated in a variant of the simulation above. The true means and standard deviations are 4 and 2.\n\n\nmake_dataset <- function(n = 2000, p = 0.95, sigma = 2) {\n  x <- c(rnorm(n * p, 0, sigma), rt(n * (1 - p), df = 2 * sigma ^ 2 / (sigma ^ 2 - 1)))\n  data.frame(mu_hat = mean(x), sigma_hat = sd(x), med_hat = median(x), iqr_hat = IQR(x))\n}\n\nx_sim <- map_dfr(1:1000, ~ make_dataset(100, 0.9), .id = \"replicate\")\nggplot(x_sim) + \n  geom_point(aes(x = mu_hat, y = sigma_hat))\n\n\n\n\nThe fact that there are outliers can itself invalidate our \\(z\\)-scores. A more robust alternative is to compare each point to the median and interquartile range (IQR, the difference between the 3rd and 1st quartile). Indeed, for the dataset above, the comparable median and IQR estimates are given below. Note that they are much more stable.\n\n\nggplot(x_sim) + \n  geom_point(aes(x = med_hat, y = iqr_hat))\n\n\n\n\nWhat about multivariate anomalies? They can be tricky to detect because a point can appear typical from a few dimensions viewed independently, but become outlying when the dimensions are viewed together. Consider the example below, where the red point blends into the histograms for either dimension viewed on its own. The data are generated from a bivariate normal distribution, but with one red outlying point at (1.5, -1.5). The point clearly stands out in two dimensions.\n\n\nSigma <- matrix(c(1, 0.9, 0.9, 1), ncol = 2)\nx <- mvrnorm(500, mu = c(0, 0), Sigma = Sigma) %>%\n  rbind(c(1.5, -1.5)) %>%\n  data.frame() %>%\n  mutate(type = c(rep(\"normal\", 500), \"anomaly\"))\n\nggplot(x, aes(x = X1, y = X2, col = type)) +\n  geom_point() +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  coord_fixed()\n\n\n\n\nHowever, if we had only looked at one-dimensional \\(z\\)-scores, we would have completely missed the anomaly.\n\n\nx_long <- x %>%\n  pivot_longer(X1:X2, names_to = \"dimension\")\n\nggplot(x_long, aes(x = value)) +\n  geom_histogram(binwidth = 0.4) +\n  geom_rug(\n    data = x_long %>% filter(type == \"anomaly\"), \n    col = \"red\", size = 2 \n  ) +\n  scale_y_continuous(expand = c(0, 0)) +\n  facet_wrap(~ dimension) +\n  theme(panel.border = element_rect(fill = NA, size = 1))\n\n\n\n\nTo be able to detect these types of multidimensional outliers, the Profiler paper uses the Mahalanobis distance. This distance works by estimating a jointly normal distribution across dimensions and then looking for points that would have low probability under the estimate distribution.\n\n\nmu_hat <- apply(x[, 1:2], 2, mean)\nsigma_hat <- cov(x[, 1:2])\nx <- x %>%\n  mutate(D2 = mahalanobis(x[, 1:2], mu_hat, sigma_hat))\n\nggplot(x) +\n  geom_point(\n    aes(x = X1, y = X2, col = sqrt(D2))\n  ) +\n  scale_color_gradient2(low = \"#fffbfc\", high = \"#c50f2c\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-01-30-week5-3/week5-3_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-22T11:08:30-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-30-week5-2/",
    "title": "Missing Data (Part 2)",
    "description": "A deeper look at missing data, imputation, and characterization.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-22",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"MASS\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"naniar\")\nlibrary(\"rpart\")\nlibrary(\"simputation\")\n\n\n\nThe previous notes described how visualization can help quantify how much missingness if present, and where it occurs. Here, we will explore how visualization is also helpful in efforts to impute and characterize missing values.\nFor example, visualization is helpful for understanding the results of imputation algorithms. Before describing this, it will be helpful to have a crash course on missing data imputation.\nImputation algorithms try to replace all the missing values with plausible values. The reason we might do this is that we don’t want to discard all the observations with the missing values, but our usual models might throw errors if we provide data with missing values.\nMedian imputation replaces each missing values in any given column by the median of the observed values in that field. It works one column at a time.\n\n\nx <- data.frame(value = c(rnorm(450), rep(NA, 50))) %>%\n  sample_frac(1) %>% # randomly reorder\n  bind_shadow() %>% # create column checking if missing\n  mutate(imputed = naniar::impute_median(value))\n\nx\n\n\n# A tibble: 500 x 3\n     value value_NA imputed\n     <dbl> <fct>      <dbl>\n 1 -1.01   !NA      -1.01  \n 2 -0.0649 !NA      -0.0649\n 3 -0.600  !NA      -0.600 \n 4  0.468  !NA       0.468 \n 5 -0.0619 !NA      -0.0619\n 6 -1.90   !NA      -1.90  \n 7 -0.561  !NA      -0.561 \n 8 -0.677  !NA      -0.677 \n 9  0.226  !NA       0.226 \n10 -0.894  !NA      -0.894 \n# … with 490 more rows\n\n\n\nggplot(x) +\n  geom_histogram(aes(x = imputed, fill = value_NA)) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nWe can be cleverer though, if we imagine that multiple columns are related in some way. Consider the scatterplot below. When both observations are present, a point is drawn along the middle. When there is a missing value in one of the columns, we plot the one dimension that we do have along the appropriate edge. How would you impute the second column if you knew its value in the first column was low?\n\n\nSigma <- matrix(c(1, 0.9, 0.9, 1), 2)\nx <- mvrnorm(500, c(0, 0), Sigma = Sigma) %>%\n  as.data.frame()\nfor (j in seq_len(2)) {\n  x[sample(nrow(x), 50), j] <- NA\n}\n\nggplot(x) +\n  geom_miss_point(aes(x = V1, y = V2), jitter = 0) +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_fixed()\n\n\n\n\nYou would probably project the missing values onto the line that goes through the bulk of the fully observed data.\n\n\n\nMultiple imputation methods formalize this intuition. They make use of multivariate relationships between columns to guess plausible values for missing data. This is an example of multiple imputation on the airquality dataset – the multivariate relationship between ozone, temperature, and wind is used to fill in missing values for ozone.\n\n\naq_imputed <- airquality %>%\n  bind_shadow() %>%\n  as.data.frame() %>%\n  impute_lm(Ozone ~ Temp + Wind)\n\nggplot(aq_imputed) + \n  geom_point(aes(x = Temp, y = Ozone)) +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nBut how can we tell if a multiple imputation method is effective? We can plot the data, making sure to distinguish between true and imputed observations.\n\n\nggplot(aq_imputed) + \n  geom_point(aes(x = Temp, y = Ozone, col = Ozone_NA)) +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nFinally, let’s ask, why are the data missing in the first place? A natural idea is to try to find characteristics of observations that are predictive of their having missing values for some particular field. For example, maybe respondents within a given age group always leave a question blank. To this end, a well chosen plot can be very suggestive.\nWe can illustrate this idea with the airquality data. It seems that the sensor might have broken down in June. When there are very many possible variables to compare against, a model can be especially helpful for guiding the search for informative plots. We’ll develop this idea further two lectures from now, when we look at how mutual information is used to guide the visualization search in Profiler.\n\n\nrpart_model <- airquality %>%\n  add_prop_miss() %>%\n  rpart(prop_miss_all ~ ., data = .)\nrpart_model$variable.importance\n\n\n     Month       Temp    Solar.R        Day       Wind      Ozone \n0.20606801 0.18878180 0.13064962 0.09943322 0.07316449 0.00638807 \n\n\n\nggplot(airquality) +\n  geom_miss_point(aes(x = Ozone, y = Solar.R)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(~ Month)\n\n\n\n\nIf we can predict missingness well, then the data are not missing at random. The bad news is that more specialized imputation strategies may be necessary. The good news is that we may be able to actually characterize the mechanism behind the missing data.\n\n\n\n",
    "preview": "posts/2021-01-30-week5-2/week5-2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-02-21T15:46:05-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-30-week5-1/",
    "title": "Missing Data (Part 1)",
    "description": "A look at how visualization can help characterize missing data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-21",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"naniar\")\nlibrary(\"UpSetR\")\ntheme_set(theme_bw())\n\n\n\nVisualization is about more than simply communicating results at the end of a data analysis. It can be used to improve the process of data analysis itself. This is the subject of this week’s notes.\nWhen data science workflows are opaque — when sequences of commands are followed blindly — that’s when mistakes are most likely to be made1. Visualization can help improve the transparency of different steps across the workflow.\nTo make this idea concrete, let’s consider missing data. We can encounter missing data for a variety of reasons. Perhaps a sensor was broken or no response was submitted. Maybe several datasets were merged, but some fields are only present in the more recent versions.\nIf there are missing data, and if we fail to account for it, we might accidentally draw incorrect conclusions. Or, if we are going to attempt to impute the missing values, we should verify the assumptions of whatever imputation algorithm we intend to use.\nA first plot to make is to look at the proportion of missing data within each field. Let’s consider the riskfactors data, which is a subset from the Behavioral Risk Factor Surveillance System.\n\n\ngg_miss_var(riskfactors)\n\n\n\n\nThis histogram doesn’t tell us whether missing data tend to co-occur across a few different dimensions. For that, we can count the different missingness patterns.\n\n\ngg_miss_upset(riskfactors)\n\n\n\n\nNeither of these approaches tell us whether missing data tend to occur in clumps along rows of the data. We might expect this if our sensor broke down within a particular time interval, for example. For this, we can make a type of heatmap across all the data.\n\n\nvis_miss(riskfactors)\n\n\n\n\nThere doesn’t seem to be much clumping in that dataset, but consider the airquality dataset below, whose rows are sorted by time.\n\n\nvis_miss(airquality)\n\n\n\n\nThis visualization actually has a formal name, the shadow matrix2, and it comes up in theoretical analysis of missing data.\n\nProf. Broman has noted a compendium of horror stories↩︎\nWhich when you think about it sounds pretty dystopian / scifi.↩︎\n",
    "preview": "posts/2021-01-30-week5-1/week5-1_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-21T15:45:57-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-27-week4-4/",
    "title": "Tidy Data Example",
    "description": "An extended example of tidying a real-world dataset.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-18",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"stringr\")\nlibrary(\"skimr\")\n\n\n\nLet’s work through the example of tidying a WHO dataset. This was discussed in the reading and is good practice in pivoting and deriving new variables.\nThe raw data, along with a summary from the skimr package, is shown below (notice the small multiples!).\n\n\nwho\n\n\n# A tibble: 7,240 x 60\n   country iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534\n   <chr>   <chr> <chr> <int>       <int>        <int>        <int>\n 1 Afghan… AF    AFG    1980          NA           NA           NA\n 2 Afghan… AF    AFG    1981          NA           NA           NA\n 3 Afghan… AF    AFG    1982          NA           NA           NA\n 4 Afghan… AF    AFG    1983          NA           NA           NA\n 5 Afghan… AF    AFG    1984          NA           NA           NA\n 6 Afghan… AF    AFG    1985          NA           NA           NA\n 7 Afghan… AF    AFG    1986          NA           NA           NA\n 8 Afghan… AF    AFG    1987          NA           NA           NA\n 9 Afghan… AF    AFG    1988          NA           NA           NA\n10 Afghan… AF    AFG    1989          NA           NA           NA\n# … with 7,230 more rows, and 53 more variables: new_sp_m3544 <int>,\n#   new_sp_m4554 <int>, new_sp_m5564 <int>, new_sp_m65 <int>,\n#   new_sp_f014 <int>, new_sp_f1524 <int>, new_sp_f2534 <int>,\n#   new_sp_f3544 <int>, new_sp_f4554 <int>, new_sp_f5564 <int>,\n#   new_sp_f65 <int>, new_sn_m014 <int>, new_sn_m1524 <int>,\n#   new_sn_m2534 <int>, new_sn_m3544 <int>, new_sn_m4554 <int>,\n#   new_sn_m5564 <int>, new_sn_m65 <int>, new_sn_f014 <int>,\n#   new_sn_f1524 <int>, new_sn_f2534 <int>, new_sn_f3544 <int>,\n#   new_sn_f4554 <int>, new_sn_f5564 <int>, new_sn_f65 <int>,\n#   new_ep_m014 <int>, new_ep_m1524 <int>, new_ep_m2534 <int>,\n#   new_ep_m3544 <int>, new_ep_m4554 <int>, new_ep_m5564 <int>,\n#   new_ep_m65 <int>, new_ep_f014 <int>, new_ep_f1524 <int>,\n#   new_ep_f2534 <int>, new_ep_f3544 <int>, new_ep_f4554 <int>,\n#   new_ep_f5564 <int>, new_ep_f65 <int>, newrel_m014 <int>,\n#   newrel_m1524 <int>, newrel_m2534 <int>, newrel_m3544 <int>,\n#   newrel_m4554 <int>, newrel_m5564 <int>, newrel_m65 <int>,\n#   newrel_f014 <int>, newrel_f1524 <int>, newrel_f2534 <int>,\n#   newrel_f3544 <int>, newrel_f4554 <int>, newrel_f5564 <int>,\n#   newrel_f65 <int>\n\n\n\nskim(who)\n\n\nTable 1: Data summary\nName\nwho\nNumber of rows\n7240\nNumber of columns\n60\n_______________________\n\nColumn type frequency:\n\ncharacter\n3\nnumeric\n57\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ncountry\n0\n1\n4\n52\n0\n219\n0\niso2\n0\n1\n2\n2\n0\n219\n0\niso3\n0\n1\n3\n3\n0\n219\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nyear\n0\n1.00\n1996.56\n9.83\n1980\n1988.00\n1997.0\n2005.00\n2013\n▇▇▇▇▇\nnew_sp_m014\n4067\n0.44\n83.71\n316.14\n0\n0.00\n5.0\n37.00\n5001\n▇▁▁▁▁\nnew_sp_m1524\n4031\n0.44\n1015.66\n4885.38\n0\n9.00\n90.0\n502.00\n78278\n▇▁▁▁▁\nnew_sp_m2534\n4034\n0.44\n1403.80\n5718.39\n0\n14.00\n150.0\n715.50\n84003\n▇▁▁▁▁\nnew_sp_m3544\n4021\n0.44\n1315.88\n6003.26\n0\n13.00\n130.0\n583.50\n90830\n▇▁▁▁▁\nnew_sp_m4554\n4017\n0.45\n1103.86\n5441.06\n0\n12.00\n102.0\n440.00\n82921\n▇▁▁▁▁\nnew_sp_m5564\n4022\n0.44\n800.70\n4418.31\n0\n8.00\n63.0\n279.00\n63814\n▇▁▁▁▁\nnew_sp_m65\n4031\n0.44\n682.82\n4089.14\n0\n8.00\n53.0\n232.00\n70376\n▇▁▁▁▁\nnew_sp_f014\n4066\n0.44\n114.33\n504.63\n0\n1.00\n7.0\n50.75\n8576\n▇▁▁▁▁\nnew_sp_f1524\n4046\n0.44\n826.11\n3552.02\n0\n7.00\n66.0\n421.00\n53975\n▇▁▁▁▁\nnew_sp_f2534\n4040\n0.44\n917.30\n3580.15\n0\n9.00\n84.0\n476.25\n49887\n▇▁▁▁▁\nnew_sp_f3544\n4041\n0.44\n640.43\n2542.51\n0\n6.00\n57.0\n308.00\n34698\n▇▁▁▁▁\nnew_sp_f4554\n4036\n0.44\n445.78\n1799.23\n0\n4.00\n38.0\n211.00\n23977\n▇▁▁▁▁\nnew_sp_f5564\n4045\n0.44\n313.87\n1381.25\n0\n3.00\n25.0\n146.50\n18203\n▇▁▁▁▁\nnew_sp_f65\n4043\n0.44\n283.93\n1267.94\n0\n4.00\n30.0\n129.00\n21339\n▇▁▁▁▁\nnew_sn_m014\n6195\n0.14\n308.75\n1727.25\n0\n1.00\n9.0\n61.00\n22355\n▇▁▁▁▁\nnew_sn_m1524\n6210\n0.14\n513.02\n3643.27\n0\n2.00\n15.5\n102.00\n60246\n▇▁▁▁▁\nnew_sn_m2534\n6218\n0.14\n653.69\n3430.03\n0\n2.00\n23.0\n135.50\n50282\n▇▁▁▁▁\nnew_sn_m3544\n6215\n0.14\n837.87\n8524.53\n0\n2.00\n19.0\n132.00\n250051\n▇▁▁▁▁\nnew_sn_m4554\n6213\n0.14\n520.79\n3301.70\n0\n2.00\n19.0\n127.50\n57181\n▇▁▁▁▁\nnew_sn_m5564\n6219\n0.14\n448.62\n3488.68\n0\n2.00\n16.0\n101.00\n64972\n▇▁▁▁▁\nnew_sn_m65\n6220\n0.14\n460.36\n3991.90\n0\n2.00\n20.5\n111.75\n74282\n▇▁▁▁▁\nnew_sn_f014\n6200\n0.14\n291.95\n1647.30\n0\n1.00\n8.0\n58.00\n21406\n▇▁▁▁▁\nnew_sn_f1524\n6218\n0.14\n407.90\n2379.13\n0\n1.00\n12.0\n89.00\n35518\n▇▁▁▁▁\nnew_sn_f2534\n6224\n0.14\n466.26\n2272.86\n0\n2.00\n18.0\n103.25\n28753\n▇▁▁▁▁\nnew_sn_f3544\n6220\n0.14\n506.59\n5013.53\n0\n1.00\n11.0\n82.25\n148811\n▇▁▁▁▁\nnew_sn_f4554\n6222\n0.14\n271.16\n1511.72\n0\n1.00\n10.0\n76.75\n23869\n▇▁▁▁▁\nnew_sn_f5564\n6223\n0.14\n213.39\n1468.62\n0\n1.00\n8.0\n56.00\n26085\n▇▁▁▁▁\nnew_sn_f65\n6221\n0.14\n230.75\n1597.70\n0\n1.00\n13.0\n74.00\n29630\n▇▁▁▁▁\nnew_ep_m014\n6202\n0.14\n128.61\n460.14\n0\n0.00\n6.0\n55.00\n7869\n▇▁▁▁▁\nnew_ep_m1524\n6214\n0.14\n158.30\n537.74\n0\n1.00\n11.0\n88.00\n8558\n▇▁▁▁▁\nnew_ep_m2534\n6220\n0.14\n201.23\n764.05\n0\n1.00\n13.0\n124.00\n11843\n▇▁▁▁▁\nnew_ep_m3544\n6216\n0.14\n272.72\n3381.41\n0\n1.00\n10.5\n91.25\n105825\n▇▁▁▁▁\nnew_ep_m4554\n6220\n0.14\n108.11\n380.61\n0\n1.00\n8.5\n63.25\n5875\n▇▁▁▁▁\nnew_ep_m5564\n6225\n0.14\n72.17\n234.55\n0\n1.00\n7.0\n46.00\n3957\n▇▁▁▁▁\nnew_ep_m65\n6222\n0.14\n78.94\n227.34\n0\n1.00\n10.0\n55.00\n3061\n▇▁▁▁▁\nnew_ep_f014\n6208\n0.14\n112.89\n446.55\n0\n0.00\n5.0\n50.00\n6960\n▇▁▁▁▁\nnew_ep_f1524\n6219\n0.14\n149.17\n543.89\n0\n1.00\n9.0\n78.00\n7866\n▇▁▁▁▁\nnew_ep_f2534\n6219\n0.14\n189.52\n761.79\n0\n1.00\n12.0\n95.00\n10759\n▇▁▁▁▁\nnew_ep_f3544\n6219\n0.14\n241.70\n3218.50\n0\n1.00\n9.0\n77.00\n101015\n▇▁▁▁▁\nnew_ep_f4554\n6223\n0.14\n93.77\n339.33\n0\n1.00\n8.0\n56.00\n6759\n▇▁▁▁▁\nnew_ep_f5564\n6223\n0.14\n63.04\n212.95\n0\n1.00\n6.0\n42.00\n4684\n▇▁▁▁▁\nnew_ep_f65\n6226\n0.14\n72.31\n202.72\n0\n0.00\n10.0\n51.00\n2548\n▇▁▁▁▁\nnewrel_m014\n7050\n0.03\n538.18\n2082.18\n0\n5.00\n32.5\n210.00\n18617\n▇▁▁▁▁\nnewrel_m1524\n7058\n0.03\n1489.51\n6848.18\n0\n17.50\n171.0\n684.25\n84785\n▇▁▁▁▁\nnewrel_m2534\n7057\n0.03\n2139.72\n7539.87\n0\n25.00\n217.0\n1091.00\n76917\n▇▁▁▁▁\nnewrel_m3544\n7056\n0.03\n2036.40\n7847.94\n0\n24.75\n208.0\n851.25\n84565\n▇▁▁▁▁\nnewrel_m4554\n7056\n0.03\n1835.07\n8324.28\n0\n19.00\n175.0\n688.50\n100297\n▇▁▁▁▁\nnewrel_m5564\n7055\n0.03\n1525.30\n8760.27\n0\n13.00\n136.0\n536.00\n112558\n▇▁▁▁▁\nnewrel_m65\n7058\n0.03\n1426.00\n9431.99\n0\n17.00\n117.0\n453.50\n124476\n▇▁▁▁▁\nnewrel_f014\n7050\n0.03\n532.84\n2117.78\n0\n5.00\n32.5\n226.00\n18054\n▇▁▁▁▁\nnewrel_f1524\n7056\n0.03\n1161.85\n4606.76\n0\n10.75\n123.0\n587.75\n49491\n▇▁▁▁▁\nnewrel_f2534\n7058\n0.03\n1472.80\n5259.59\n0\n18.00\n161.0\n762.50\n44985\n▇▁▁▁▁\nnewrel_f3544\n7057\n0.03\n1125.01\n4210.58\n0\n12.50\n125.0\n544.50\n38804\n▇▁▁▁▁\nnewrel_f4554\n7057\n0.03\n877.27\n3556.18\n0\n10.00\n92.0\n400.50\n37138\n▇▁▁▁▁\nnewrel_f5564\n7057\n0.03\n686.41\n3379.33\n0\n8.00\n69.0\n269.00\n40892\n▇▁▁▁▁\nnewrel_f65\n7055\n0.03\n683.76\n3618.47\n0\n9.00\n69.0\n339.00\n47438\n▇▁▁▁▁\n\nAccording to the data dictionary, the columns have the following meanings,\nThe first three letters -> are we counting new or old cases of TB?\nNext two letters -> Type of tab.\nSixth letter -> Sex of patients\nRemaining numbers -> Age group. E.g., 3544 should be interpreted as 35 - 44 years old.\nOur first step is to pivot_longer. There is quite a bit of information implicitly stored in the column names, and we want to make those variables explicitly available for visual encoding.\n\n\nwho_longer <- who %>% \n  pivot_longer(\n    cols = new_sp_m014:newrel_f65,  # notice we can refer to groups of columns without naming each one\n    names_to = \"key\", \n    values_to = \"cases\", \n    values_drop_na = TRUE # if a cell is empty, we do not keep it in the tidy version\n  )\n\nwho_longer\n\n\n# A tibble: 76,046 x 6\n   country     iso2  iso3   year key          cases\n   <chr>       <chr> <chr> <int> <chr>        <int>\n 1 Afghanistan AF    AFG    1997 new_sp_m014      0\n 2 Afghanistan AF    AFG    1997 new_sp_m1524    10\n 3 Afghanistan AF    AFG    1997 new_sp_m2534     6\n 4 Afghanistan AF    AFG    1997 new_sp_m3544     3\n 5 Afghanistan AF    AFG    1997 new_sp_m4554     5\n 6 Afghanistan AF    AFG    1997 new_sp_m5564     2\n 7 Afghanistan AF    AFG    1997 new_sp_m65       0\n 8 Afghanistan AF    AFG    1997 new_sp_f014      5\n 9 Afghanistan AF    AFG    1997 new_sp_f1524    38\n10 Afghanistan AF    AFG    1997 new_sp_f2534    36\n# … with 76,036 more rows\n\nThe new column key contains several variables at once. We can separate it into gender and age group.\n\n\nwho_separate <- who_longer %>% \n  mutate(key = str_replace(key, \"newrel\", \"new_rel\")) %>%\n  separate(key, c(\"new\", \"type\", \"sexage\"), sep = \"_\") %>%\n  separate(sexage, c(\"sex\", \"age\"), sep = 1)\n\nwho_separate\n\n\n# A tibble: 76,046 x 9\n   country     iso2  iso3   year new   type  sex   age   cases\n   <chr>       <chr> <chr> <int> <chr> <chr> <chr> <chr> <int>\n 1 Afghanistan AF    AFG    1997 new   sp    m     014       0\n 2 Afghanistan AF    AFG    1997 new   sp    m     1524     10\n 3 Afghanistan AF    AFG    1997 new   sp    m     2534      6\n 4 Afghanistan AF    AFG    1997 new   sp    m     3544      3\n 5 Afghanistan AF    AFG    1997 new   sp    m     4554      5\n 6 Afghanistan AF    AFG    1997 new   sp    m     5564      2\n 7 Afghanistan AF    AFG    1997 new   sp    m     65        0\n 8 Afghanistan AF    AFG    1997 new   sp    f     014       5\n 9 Afghanistan AF    AFG    1997 new   sp    f     1524     38\n10 Afghanistan AF    AFG    1997 new   sp    f     2534     36\n# … with 76,036 more rows\n\nWhile we have performed each step one at a time, it’s possible to chain them into a single block of code. This is good practice, because it avoids having to define intermediate variables that are only ever used once. This is also typically more concise.\n\n\nwho %>%\n  pivot_longer(\n    cols = new_sp_m014:newrel_f65, \n    names_to = \"key\", \n    values_to = \"cases\", \n    values_drop_na = TRUE # if a cell is empty, we do not keep it in the tidy version\n  ) %>%\n  mutate(key = str_replace(key, \"newrel\", \"new_rel\")) %>%\n  separate(key, c(\"new\", \"type\", \"sexage\"), sep = \"_\") %>%\n  separate(sexage, c(\"sex\", \"age\"), sep = 1)\n\n\n# A tibble: 76,046 x 9\n   country     iso2  iso3   year new   type  sex   age   cases\n   <chr>       <chr> <chr> <int> <chr> <chr> <chr> <chr> <int>\n 1 Afghanistan AF    AFG    1997 new   sp    m     014       0\n 2 Afghanistan AF    AFG    1997 new   sp    m     1524     10\n 3 Afghanistan AF    AFG    1997 new   sp    m     2534      6\n 4 Afghanistan AF    AFG    1997 new   sp    m     3544      3\n 5 Afghanistan AF    AFG    1997 new   sp    m     4554      5\n 6 Afghanistan AF    AFG    1997 new   sp    m     5564      2\n 7 Afghanistan AF    AFG    1997 new   sp    m     65        0\n 8 Afghanistan AF    AFG    1997 new   sp    f     014       5\n 9 Afghanistan AF    AFG    1997 new   sp    f     1524     38\n10 Afghanistan AF    AFG    1997 new   sp    f     2534     36\n# … with 76,036 more rows\n\nA recommendation for visualization in javascript. We have only discussed tidying in R. While there is work to implement tidy-style transformations in javascript, the R tidyverse provides a more mature suite of tools. If you are making an interactive visualization in javascript, I recommend first tidying data in R so that each row corresponds to a visual mark and each column to a visual property. You can always save the result as either a json or csv, which can serve as the source data for your javascript visualization.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-07T11:13:22-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-27-week4-3/",
    "title": "Deriving Variables",
    "description": "Using `separate`, `mutate`, and `summarise` to derive new variables for\ndownstream visualization.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-17",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"tidyr\")\nlibrary(\"dplyr\")\n\n\n\nIt’s easiest to define visual encodings when the variables we want to encode are contained in their own columns. After sketching out a visualization of interest, we may find that these variables are not explicitly represented among the columns of the raw dataset. In this case, we may need to derive them based on what is available. The dplyr and tidyr packages provide functions for deriving new variables, which we review in these notes.\nSometimes a single column is used to implicitly store several variables. To make the data tidy, separate can be used to split that single column into several columns, each of which corresponds to exactly one variable.\nThe block below separates our earlier table3, which stored rate as a fraction in a character column. The original table was,\n\n\ntable3\n\n\n# A tibble: 6 x 3\n  country      year rate             \n* <chr>       <int> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\nand the separated version is,\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) # try without convert, and compare the data types of the columns\n\n\n# A tibble: 6 x 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nNote that this function has an inverse, called unite, which can merge several columns into one. This is sometimes useful, but not as often as separate, since it isn’t needed to tidy a dataset.\nSeparating a single column into several is a special case of a more general operation, mutate, which defines new columns as functions of existing ones. We have used this is in previous lectures, but now we can philosophically justify it: the variables we want to encode need to be defined in advance.\nFor example, we may want to create a column rate that includes cases over population,\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) %>%\n  mutate(rate = cases / year)\n\n\n# A tibble: 6 x 5\n  country      year  cases population    rate\n  <chr>       <int>  <int>      <int>   <dbl>\n1 Afghanistan  1999    745   19987071   0.373\n2 Afghanistan  2000   2666   20595360   1.33 \n3 Brazil       1999  37737  172006362  18.9  \n4 Brazil       2000  80488  174504898  40.2  \n5 China        1999 212258 1272915272 106.   \n6 China        2000 213766 1280428583 107.   \n\nSometimes, the variables of interest are functions of several rows. For example, perhaps we want to visualize averages of a variable across age groups. In this case, we can derive a summary across groups of rows using the group_by-followed-by-summarise pattern.\nFor example, perhaps we want the average rate over both years.\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) %>%\n  mutate(rate = cases / year) %>%\n  group_by(country) %>%\n  summarise(avg_rate = mean(rate))\n\n\n# A tibble: 3 x 2\n  country     avg_rate\n* <chr>          <dbl>\n1 Afghanistan    0.853\n2 Brazil        29.6  \n3 China        107.   \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-07T11:13:17-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-27-week4-2/",
    "title": "Pivoting",
    "description": "Tools for reshaping data into tidy format.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-16",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\ntheme_set(theme_bw())\n\n\n\nPivoting refers to the process of changing the interpretation of each row in a data frame. It is useful for addressing problems 1 - 2 in the previous lecture, which we repeat here for completeness.\nA variable might be implicitly stored within column names, rather than explicitly stored in its own column.\nThe same observation may appear in multiple rows, where each instance of the row is associated with a different variable.\n\nTo address (a), we can use the pivot_longer function in tidyr. It takes an implicitly stored variable and explicitly stores it in a column defined by the names_to argument. In\nThe example below shows pivot_longer being used to tidy one of the non-tidy tuberculosis datasets. Note that the data has doubled in length, because there are now two rows per country (one per year).\nFor reference, these are the original data.\n\n\ntable4a\n\n\n# A tibble: 3 x 3\n  country     `1999` `2000`\n* <chr>        <int>  <int>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\nThis step lengthens the data,\n\n\ntable4a_longer <- table4a %>% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntable4a_longer\n\n\n# A tibble: 6 x 3\n  country     year   cases\n  <chr>       <chr>  <int>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\ndim(table4a)\n\n\n[1] 3 3\n\ndim(table4a_longer)\n\n\n[1] 6 3\n\nWe can pivot both the population and the cases table, then combine them using a join operation. A join operation matches rows across two tables according to their shared columns.\n\n\n# helper function, to avoid copying and pasting code\npivot_fun <- function(x, value_column = \"cases\") {\n  x %>%\n    pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = value_column)\n}\n\ntable4 <- left_join(\n  pivot_fun(table4a), # look for all country x year combinations in left table\n  pivot_fun(table4b, \"population\") # and find matching rows in right table\n)\ntable4\n\n\n# A tibble: 6 x 4\n  country     year   cases population\n  <chr>       <chr>  <int>      <int>\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\nThis lets us make the year vs. rate plot that we had tried to put together in the last lecture. It’s much easier to recognize trends when comparing the rates, than when looking at the raw case counts.\n\n\nggplot(table4, aes(x = year, y = cases / population, col = country)) +\n  geom_point() +\n  geom_line(aes(group = country))\n\n\n\n\nTo address (b), we can use the pivot_wider function. It spreads the column in the values_from argument across new columns specified by the names_from argument.\nThe example below shows pivot_wider being used to tidy one of the other non-tidy datasets. Note when there are more than two levels in the names_from column, this will always be wider than the starting data frame, which is why this operation is called pivot_wider.\nFor reference, here is table2 before pivoting.\n\n\ntable2\n\n\n# A tibble: 12 x 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\nNow, we spread the cases and population variables into their own columns.\n\n\ntable2 %>%\n    pivot_wider(names_from = type, values_from = count)\n\n\n# A tibble: 6 x 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n",
    "preview": "posts/2021-01-27-week4-2/week4-2_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-02-07T11:13:16-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-27-week4-1/",
    "title": "Tidy Data",
    "description": "The definition of tidy data, and why it's often helpful for visualization.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-15",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\ntheme_set(theme_bw())\n\n\n\nA dataset is called tidy if rows correspond to distinct observations and columns correspond to distinct variables.\n\nFor visualization, it is important that data be in tidy format. This is because (a) each visual mark will be associated with a row of the dataset and (b) properties of the visual marks will determined by values within the columns. A plot that is easy to create when the data are in tidy format might be very hard to create otherwise.\nThe tidy data might seem like an idea so natural that it’s not worth teaching (let alone formalizing). However, exceptions are encountered frequently, and it’s important that you be able to spot them. Further, there are now many utilities for “tidying” data, and they are worth becoming familiar with.\nHere is an example of a tidy dataset.\n\n\ntable1\n\n\n# A tibble: 6 x 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nIt is easy to visualize the tidy dataset.\n\n\nggplot(table1, aes(x = year, y = cases, col = country)) +\n  geom_point() +\n  geom_line()\n\n\n\n\nBelow are three non-tidy versions of the same dataset. They are representative of more general classes of problems that may arise,\nA variable might be implicitly stored within column names, rather than explicitly stored in its own column. Here, the years are stored as column names. It’s not really possible to create the plot above using the data in this format.\n\n\n\ntable4a # cases\n\n\n# A tibble: 3 x 3\n  country     `1999` `2000`\n* <chr>        <int>  <int>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b # population\n\n\n# A tibble: 3 x 3\n  country         `1999`     `2000`\n* <chr>            <int>      <int>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\nThe same observation may appear in multiple rows, where each instance of the row is associated with a different variable. Here, the observations are the country by year combinations.\n\n\ntable2\n\n\n# A tibble: 12 x 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\nA single column actually stores multiple variables. Here, rate is being used to store both the population and case count variables.\n\n\ntable3\n\n\n# A tibble: 6 x 3\n  country      year rate             \n* <chr>       <int> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\nThe trouble is that this variable has to be stored as a character; otherwise, we lose access to the original population and case variable. But, this makes the plot useless.\n\n\nggplot(table3, aes(x = year, y = rate)) +\n  geom_point() +\n  geom_line(aes(group = country))\n\n\n\n\nThe next few lectures provide tools for addressing these three problems.\nA few caveats are in order. It’s easy to become a tidy-data purist, and lose sight of the bigger data-analytic picture. To prevent that, first, remember that what is or is not tidy may be context dependent. Maybe you want to treat each week as an observation, rather than each day. Second, know that there are sometimes computational reasons to prefer non-tidy data. For example, “long” data often require more memory, since column names that were originally stored once now have to be copied onto each row. Certain statistical models are also sometimes best framed as matrix operations on non-tidy datasets.\n\n\n\n",
    "preview": "posts/2021-01-27-week4-1/week4-1_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-02-07T11:13:14-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-25-week3-5/",
    "title": "Dynamic Linking in the Wild",
    "description": "A look at real-world examples of dynamic linking.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nThese notes consider real-world systems that use dynamic linking. They are less practical most of the notes in this course, but there are approaches that are good to know about, and which you might want to learn about from a hands-on point of view in your future projects.\nOverview + Detail: Google Maps. We can see details of a location without losing context of the surrounding area.\n\nMulti-form Display: Microarray Explorer\n\nOne encoding through a central scatterplot.\nA complementary encoding through time series.\nDetail names of the selected genes. This is more convenient than having to tooltip over all the individual genes.\nSelections in one display update all the others.\nMultiple techniques: Cerebral.\n a. There are many pieces in this visualization. Let’s deconstruct the components,\nA gene network (known interactions)\nSmall multiples showing the same network colored in at different time points\nTime series showing the individual genes over time. (value of multiform: time series make changes over time obvious, but network shows relationships between genes)\nLet’s examine the visualization strategies being used,\nMulti-form visualization: Time series and networks refer to the same underlying genes, but support different comparisons.\nSmall-multiples: One of the panels shows the same genes shaded in across many time points.\nOverview + detail: The central display summaries the gene networks over time, and the small multiples can be used to understand the network at a particular timepoint.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-12T08:43:46-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-25-week3-4/",
    "title": "Dynamic Linking",
    "description": "Combining faceting with dynamic queries.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-12",
    "categories": [],
    "contents": "\nReading, Recording, Notebook, Rmarkdown\nIt is often useful to combine multi-view composition with dynamic queries. The basic idea is to (a) show different aspects of a dataset using different views, and then (b) link the views using dynamic queries. This strategy is sometimes called dynamic linking.\nTo make this concrete, consider cross-filtering. In the visualization below, we can see how different fields of a flights dataset are related to one another.\n\n\nrobservable(\"@krisrs1128/week-3-4\", include = 4, height = 235)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/week-3-4\",\"include\":4,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\n{\n  const brush = vl.selectInterval().encodings('x').resolve('intersect'),\n        hist = vl.markBar().encode(\n          vl.x().fieldQ(vl.repeat('row')).bin({maxbins: 100}), // up to 100 bins, but no smaller than 1 unit\n          vl.y().count().title(null) // no y-axis title\n        );\n  \n  return vl.layer(\n      hist.select(brush).encode(vl.color().value('lightgrey')),\n      hist.transform(vl.filter(brush))\n    )\n    .width(900).height(100)\n    .repeat({row: ['delay', 'distance', 'time']})\n    .data(flights)\n    .config({view: {stroke: null}}) // no outline\n    .render();\n}\n\n\nrobservable(\"@krisrs1128/week-3-4\", include = 5, height = 430)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/week-3-4\",\"include\":5,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nThe implementation here is subtle.\nThree grey histograms are drawn using the repeat strategy. These serve as a reference and are never updated.\nA selectInterval is bound to each of these grey histograms. This is what lets us select ranges of values, with which we will update the blue histogram.\nThe blue histograms are then drawn using only the data that has passed through a filter defined by the above selectInterval. Every time the brush is moved, the histograms are recomputed and redrawn.\n\nAside: Scented widgets are an other example of how coordinated views and dynamic queries are useful together. There, though, one of the views had a more central role.\nConsidered abstractly, there are two axes along which we can think of coordinated views. The first is whether we are encoding the data in the same way. Second, are we sharing the same data across the different views? We can recover many of our earlier plots from this point of view, and it is a useful conceptual aid when exploring the space of design possibilities for a dataset.\nWhy are these all effective?\nOverview + detail: Transition between levels of complexity, without losing context.\nSmall multiples: Enable high information density by sharing the same encodings across many subsets.\nMulti-form: Allows different encodings that are suited to complementary visual tasks.\n\n",
    "preview": {},
    "last_modified": "2021-01-31T18:10:46-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-25-week3-3/",
    "title": "Dynamic Queries (Part 2)",
    "description": "An introduction to details-on-demand.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [],
    "contents": "\nReading, Recording, Notebook, Rmarkdown\nOne common type of dynamic query is to ask the identity of a particular observation. A generalization of this query is the details-on-demand class of interactions. We will see examples of both in these notes.\nTo reveal the identity of an observation, we can reveal a label on hover. This is essentially a tooltip, but we have re-implemented it manually using a selection. The reason is that this allows additional customization in the structure of the interaction and visual update.\n{\n  /* Define behavior of hover and click selections */\n  const hover = vl.selectSingle()\n    .on('mouseover') // select on mouseover\n    .nearest(true)   // select nearest point to mouse cursor\n    .empty('none');  // empty selection should match nothing\n  \n  const click = vl.selectMulti()\n    .empty('none');  // empty selection matches no points\n  \n  /* Define encodings for full and filtered data */\n  const all_points = vl.markCircle().encode(\n    vl.x().fieldQ('Rotten_Tomatoes_Rating'),\n    vl.y().fieldQ('IMDB_Rating')\n  )\n  \n  const filter = vl.filter(vl.or(hover, click)),\n        subset_points = all_points.transform(filter);\n\n  // mark properties for new layers\n  const halo = {size: 100, stroke: 'firebrick', strokeWidth: 3},\n        label = {dx: 4, dy: -8, align: 'right'},\n        white = {stroke: 'white', strokeWidth: 2};\n\n  // layer scatter plot points, halo annotations, and title labels\n  return vl.data(movies)\n    .layer(\n      all_points.select(hover, click),\n      subset_points.markPoint(halo),\n      subset_points.markText(label, white).encode(vl.text().fieldN('Title')),\n      subset_points.markText(label).encode(vl.text().fieldN('Title'))\n    )\n    .width(600)\n    .height(450)\n    .render();\n}\n\n\nrobservable(\"@krisrs1128/week-3-3\", include = 4, height = 480)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/week-3-3\",\"include\":4,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nThis kind of labeling is a good middle ground between overloading the user with all details (putting all labels in the plot) and hiding the information completely (no labels). This is one of the benefits of interactive visualization — we can adapt the complexity of a visualization based on user inputs.\nAside: Notice that we didn’t need to directly hover over a point in order for the label to be revealed. Behind the scenes, a the nearest-neighbor (Voronoi) tessellation has been computed, and the labels change as soon as you transition from one cell to another. This article provides a detailed description, if you are curious (we won’t refer to this again, though).\n\nAnother example of details-on-demand is overview + detail1 time series visualization. The fine-grained structure in the time series might not be visible in the overview, but the user can zoom into features of interest using the selector. We can understand detail without losing our orientation in the overall time series.\n{\n  const selector = vl.selectInterval().encodings('x');\n  const x = vl.x().fieldT('date').title(null);\n  \n  // reusable time series encoding\n  const base = vl.markArea()\n    .encode(x, vl.y().fieldQ('price'))\n    .width(700);\n  \n  const overview = base.select(selector).height(60);\n  const detail = base.encode(\n    x.scale({domain: selector})\n  );\n\n  // return layered view\n  return vl.vconcat(detail, overview)\n    .data(sp500)\n    .render();\n}\n\n\nrobservable(\"@krisrs1128/week-3-3\", include = 5, height = 475)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/week-3-3\",\"include\":5,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nThe visualization is implemented by re-using the same base visual encoding (markArea time series) and (a) binding a selector to a smaller (60 pixel high) full time series and (b) adapting the scale of the larger time series, so that it only displays years within the current selection interval.\nFinally, we note that dynamic queries can be chained together in a sequence. They allow the user to navigate between static visualizations that display information across subsets and at different resolution.\nThese visualizations are more complex to implement, but it’s worth sharing an example. One of the earliest examples of details on demand was the film finder app.\n\nIt let’s you transition between an overview of all movies and details about selected ones. The key is that the transition is effortless — we can focus in on a few movies without losing our bearings in the broader context.\nYou can watch a whole (delightfully retro) video about this visualization here.\n\nVisualization people really like abstract compound words↩︎\n",
    "preview": {},
    "last_modified": "2021-01-31T18:10:45-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-25-week3-2/",
    "title": "Dynamic Queries (Part 1)",
    "description": "Using visualization to support query building.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-10",
    "categories": [],
    "contents": "\nReading, Recording, Notebook, Rmarkdown\nBefore, we only selected based on observations. How can we select based on attributes? One idea is to introduce widgets outside of the visualization through which the user can define queries. Selections are then bound to widgets, letting us use conditional encodings like before, but with respect to this new class of inputs.\n{\n  const selection = vl.selectSingle('Select') // name the selection 'Select'\n    .fields('Major_Genre')          // limit selection to the Major_Genre field\n    .init({Major_Genre: genres[0]}) // use first genre entry as initial value\n    .bind(vl.menu(genres));         // bind to a menu of unique genre values\n  \n  // scatter plot, modify opacity based on genre selection\n  return vl.markCircle()\n    .data(movies)\n    .select(selection)\n    .encode(\n      vl.x().fieldQ('Rotten_Tomatoes_Rating'),\n      vl.y().fieldQ('IMDB_Rating'),\n      vl.tooltip().fieldN('Title'),\n      vl.opacity().if(selection, vl.value(0.75)).value(0.05)\n    )\n    .render();\n}\n\n\nrobservable(\"@krisrs1128/week-3-2\", include = 6, height = 325)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/week-3-2\",\"include\":6,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nBesides menu bindings, there are checkbox, radio, and slider inputs. If you revisit the gapminder visualization in Lecture 1-3, you will see that the interaction was accomplished by binding a selection to a slider input.\nA selection can combine multiple bindings. For example, in the block below, we filter by both MPAA rating and genre. The only part of the code that is different is the definition of the selection object in the first few lines.\n {  \n  // single-value selection over [Major_Genre, MPAA_Rating] pairs\n  // use specific hard-wired values as the initial selected values\n  const selection = vl.selectSingle('Select')\n    .fields('Major_Genre', 'MPAA_Rating')\n    .init({Major_Genre: 'Drama', MPAA_Rating: 'R'})\n    .bind({Major_Genre: vl.menu(genres), MPAA_Rating: vl.radio(mpaa)});\n  \n  // scatterplot, modify opacity based on selection\n  return vl.markCircle()\n    .data(movies)\n    .select(selection)\n    .encode(\n      vl.x().fieldQ('Rotten_Tomatoes_Rating'),\n      vl.y().fieldQ('IMDB_Rating'),\n      vl.tooltip().fieldN('Title'),\n      vl.opacity().if(selection, vl.value(0.75)).value(0.05)\n    )\n    .render();\n}\n\n\nrobservable(\"@krisrs1128/week-3-2\", include = 7, height = 335)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/week-3-2\",\"include\":7,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nInterpretation 1: Dynamic queries create the visual analog of a database interaction. Rather than using a programming-based interface to filter elements or select attributes, we can design interactive visual equivalents.\nInterpretation 2: Dynamic queries allow rapid evaluation of conditional probabilities. The visualization above was designed to answer: What is the joint distribution of movie ratings, conditional on being a drama?\nThese visualizations are an instance of the more general idea of using filtering to reduce complexity in data. Filtering is an especially powerful technique in the interactive paradigm, where it is possible to easily reverse (or compare) filtering choices.\nScented Widgets\nQueries can be improved by using scented widgets. These are widgets that are themselves mini-visualizations, designed to provide guidance (« scent ») about potentially interesting queries.\nThis is an example of selecting films between two years, using a traditional text-based input. While easy to implement, the user is left to guess about when would be interesting combinations of years, and transitions between queries are clumsy — you have to reclick a new value each time.\nA scented widget approach both reveals when most movies were made and makes it easy to transition between queries. (Notice the future movies! Real data always have quality issues…)\n{\n  const brush = vl.selectInterval()\n    .encodings('x'); // limit selection to x-axis (year) values\n  \n  // dynamic query histogram\n  const years = vl.markBar()\n    .data(movies)\n    .select(brush)\n    .encode(\n      vl.x().year('Release_Date').title('Films by Release Year'),\n      vl.y().count().title(null)\n    )\n    .width(500)\n    .height(40);\n  \n  // ratings scatter plot\n  const ratings = vl.markCircle()\n    .data(movies)\n    .encode(\n      vl.x().fieldQ('Rotten_Tomatoes_Rating'),\n      vl.y().fieldQ('IMDB_Rating'),\n      vl.tooltip().fieldN('Title'),\n      vl.opacity().if(brush, vl.value(0.75)).value(0.05)\n    )\n    .width(500)\n    .height(350);\n\n  return vl.vconcat(years, ratings).render();\n}\n\n\nrobservable(\"@krisrs1128/week-3-2\", include = 8, height = 390)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/week-3-2\",\"include\":8,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-31T18:10:44-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-25-week3-1/",
    "title": "Selections",
    "description": "A look at a fundamental building block fo interactive visualization.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-09",
    "categories": [],
    "contents": "\nReading, Recording, Notebook, Rmarkdown\nA major difference between doing visualization on paper and on computers is that visualization on computers can make use of interactivity. An interactive visualization is one that changes in response to user cues. This allows a display to update in a way that provides a visual comparison that was not available in a previous view. In this way, interactive visualization allows users to answer a sequence of questions.\nSelection, both of observations and of attributes, is fundamental to interactive visualization. This is because it precedes other interactive operations: you can select a subset of observations to filter down to or attributes to coordinate across multiple displays (we consider both types of interactivity in later lectures).\nLet’s see how selection is implemented in vega-lite. We begin with an example, and then deconstruct it. The raw data are available here.\n\n\nrobservable(\"@krisrs1128/week-3-1\", include = 3, height = 180)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/week-3-1\",\"include\":3,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\n{\n  const selection = vl.selectSingle();\n  \n  return vl.markCircle()\n  .data(cars)\n  .select(selection)\n  .encode(\n    vl.x().fieldQ('Horsepower'),\n    vl.y().fieldQ('Miles\\_per\\_Gallon'),\n    vl.color().if(selection, vl.fieldO('Cylinders')).value('grey')\n  )\n  .height(180)\n  .render();\n}\nIn the first line, we defined a selection object. There are special selection objects, differentiated by the type of interaction they support.\nvl.selectSingle(): Allows selection of one observation at a time.\nvl.selectMulti(): Allows selection of several observations.\nvl.selectInterval(): Allows selection of contiguous intervals of observations.\nTo associate the selection with the points, we bound them to the vl.markCircle() block using the call .select(selection). This is how vega-lite knows that the user interaction should be associated with individual points.\nWe then apply the idea of conditional encoding to allow the visualization to respond to user selections. Conditional encoding changes the visual encodings of marks depending on whether they are contained within a user selected set. For example, in the plot above, if the observation is within the selected set, it is colored in by the number of cylinders in the car, otherwise it is grey. This is implemented by the .if(selection, vl.fieldO('Cylinders')).value('grey') modification of the original vl.color() encoding.\nTo understand the behavior of the three interaction techniques above, let’s regenerate this plot with the other two types of selections.\nfunction plot(selection) {\n  return vl.markCircle()\n.data(cars)\n.select(selection)\n.encode(\n  vl.x().fieldQ('Horsepower'),\n  vl.y().fieldQ('Miles\\_per\\_Gallon'),\n  vl.color().if(selection, vl.fieldO('Cylinders')).value('grey')\n)\n.width(240)\n.height(180);\n}\nWe can place the plots side-by-side using the horizontal concatenation we learned in the composition reading.\nvl.hconcat(\n  plot(vl.selectSingle()).title('Single (Click)'), \n  plot(vl.selectMulti()).title('Multi (Shift-Click)'), \n  plot(vl.selectInterval()).title('Interval (Drag)'),\n  plot(vl.selectMulti().on(\"mouseover\").nearest(true)).title('Mouseover Painting')\n).render()\n\n\nrobservable(\"@krisrs1128/week-3-1\", include = 5, height = 220)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/week-3-1\",\"include\":5,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nAre there ways that we can select multiple observations based on attributes other than their position in a plot (that is, without having to click or drag on them within the plot)? The answer is yes, using dynamic queries, which are discussed in the next recording.\nSelection — and interaction more generally — are reversing our usual way of thinking about encoding in visualization. Usually, we are mapping abstract data fields to specific visual marks and their properties. In interaction, we have to start with visual marks and refer to the underlying observations. This then allows changes in the visual marks, and we can iterate.\nA corollary of this point is that selections require two specifications: the mechanics of interaction and the way marks and their encodings change in response to an input. It is possible to experiment with variations in one while keeping the other constant.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-12T08:43:31-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-week2-4/",
    "title": "Ridge Plots",
    "description": "An extended example of faceting with data summaries.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-04",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nRidge plots are a special case of small multiples that are particularly suited to displaying multiple parallel time series or densities.\n\n\nlibrary(\"dplyr\")\nlibrary(\"dslabs\")\nlibrary(\"ggplot2\")\nlibrary(\"ggridges\")\ntheme_set(theme_bw())\n\n\n\nAn example ridge plot for the gapminder dataset is shown below. The effectiveness of this plot comes from the fact that multiple densities can be displayed in close proximity to one another, making it possible to facet across many variables in a space-efficient way.\n\n\ndata(gapminder)\ngapminder <- gapminder %>%\n  mutate(\n    dollars_per_day = (gdp / population) / 365,\n    group = case_when(\n      region %in% c(\"Western Europe\", \"Northern Europe\",\"Southern Europe\",  \"Northern America\",  \"Australia and New Zealand\") ~ \"West\",\n      region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n      region %in% c(\"Caribbean\", \"Central America\",  \"South America\") ~ \"Latin America\",\n      continent == \"Africa\" &  region != \"Northern Africa\" ~ \"Sub-Saharan\", \n      TRUE ~ \"Others\"\n    ),\n    group = factor(group, levels = c(\"Others\", \"Latin America\",  \"East Asia\", \"Sub-Saharan\", \"West\")),\n    west = ifelse(group == \"West\", \"West\", \"Developing\")\n  )\n\n\n\n\n\npast_year <- 1970\ngapminder_subset <- gapminder %>%\n  filter(year == past_year, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +  \n  geom_density_ridges() +\n  scale_x_log10()\n\n\n\n\nYou might wonder, why not plot the raw data? We do this using a geom_point below. The result isn’t as satisfying, because, while the shape of the ridges popped out immediately in the original plot, we have to invest a bit more effort to understand the regions of higher density in the raw data plot. Also, when there are many samples, the entire range might appeared covered in marks when in fact there are some intervals with higher density than others.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_point(shape = \"|\", size = 5) +\n  scale_x_log10()\n\n\n\n\nA nice compromise is to include both the raw positions and the smoothed densities.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_density_ridges(\n    jittered_points = TRUE, \n    position = position_points_jitter(height = 0),\n    point_shape = '|', point_size = 3\n  ) +\n  scale_x_log10()\n\n\n\n\nTo exercise our knowledge of both faceting and ridge plots, let’s study a particular question in depth: How did the gap between rich and poor countries change between 1970 and 2010?\nA first reasonable plot is to facet year and development status. While the rich countries have become slightly richer, there is a larger shift in the incomes for the poorer countries.\n\n\npast_year <- 1970\npresent_year <- 2010\nyears <- c(past_year, present_year)\n\ngapminder_subset <- gapminder %>%\n  filter(year %in% years, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day)) +\n  geom_histogram(binwidth = 0.25) +\n  scale_x_log10() +\n  facet_grid(year ~ west)\n\n\n\n\nWe can express this message more compactly by overlaying densities, but the attempt below is misleading, because it gives the same total area to both groups of countries. This doesn’t make sense, considering there are more than 4 times as many developing vs. western countries (87 vs. 21).\n\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\n\nWe can adjust the areas of the curves by directly referencing the ..count variable, which is implicitly computed by ggplot2 while it’s computing these densities.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, y = ..count.., fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\n\nCan we attribute the changes to specific regions? Let’s apply our knowledge of ridge plots.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, y = group, fill = as.factor(year))) +\n  geom_density_ridges(alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_x_log10()\n\n\n\n\nAlternatively, we can overlay densities from the different regions. Both plots suggest that much of the increase in incomes can be attributed to countries in Latin America and East Asia.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = group, y = ..count..)) +\n  geom_density(alpha = 0.7, bw = .12, position = \"stack\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_x_log10(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0, 0.15, 0)) +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-01-20-week2-4/week2-4_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-02-01T13:57:12-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-week2-3/",
    "title": "Concatenation and Repetition",
    "description": "Adapting the small multiples principle to fields that are not exactly\nparallel.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-03",
    "categories": [],
    "contents": "\nReading, Recording, Notebook, Rmarkdown\nFaceting is useful whenever you want different rows of your dataset to appear in different panels. What if you want to compare different columns, or work with several datasets? A more general alternative is to use concatenation or repetition.\nWe’re going to illustrate this using vega-lite, but the principles also apply to ggplot21. Suppose we want to plot several weather variables next to one another – we can use hconcat. The idea here is to construct each plot separately and then combine them only at the very end.\n{\n\n  const width = 180,\n        height = 130;\n  \n  // maximum temperature plot\n  const temp_max = vl.markLine()\n    .data(weather)\n    .encode(\n      vl.x().month(\"date\"),\n      vl.y().average(\"temp_max\"),\n      vl.color().fieldN(\"location\")\n    )    \n    .width(width)\n    .height(height);\n\n  // precipitation plot\n  const precip = vl.markLine()\n    .data(weather)\n    .encode(\n      vl.x().month(\"date\"),\n      vl.y().average(\"precipitation\"),\n      vl.color().fieldN(\"location\")\n    )    \n    .width(width)\n    .height(height);\n\n  // precipitation plot\n  const wind = vl.markLine()\n    .data(weather)\n    .encode(\n      vl.x().month(\"date\"),\n      vl.y().average(\"wind\"),\n      vl.color().fieldN(\"location\")\n    )\n    .width(width)\n    .height(height);\n  \n  return vl.hconcat(temp_max, precip, wind)\n    .data(weather)\n    .render()\n}\n\n\nrobservable(\"@krisrs1128/examples-of-repetition\", include = 4, height = 220)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-repetition\",\"include\":4,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nThis implementation is straightforward, but very clumsy. Whenever you find yourself copying and pasting code you should ask yourself whether there is a more elegant way to implement the same idea. In this case, there is, by reusing the same template for everything except the \\(y\\)-axis encoding.\n{\n  const width = 180,\n        height = 130;\n  \n  const base = vl.markLine()\n    .data(weather)\n    .encode(\n      vl.x().month(\"date\"),\n      vl.color().fieldN(\"location\")\n    )\n    .width(width)\n    .height(height);\n  \n  const temp_max = base.encode(vl.y().average(\"temp_max\")),\n        precip = base.encode(vl.y().average(\"precipitation\")),\n        wind = base.encode(vl.y().average(\"wind\"));\n  \n  return vl.hconcat(temp_max, precip, wind).render()\n}\n\n\nrobservable(\"@krisrs1128/examples-of-repetition\", include = 5, height = 220)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-repetition\",\"include\":5,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nThat’s better, but we can be even more concise, by using repetition. This lets us reuse the same template by referring to an abstract vl.repeat() object in the encoding.\n{\n  const width = 180,\n        height = 130;\n  \n  return vl.markLine()\n    .data(weather)\n    .encode(\n      vl.x().month(\"date\"),\n      vl.color().fieldN(\"location\"),\n      vl.y().average(vl.repeat(\"column\"))\n    )\n    .width(width)\n    .height(height)\n    .repeat({\"column\": [\"temp_max\", \"precipitation\", \"wind\"]})\n    .render()\n}\n\n\nrobservable(\"@krisrs1128/examples-of-repetition\", include = 6, height = 220)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-repetition\",\"include\":6,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nLet’s use this idea to generate a scatterplot matrix, a type of plot that shows all pairs of scatterplots between columns in a dataset. This type of plot is often useful in revealing correlations between fields.\n{\n  const width = 140,\n        height = 140;\n  \n  return vl.markPoint({filled: true, size: 3, opacity: 0.8})\n    .data(weather)\n    .encode(\n      vl.x().fieldQ(vl.repeat(\"column\")),\n      vl.y().fieldQ(vl.repeat(\"row\")),\n      vl.color().fieldN(\"location\")\n    )\n    .width(width)\n    .height(height)\n    .repeat({\n      column: [\"temp_max\", \"precipitation\", \"wind\"],\n      row: [\"temp_max\", \"precipitation\", \"wind\"],\n    })    \n    .render()\n}\n\n\nrobservable(\"@krisrs1128/examples-of-repetition\", include = 7)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-repetition\",\"include\":7,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\n\nThe analogous function is called grid.arrange from the gridExtra package↩︎\n",
    "preview": {},
    "last_modified": "2021-02-01T13:57:05-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-week2-2/",
    "title": "Faceting (Part 2)",
    "description": "A look at faceting in vega-lite.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [],
    "contents": "\nReading, Recording, Notebook, Rmarkdown\nIn these notes, we’re going to see how to facet using vega-lite. We’re going to look at the weather data, which shows the precipitation and temperature for NYC and Seattle over the course of a year.\n\n\nrobservable(\"@krisrs1128/examples-of-faceting\", include = 4, height = 280)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-faceting\",\"include\":4,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nThe most direct way to facet is to use the row and column encoding types. For example, here we are faceting histograms of temperature across the type of weather.\n{\n  const seattle = weather.filter(d => d.location == \"Seattle\");\n  const colors = {\n    domain: ['drizzle', 'fog', 'rain', 'snow', 'sun'],\n    range: ['#aec7e8', '#c7c7c7', '#1f77b4', '#9467bd', '#e7ba52'] // weather themed colors\n  };\n  \n  return vl.markBar()\n    .data(seattle)\n    .encode(\n      vl.x().fieldQ('temp_max').title('Temperature (°C)'),\n      vl.y().count().title(\"Number of Days\"),\n      vl.color().fieldN(\"weather\").scale(colors),\n      vl.column().fieldN(\"weather\")\n    )\n    .width(175)\n    .height(120)\n    .render()\n}\n\n\nrobservable(\"@krisrs1128/examples-of-faceting\", include = 5, height = 200)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-faceting\",\"include\":5,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nIf we want, we can facet by both weather and city, using row and column to distinguish between variations along rows and columns.\n{\n  const colors = {\n    domain: ['drizzle', 'fog', 'rain', 'snow', 'sun'],\n    range: ['#aec7e8', '#c7c7c7', '#1f77b4', '#9467bd', '#e7ba52'] // weather themed colors\n  };\n  \n  return vl.markBar()\n    .data(weather)\n    .encode(\n      vl.x().fieldQ('temp_max').title('Temperature (°C)'),\n      vl.y().count().title(\"Number of Days\"),\n      vl.color().fieldN(\"weather\").scale(colors),\n      vl.column().fieldN(\"weather\"),\n      vl.row().fieldN(\"location\")\n    )\n    .width(175)\n    .height(120)\n    .render()\n}\n\n\nrobservable(\"@krisrs1128/examples-of-faceting\", include = 6)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-faceting\",\"include\":6,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nThere is an alternative approach which is more general. The approach above defined the facet within the encoding for the markBar. There are situations where we might want to facet a plot that is made from more than just one type of mark (e.g., a line overlaid on points). In this case, we will want to create a facet call that can be applied to a full plot specification. This is exactly analogous to facet_wrap and facet_grid, where the faceting operator can be applied to several geom layers simultaneously. Notice that we need to put the data call after the facet – the faceting won’t know to what it should apply otherwise (try moving the command up).\n{\n  const colors = {\n    domain: ['drizzle', 'fog', 'rain', 'snow', 'sun'],\n    range: ['#aec7e8', '#c7c7c7', '#1f77b4', '#9467bd', '#e7ba52']\n  };\n  \n  return vl.markBar()\n    .encode(\n      vl.x().fieldQ('temp_max').title('Temperature (°C)'),\n      vl.y().count().title(\"Number of Days\"),\n      vl.color().fieldN(\"weather\").scale(colors)\n    )\n    .width(175)\n    .height(120)\n    .facet({column: vl.field(\"weather\"), row: vl.field(\"location\")})\n    .data(weather)\n    .render()\n}\n\n\nrobservable(\"@krisrs1128/examples-of-faceting\", include = 7)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-faceting\",\"include\":7,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nHere’s an example that shows why this is useful: How do the minimum and maximum temperatures vary between NYC and Seattle? In addition to showing the minimum and maximum temperatures (a ribbon), we may want to show the midpoint between these as its own line. For just NYC, we can make this plot using vl.layer to superimpose the two marks,\n{\n  const nyc_ = weather.derive({temp_mid: d => 0.5 * (d.temp_min + d.temp_max)})\n    .filter(d => d.location == \"New York\")\n  \n  // ribbon layer\n  const tempMinMax = vl.markArea({opacity: 0.3}).encode(\n    vl.x().month('date'), // averages months over several years\n    vl.y().average('temp_max').title('Temperature (°C)'),\n    vl.y2().average('temp_min')\n  );\n\n  // line layer\n  const tempMid = vl.markLine().encode(\n    vl.x().month('date'),\n    vl.y().average('temp_mid')\n  );\n\n  // overlay\n  return vl.layer(tempMinMax, tempMid)\n    .data(nyc_)\n    .render();\n}\n\n\nrobservable(\"@krisrs1128/examples-of-faceting\", include = 8, height = 220)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-faceting\",\"include\":8,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nNow, within each facet, we want to overlay two types of marks. We can no longer specify .column() or .row() within the .encode() calls for each mark. Instead, we have to specify a facet at the end, after we’ve already overlayed the two elements.\n{\n  const weather_ = weather.derive({temp_mid: d => 0.5 * (d.temp_min + d.temp_max)});\n\n  // same ribbon, colored by location\n  const tempMinMax = vl.markArea({opacity: 0.3}).encode(\n    vl.x().month('date'),\n    vl.y().average('temp_max').title('Temperature (°C)'),\n    vl.y2().average('temp_min'),\n    vl.color().fieldN(\"location\")\n  );\n\n  // same line, colored by location\n  const tempMid = vl.markLine().encode(\n    vl.x().month('date'),\n    vl.y().average('temp_mid'),\n    vl.color().fieldN(\"location\")\n  );\n\n  return vl.layer(tempMinMax, tempMid)\n    .width(300)\n    .height(200)\n    .facet({column: vl.field(\"location\")})\n    .data(weather_)\n    .render();\n}\n\n\nrobservable(\"@krisrs1128/examples-of-faceting\", include = 9, height = 230)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/examples-of-faceting\",\"include\":9,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-01T13:57:04-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-week2-1/",
    "title": "Faceting (Part 1)",
    "description": "Using small multiples to create information dense plots.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      },
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2021-02-01",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nIt might seem like we’re limited with the total number of variables we can display at a time. While there are many types of encodings we could in theory use, only a few them are very effective, and they can interfere with one another.\nNot all is lost, though! A very useful idea for visualizing high-dimensional data is the idea of small multiples. It turns out that our eyes are pretty good at making sense of many small plots, as long as there is some shared structure across the plots.\n\nLet’s see these ideas in action. These are libraries we need.\n\n\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"dslabs\")\nlibrary(\"readr\")\ntheme_set(theme_bw())\n\n\n\nIn ggplot2, we can implement this idea using the facet_wrap and facet_grid commands. We specify the column in the data.frame along which we want to generate comparable small multiples.\n\n\nyears <- c(1962, 1980, 1990, 2000, 2012)\ncontinents <- c(\"Europe\", \"Asia\")\ngapminder_subset <- gapminder %>%\n  filter(year %in% years, continent %in% continents)\n\nggplot(\n    gapminder_subset, \n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(. ~ year) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nIn facet grid, you specify whether you want the plot to be repeated across rows or columns, depending on whether you put the variable before or after the tilde.\n\n\nggplot(\n    gapminder_subset, \n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(year ~ .) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nYou can also facet by more than one variable at a time, specifying which variables should go in rows and which should go in columns again using the tilde.\n\n\nggplot(\n    gapminder %>% filter(year %in% years),\n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(year ~ continent) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nSometimes, you just want to see the display repeated over groups, but you don’t really need them to all appear in the same row or column. In this case, you can use facet_wrap.\n\n\nggplot(\n    gapminder %>% filter(year %in% years),\n    aes(x = fertility, y = life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_wrap(~ year)\n\n\n\n\nJust to illustrate, faceting makes sense for datasets other than scatterplots. This example also shows that faceting will apply to multiple geom layers at once.\nThe dataset shows the abundances of five different bacteria across three different subjects over time, as they were subjected to antibiotics. The data were the basis for this study.\n\n\nantibiotic <- read_csv(\"https://uwmadison.box.com/shared/static/5jmd9pku62291ek20lioevsw1c588ahx.csv\")\nhead(antibiotic)\n\n\n# A tibble: 6 x 7\n  species  sample value ind    time svalue antibiotic     \n  <chr>    <chr>  <dbl> <chr> <dbl>  <dbl> <chr>          \n1 Unc05qi6 D1         0 D         1   NA   Antibiotic-free\n2 Unc05qi6 D2         0 D         2   NA   Antibiotic-free\n3 Unc05qi6 D3         0 D         3    0   Antibiotic-free\n4 Unc05qi6 D4         0 D         4    0   Antibiotic-free\n5 Unc05qi6 D5         0 D         5    0   Antibiotic-free\n6 Unc05qi6 D6         0 D         6    0.2 Antibiotic-free\n\nI have also separately computed running averages for each of the variables – this is in the svalue column. We’ll discuss ways to do this during the week on time series visualization.\n\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\n\nIt seems like some of the species are much more abundant than others. In this situation, it might make sense to rescale the \\(y\\)-axis. Though, this is always a risky decision – people might easily misinterpret the plot and conclude that the different species all have the same abundances. Nonetheless, it can’t hurt to try, using the scale argument to facet_grid.\n\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind, scale = \"free_y\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\n\nUnlike the years example, the facets don’t automatically come with their own natural order. We can define an order based on the average value of the responses over the course of the survey, and then change the factor levels of the Species column to reorder the panels.\n\n\nspecies_order <- antibiotic %>%\n  group_by(species) %>%\n  summarise(avg_value = mean(value)) %>%\n  arrange(desc(avg_value)) %>%\n  pull(species)\n\nantibiotic <- antibiotic %>%\n  mutate(species = factor(species, levels = species_order))\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind, scale = \"free_y\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-01-20-week2-1/week2-1_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-01T13:57:03-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-18-week1-5/",
    "title": "A Vocabulary of Marks",
    "description": "Examples of marks and their encodings in both ggplot2 and vega-lite.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-01-29",
    "categories": [],
    "contents": "\n\n\n\nReading, Recording, Rmarkdown, Observable\nThe choice of encodings can have a strong effect on (1) the types of comparisons that a visualization suggests and (2) the accuracy of the conclusions that readers leave with. With this in mind, it’s in our best interest to build a rich vocabulary of potential visual encodings. The more kinds of marks and encodings that are at your fingertips, the better your chances are that you’ll arrive at a configuration that helps you achieve your purpose.\nSo, let’s look at a few different types of marks and encodings in both ggplot2 and vega-lite. Before we get started, let’s load up the libraries that will be used in these notes.\n\n\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"robservable\")\ntheme_set(theme_bw())\n\n\n\nPoint Marks\nLet’s read in the gapminder dataset that we used in the introduction to vega-lite.\n\n\n# load data\ngapminder <- read_csv(\"https://uwmadison.box.com/shared/static/dyz0qohqvgake2ghm4ngupbltkzpqb7t.csv\", col_types = cols()) %>%\n  mutate(cluster = as.factor(cluster)) # specify that cluster is nominal\ngap2000 <- gapminder %>%\n  filter(year == 2000) # keep only year 2000\n\n\n\nPoint marks can encode data fields using their \\(x\\) and \\(y\\) positions, color, size, and shape. Below, each mark is a country, and we’re using shape and the \\(y\\) position to distinguish between country clusters.\n\n\nggplot(gap2000) +\n  geom_point(aes(x = fertility, y = cluster, shape = cluster))\n\n\n\n\nAside from some small differences in the syntax and styling, the result is almost identical in vega-lite.\nvl.markPoint()\n  .data(data2000)\n  .encode(\n    vl.x().fieldQ('fertility'),\n    vl.y().fieldN('cluster'),\n    vl.shape().fieldN('cluster')\n  )\n  .render()\n\n\nrobservable(\"@krisrs1128/vocabulary-of-marks\", include = 5, height = 170)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/vocabulary-of-marks\",\"include\":5,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nWe can specify different types of shapes using the “shape” parameter outside of the aes encoding.\n\n\nggplot(gap2000) +\n  geom_point(aes(x = fertility, y = cluster), shape = 15)\n\n\n\n\nFor the equivalent in vega-lite, use markSquare,\nvl.markSquare({size: 100})\n  .data(data2000)\n  .encode(\n    vl.x().fieldQ('fertility'),\n    vl.y().fieldN('cluster')\n  )\n  .render()\n\n\nrobservable(\"@krisrs1128/vocabulary-of-marks\", include = 6, height = 170)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/vocabulary-of-marks\",\"include\":6,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nBar Marks\nBar marks let us associate a continuous field with a nominal one.\n\n\nggplot(gap2000) +\n  geom_bar(aes(x = country, y = pop), stat = \"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90),\n    panel.grid.major.x = element_blank()\n  )\n\n\n\n\nvl.markBar()\n  .data(data2000)\n  .encode(\n    vl.x().fieldN('country'),\n    vl.y().fieldQ('pop')\n  )\n  .render()\n\n\nrobservable(\"@krisrs1128/vocabulary-of-marks\", include = 7)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/vocabulary-of-marks\",\"include\":7,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nTo make comparisons between countries with similar populations easier, we can order them by population (alphabetical ordering is not that meaningful). To compare clusters, we can color in the bars.\n\n\nggplot(gap2000) +\n  geom_bar(\n    aes(x = reorder(country, -pop, mean), y = pop, fill = cluster), \n    stat = \"identity\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(expand = c(0, 0, .1, .1)) +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.2, size = 8), # more readable axes\n    panel.grid.major.x = element_blank()\n  )\n\n\n\n\nHere is the same plot in vega-lite.\nvl.markBar()\n  .data(data2000)\n  .encode(\n    vl.x().fieldN('country').sort(\"-y\"),\n    vl.y().fieldQ('pop'),\n    vl.color().field(\"cluster\").scale({\"scheme\": \"set2\"})\n  )\n  .width(500)\n  .render()\n\n\nrobservable(\"@krisrs1128/vocabulary-of-marks\", include = 8)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/vocabulary-of-marks\",\"include\":8,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nIn the plot above, each bar is anchored at 0. Instead, we could have each bar encode two continuous values, a top and bottom. To illustrate, let’s compare the minimum and maximimum life expectancies within each country cluster.\nvega-lite lets you compute summary statistics within the encoding step (see below). For ggplot2, we’ll need to create a new data.frame with just the summary information. For this, we group_by each cluster, so that a summarise call finds the minimum and maximum life expectancies restricted to each cluster.\n\n\n# find summary statistics\nlife_ranges <- gap2000 %>%\n  group_by(cluster) %>%\n  summarise(\n    min_life = min(life_expect),\n    max_life = max(life_expect)\n  )\n\n# look at a few rows\nhead(life_ranges)\n\n\n# A tibble: 6 x 3\n  cluster min_life max_life\n  <fct>      <dbl>    <dbl>\n1 0           42.1     63.6\n2 1           70.5     80.6\n3 2           43.4     53.4\n4 3           58.1     79.8\n5 4           66.7     82  \n6 5           57.0     79.7\n\n# plot them\nggplot(life_ranges) +\n  geom_segment(\n    aes(x = min_life, xend = max_life, y = cluster, yend = cluster),\n    size = 4\n  ) +\n  xlim(0, 85)\n\n\n\n\nNotice here that we can just specify .min() and .max() within the encoding, rather than precomputing the summary statistics.\nvl.markBar()\n  .data(data2000)\n  .encode(\n    vl.x().min('life_expect'),\n    vl.x2().max('life_expect'),\n    vl.y().fieldN('cluster')\n  )\n  .render()\n\n\nrobservable(\"@krisrs1128/vocabulary-of-marks\", include = 9, height = 170)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/vocabulary-of-marks\",\"include\":9,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nLine Marks\nLine marks are useful for comparing changes. Our eyes naturally focus on rates of change when we see lines. Below, we’ll plot the fertility over time, colored in by country cluster.\n\n\nggplot(gapminder) +\n  geom_line(\n    aes(x = year, y = fertility, col = cluster, group = country),\n      alpha = 0.7, size = 0.9\n  ) +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nThis is the equivalent code for vega-lite. You can get a country name by hovering over the lines.\nvl.markLine({opacity: 0.7, size: 3})\n  .data(data)\n  .encode(\n    vl.x().fieldO('year'),\n    vl.y().fieldQ('fertility'),\n    vl.color().fieldN('cluster').scale({\"scheme\": \"set2\"}),\n    vl.detail().fieldN(\"country\"),\n    vl.tooltip().fieldN('country')\n  )\n  .width(500)\n  .render()\n\n\nrobservable(\"@krisrs1128/vocabulary-of-marks\", include = 10, height = 330)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/vocabulary-of-marks\",\"include\":10,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nArea Marks\nArea marks have a flavor of both bar and line marks. The filled area supports absolute comparisons, while the changes in shape suggest derivatives.\n\n\npopulation_sums <- gapminder %>%\n  group_by(year, cluster) %>%\n  summarise(total_pop = sum(pop))\n\nhead(population_sums)\n\n\n# A tibble: 6 x 3\n# Groups:   year [1]\n   year cluster total_pop\n  <dbl> <fct>       <dbl>\n1  1955 0       495927174\n2  1955 1       360609771\n3  1955 2        60559800\n4  1955 3       355392405\n5  1955 4       854125031\n6  1955 5        56064015\n\nggplot(population_sums) +\n  geom_area(\n    aes(x = year, y = total_pop, fill = cluster)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(expand = c(0, 0, .1, .1)) +\n  scale_x_continuous(expand = c(0, 0))\n\n\n\n\nIn the vega-lite version, we can add a tooltip that shows country names on mouseover. It’s not so clear where one country begins and another starts, though – it would be clearer if there were small lines demarcating the boundaries between countries.\nvl.markArea()\n  .data(data)\n  .encode(\n    vl.x().fieldO('year'),\n    vl.y().sum('pop'),\n    vl.color().fieldN('cluster').scale({\"scheme\": \"set2\"}),\n    vl.detail().fieldN(\"country\"),\n    vl.tooltip().fieldN('country')\n  )\n  .width(600)\n  .render()\n\n\nrobservable(\"@krisrs1128/vocabulary-of-marks\", include = 11)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/vocabulary-of-marks\",\"include\":11,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nJust like in bar marks, we don’t necessarily need to anchor the \\(y\\)-axis at 0. For example, here the bottom and top of each area mark is given by the 30% and 70% quantiles of population within each country cluster.\n\n\npopulation_ranges <- gapminder %>%\n  group_by(year, cluster) %>%\n  summarise(min_pop = quantile(pop, 0.3), max_pop = quantile(pop, 0.7))\n\nhead(population_ranges)\n\n\n# A tibble: 6 x 4\n# Groups:   year [1]\n   year cluster   min_pop   max_pop\n  <dbl> <fct>       <dbl>     <dbl>\n1  1955 0       40880121. 83941368.\n2  1955 1        4532940  25990229.\n3  1955 2        6600426. 17377594.\n4  1955 3        2221139   8671500 \n5  1955 4        9014491  61905422.\n6  1955 5        3007625  12316126.\n\nggplot(population_ranges) +\n  geom_ribbon(\n    aes(x = year, ymin = min_pop, ymax = max_pop, fill = cluster),\n    alpha = 0.8\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(expand = c(0, 0, .1, .1)) +\n  scale_x_continuous(expand = c(0, 0))\n\n\n\n\nFor these quantiles, we can’t use a built-in vega-lite summary functions, like the .min() and .max() from above. Instead, we need the javascript equivalent of dplyr’s group_by + summarise, which are called groupby and rollup.\npopulation_ranges = data.groupby(\"year\", \"cluster\")\n  .rollup({ // equivalent of dplyr summarise\n    pop_low: op.quantile(\"pop\", 0.3),\n    pop_high: op.quantile(\"pop\", 0.7)\n  })\nThese data can now be used to generate the equivalent vega-lite plot.\nvl.markArea({\"opacity\": 0.7})\n  .data(population_ranges)\n  .encode(\n    vl.x().fieldO('year'),\n    vl.y().fieldQ('pop_low').title(\"Population\"),\n    vl.y2().fieldQ('pop_high'),\n    vl.color().fieldN('cluster').scale({\"scheme\": \"set2\"})\n  )\n  .width(600)\n  .render()\n\n\nrobservable(\"@krisrs1128/vocabulary-of-marks\", include = 13)\n\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/vocabulary-of-marks\",\"include\":13,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\n\n\n\n",
    "preview": "posts/2021-01-18-week1-5/week1-5_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-01-25T10:41:44-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-18-week1-4/",
    "title": "Types, Marks, and Encodings",
    "description": "Tying together the introductions to ggplot2 and vega-lite, using the common\nlanguage of encodings.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nThe reason that the ggplot2 and vega-lite packages feel similar is that they are based on the same underlying ideas,\nEach observation (row) within a dataset corresponds to a geometric mark in a visualization.\nImportant attributes (columns) of an observation within a dataset can be encoded by properties of visual marks.\nThis encoding of rows as marks and columns as properties of the marks is illustrated in the toy diagram below.\n\nIn fact, we can roughly translate the function names between the two packages,\ngeom_* → mark*\naes → vl.encode()\nscale_* → scale()\nlabs → title()\nA good visualization makes it easy to visually compare the relevant attributes across observations. A challenge is that there are often many possible marks and encodings for any given comparison. It’s also difficult to know which comparisons are actually of interest. For these reasons, we’re going to want to build up a vocabulary of marks and encodings.\n\nField types\nTo identify good encodings, it can often help to first what the types of each field are.\nNominal: These are fields that allow identity comparisons between observations (is \\(A = B\\)?), but don’t come with any notion of order.\nCity region, country name, movie genres, file types, …\n\nOrdinal: These are fields that allow ordering comparisons between observations (is \\(A > B\\)?), but don’t let you quantify the degree of difference.\nShirt size, sports team rankings, order of appearance of characters on TV show\n\nQuantitative: These are field types that let you make distance comparisons between observations. For interval data, absolute comparisons can be made (\\(A - B\\)), while for ratio data, only relative ones (\\(\\frac{A}{B}\\)) are possible.\nTemperature, stock price, number of drinks sold in a coffee shop per day\n\nThis is not an exhaustive list, and there are subtleties,\nThe same field could be treated as nominal, ordinal, or quantitative depending on the context. Age could be nominal (underage or overage), ordinal (gen Z, millenial, …), or quantitative.\nA given field might be thought of hierarchically. There might be interesting patterns at the city, county, or region level – what seems like a single column can become the source for multiple derived fields.\nEven though temporal fields can be thought of as quantitative, they come with their own conventions. For example, the hour - week - month hierarchy is a natural one for temporal data.\nIt’s worth highlighting that, even if a particular encoding could be used for a given data type, different encodings have different effectiveness. When trying to encoding several data fields at once, a choice of one encoding over another will implicitly prioritize certain comparisons over others.\nFor example, in the figure below (Figure 5.8 here), the same two numbers are encoded using many different visual properties – position on a shared \\(y\\)-axis, position on distinct \\(y\\)-axes, etc. – and study participants were asked to gauge the difference in the numbers. People were best at comparing positions on a common scale, and worst at distinguishing differences in areas.\n\nHere is a similar example. In the left four panels, a nominal field is encoded using color (left two) and shape (middle two). The red circles are easiest to find in the left two. In the right two panels, two nominal fields are encoded, using both shape and color. It’s much harder to find the red circle – you have to scan over the entire image to find it, which you didn’t have to do at all for the first two panels.\n\nImplicit in this discussion is that there is never any “perfect” visualization for a given dataset – the quality of a visualization is a function of its intended purpose. What comparisons do you want to facilitate? The choice of encoding will strongly affect the types of comparisons that are easy to perform.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-25T10:41:43-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-16-week1-3/",
    "title": "Introduction to Vega-Lite",
    "description": "Learn the basic concepts for creating vega-lite plots, and see how the library\nsupports interactivity.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-01-27",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown, Observable\nVega-Lite is the javascript equivalent of ggplot2. It provides a grammar of graphics, where layers can be composed on top of one another. The main reason for knowing vega-lite (opposed to only ggplot2) is that it can support flexible interaction. Also, it’s a good gateway toward understanding richer data visualization packages, like d3.\n\n\n\nComponents of a Graph\nWe’re going to construct this graph step-by-step, just like in the ggplot2 lecture. I realize that this is a bit repetitive, but I want to make it obvious just how similar ggplot2 and vega-lite are.\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/introduction-to-vega-lite\",\"include\":4,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nData and Geometry\nThe first thing to do is import the necessary libraries. The arquero library (aq) makes it easier to manage data in javascript – the filtering and printing steps would be more difficult without it.\nimport {vl} from '@vega/vega-lite-api'\nimport { aq, op } from '@uwdata/arquero'\nNow we can preview and filter the raw data.\ndata = aq.fromCSV(await FileAttachment(\"gapminder.csv\").text()) // I used the file upload button to add this file\ndata2000 = data.filter(d => d.year == 2000) // filter to only 2000\ndata2000.view()\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/introduction-to-vega-lite\",\"include\":6,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nLet’s create a visual mark, without any encodings. Like in our first ggplot2 figure, the mark is collapsed into a single point at the origin. Notice the analogy betwen vega-lite’s markPoint() and ggplot2’s geom_point().\nvl.markPoint()\n  .data(data2000)\n  .render()\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/introduction-to-vega-lite\",\"include\":7,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nEncodings\nWe can deconstruct the original figure to see data fields are mapped to visual properties,\nFertility → \\(x\\)-axis coordinate\nLife expectancy → \\(y\\)-axis coordinate\nCountry cluster → Point color\nPopulation → Point size\nWe can build this up one step at a time. To encode the fertility and life expectancy fields, we use vl.x() and vl.y() in an encode() call. One subtlety is that it’s necessary to specify the data type of each field. fieldQ means that the field is “Quantitative”. We’ll review data types in the next lecture.\nvl.markPoint()\n  .data(data2000)\n  .encode(\n    vl.x().fieldQ(\"fertility\"), // what happens if you change this to fieldN()?\n    vl.y().fieldQ(\"life_expect\")\n  )\n  .render()\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/introduction-to-vega-lite\",\"include\":8,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nThe same idea can be used to encode the country cluster and population fields. fieldN specifies that the cluster field is a Nominal (i.e., categorical) variable.\nHint: How do you know which commands encode which fields? It’s often good to keep the vega-lite-api and vega-lite documentation close at hand.\nvl.markPoint()\n  .data(data2000)\n  .encode(\n    vl.x().fieldQ(\"fertility\"),\n    vl.y().fieldQ(\"life_expect\"),\n    vl.color().fieldN(\"cluster\"), // what happens if you change this to fieldO()?\n    vl.size().fieldQ(\"pop\")\n  )\n  .render()\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/introduction-to-vega-lite\",\"include\":9,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nFinishing touches\nWe’ve made headway in our visualization task, but a more critical eye will help us improve the display. It will also give us an initial look at interactivity. The main things we ought to improve in this display are,\nThe open rings are a bit distracting. They violate the principle of Smallest Effective Difference.\nSome of the points are impossibly small.\nThe axis labels are not proper English words.\nThere are distractingly many values in the legend for population size.\nWe can’t actually tell which country each point corresponds to. It would be nice to add a tooltip (a label that appears when you mouseover the point).\nFor (1), let’s replace the open rings with filled circles. We can pass options to markPoint in the same way that we passed options to ggplot geom layers. For (2), we can modify the default size range by adding on a .scale() to the vl.size() encoding – this is similar to adding a custom scale in ggplot2.\nvl.markPoint({filled: true})\n.data(data2000)\n.encode(\n  vl.x().fieldQ(\"fertility\"),\n  vl.y().fieldQ(\"life_expect\"),\n  vl.color().fieldN(\"cluster\"),\n  vl.size().fieldQ(\"pop\").scale({range: [25, 1000]}) // try changing the ranges\n)\n.render()\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/introduction-to-vega-lite\",\"include\":10,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nFor (3), we can add a .title() to fields for which we want to customize titles, and for (4), we can modify the tick count in the .legend() for size.\nvl.markPoint({filled: true})\n.data(data2000)\n.encode(\n  vl.x().fieldQ(\"fertility\").title(\"Fertility\"),\n  vl.y().fieldQ(\"life_expect\").title(\"Life Expectancy\"),\n  vl.color().fieldN(\"cluster\"),\n  vl.size().fieldQ(\"pop\").scale({range: [25, 1000]}).legend({tickCount: 3}).title(\"Population\"),\n)\n.render()\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/introduction-to-vega-lite\",\"include\":11,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nNow to (5). The idea is to think of a tooltip as another type of encoding – just like coordinate positions and colors, a label is a way of transforming a field in an abstract dataset into a visual property we can observe on a screen.\nvl.markPoint({filled: true})\n.data(data2000)\n.encode(\n  vl.x().fieldQ(\"fertility\").title(\"Fertility\"),\n  vl.y().fieldQ(\"life_expect\").title(\"Life Expectancy\"),\n  vl.color().fieldN(\"cluster\"),\n  vl.size().fieldQ(\"pop\").scale({range: [25, 1000]}).legend({tickCount: 3}).title(\"Population\"),\n  vl.tooltip().fieldN(\"country\"),\n  vl.order().fieldQ('pop').sort('descending') // if you remove this, large points prevent mouseover on the points below them\n)\n.render()\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/introduction-to-vega-lite\",\"include\":12,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nOne point I want you to take away from both this and the ggplot2 introductions is that building a plot is an iterative process. Once you develop your eye for good visual design, you will be able to take any initial plot and make it more effective.\nFinally, for a taste of what’s to come, here’s a version of the same plot that let’s us compare many years over time.\n\n\n{\"x\":{\"notebook\":\"@krisrs1128/introduction-to-vega-lite\",\"include\":13,\"hide\":null,\"input\":null,\"observers\":null,\"update_height\":true,\"update_width\":true},\"evals\":[],\"jsHooks\":[]}\nDon’t worry about the code implementation, but in case you’re curious, it’s only a few extra lines over what we’ve already implemented.\n{\n  // define a slider selector object\n  let selectYear = vl.selectSingle(\"select\").fields(\"year\")\n    .init({year: 1955})\n    .bind(vl.slider().min(1955).max(2005).step(5).name(\"Year\"))\n  \n  // update the plot depending on the value of the slider\n  return vl.markPoint({filled: true})\n    .data(data)\n    .select(selectYear) // refers to the slider bar\n    .transform(vl.filter(selectYear))\n    .encode(\n      vl.x().fieldQ(\"fertility\").title(\"Fertility\").scale({domain: [0, 9]}), // hard code the axes ranges\n      vl.y().fieldQ(\"life_expect\").title(\"Life Expectancy\").scale({domain: [0, 90]}),\n      vl.color().fieldN(\"cluster\"),\n      vl.size().fieldQ(\"pop\").scale({domain: [0, 1200000000], range: [25, 1000]}).legend({tickCount: 3}).title(\"Population\"),\n      vl.tooltip().fieldN(\"country\"),\n      vl.order().fieldQ('pop').sort('descending')\n    )\n    .render()\n}\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-25T10:44:10-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-12-week1-2/",
    "title": "Introduction to ggplot2",
    "description": "A discussion of ggplot2 terminology, and an example of iteratively refining a\nsimple scatterplot.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-01-26",
    "categories": [],
    "contents": "\n\n\n\nReading, Recording, Rmarkdown\nggplot2 is an R implementation of the Grammar of Graphics. The idea is to define the basic “words” from which visualizations are built, and then let users compose them in original ways. This is in contrast to systems with prespecified chart types, where the user is forced to pick from a limited dropdown menu of plots. Just like in ordinary language, the creative combination of simple building blocks can support a very wide range of expression.\nThese are libraries we’ll use in this lecture.\n\n\nlibrary(\"dplyr\")\nlibrary(\"dslabs\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"scales\")\n\n\n\nComponents of a Graph\nWe’re going to create this plot in these notes.\n\n\n\nEvery ggplot2 plot is made from three components,\nData: This is the data.frame that we want to visualize.\nGeometry: These are the types of visual marks that appear on the plot.\nAesthetic Mapping: This links the data with the visual marks.\nThe Data\nLet’s load up the data. Each row is an observation, and each column is an attribute that describes the observation. This is important because each mark that you see on a ggplot – a line, a point, a tile, … – had to start out as a row within an R data.frame. The visual properties of the mark (e.g., color) are determined by the values along columns. These type of data are often referred to as tidy data, and we’ll have a full week discussing this topic.\nHere’s an example of the data above in tidy format,\n\n\ndata(murders)\nhead(murders)\n\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\nThis is one example of how the same information might be stored in a non-tidy way, making visualization much harder.\n\n\nnon_tidy <- data.frame(t(murders))\ncolnames(non_tidy) <- non_tidy[1, ]\nnon_tidy <- non_tidy[-1, ]\nnon_tidy[, 1:6]\n\n\n            Alabama   Alaska  Arizona Arkansas California Colorado\nabb              AL       AK       AZ       AR         CA       CO\nregion        South     West     West    South       West     West\npopulation  4779736   710231  6392017  2915918   37253956  5029196\ntotal           135       19      232       93       1257       65\n\nOften, one of the hardest parts in making a ggplot2 plot is not coming up with the right ggplot2 commands, but reshaping the data so that it’s in a tidy format.\nGeometry\nThe words in the grammar of graphics are the geometry layers. We can associate each row of a data frame with points, lines, tiles, etc., just by referring to the appropriate geom in ggplot2. A typical plot will compose a chain of layers on top of a dataset,\n\nggplot(data) + [layer 1] + [layer 2] + …\n\nFor example, by deconstructing the plot above, we would expect to have point and text layers. For now, let’s just tell the plot to put all the geom’s at the origin.\n\n\nggplot(murders) +\n  geom_point(x = 0, y = 0) +\n  geom_text(x = 0, y = 0, label = \"test\")\n\n\n\n\nYou can see all the types of geoms in the cheat sheet. We’ll be experimenting with a few of these in a later lecture.\nAesthetic mappings\nAesthetic mappings make the connection between the data and the geometry. It’s the piece that translates abstract data fields into visual properties. Analyzing the original graph, we recognize these specific mappings,\nState Population → \\(x\\)-axis coordinate\nNumber of murders → \\(y\\)-axis coordinate\nGeographical region → color\nTo establish these mappings, we need to use the aes function. Notice that column names don’t have to be quoted – ggplot2 knows to refer back to the murders data frame in ggplot(murders).\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region))\n\n\n\n\nThe original plot used a log-scale. To transform the x and y axes, we can use scales.\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\nOnce nuance is that scales aren’t limited to \\(x\\) and \\(y\\) transformations. They can be applied to modify any relationship between a data field and its appearance on the page. For example, this changes the mapping between the region field and circle color.\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region)) +\n  scale_x_log10() +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#6a4078\", \"#aa1518\", \"#9ecaf8\", \"#50838c\")) # exercise: find better colors using https://imagecolorpicker.com/\n\n\n\n\nA problem with this graph is that it doesn’t tell us which state each point corresponds to. For that, we’ll need text labels. We can encode the coordinates for these marks again using aes, but this time within a geom_text layer.\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region)) +\n  geom_text(\n    aes(x = population, y = total, label = abb),\n    nudge_x = 0.08 # what would happen if I remove this?\n  ) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\nNote that each type of layer uses different visual properties to encode the data – the argument label is only available for the geom_text layer. You can see which aesthetic mappings are required for each type of geom by checking that geom’s documentation page, under the Aesthetics heading.\nIt’s usually a good thing to make your code as concise as possible. For ggplot2, we can achieve this by sharing elements across aes calls (e.g., not having to type population and total twice). This can be done by defining a “global” aesthetic, putting it inside the initial ggplot call.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_point(aes(col = region)) +\n  geom_text(aes(label = abb), nudge_x = 0.08) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\nFinishing touches\nHow can we improve the readability of this plot? You might already have ideas,\nPrevent labels from overlapping. It’s impossible to read some of the state names.\nAdd a line showing the national rate. This serves as a point of reference, allowing us to see whether an individual state is above or below the national murder rate.\nGive meaningful axis / legend labels and a title.\nMove the legend to the top of the figure. Right now, we’re wasting a lot of visual real estate in the right hand side, just to let people know what each color means.\nUse a better color theme.\nFor 1., the ggrepel package find better state name positions, drawing links when necessary.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) + # I moved it up so that the geom_point's appear on top of the lines\n  geom_point(aes(col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\nFor 2., let’s first compute the national murder rate,\n\n\nr <- murders %>% \n  summarize(rate = sum(total) /  sum(population)) %>%\n  pull(rate)\nr\n\n\n[1] 3.034555e-05\n\nNow, we can use this as the slope in a geom_abline layer, which encodes a slope and intercept as a line on a graph.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), size = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\nFor 3., we can add a labs layer to write labels and a theme to reposition the legend. I used unit_format from the scales package to change the scientific notation in the \\(x\\)-axis labels to something more readable.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), size = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10(labels = unit_format(unit = \"million\", scale = 1e-6)) + # used to convert scientific notation to readable labels\n  scale_y_log10() +\n  labs(\n    x = \"Population (log scale)\",\n    y = \"Total number of murders (log scale)\",\n    color = \"region\",\n    title = \"US Gun Murders in 2010\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\nFor 5., I find the gray background with reference lines a bit distracting. We can simplify the appearance using theme_bw. I also like the colorbrewer palette, which can be used by calling a different color scale.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), size = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10(labels = unit_format(unit = \"million\", scale = 1e-6)) +\n  scale_y_log10() +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"Population (log scale)\",\n    y = \"Total number of murders (log scale)\",\n    color = \"Region\",\n    title = \"US Gun Murders in 2010\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nSome bonus exercises, which will train you to look at your graphics more carefully, as well as build your familiarity with ggplot2.\nTry reducing the size of the text labels. Hint: use the size argument in geom_text_repel.\nIncrease the size of the circles in the legend. Hint: Use override.aes within a guide.\nRe-order the order of regions in the legend. Hint: Reset the factor levels in the region field of the murders data.frame.\nOnly show labels for a subset of states that are far from the national rate. Hint: Filter the murders data.frame, and use a data field specific to the geom_text_repel layer.\n\n\n\n",
    "preview": "posts/2021-01-12-week1-2/week1-2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-01-25T16:38:50-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-12-week1-1/",
    "title": "Orientation",
    "description": "How this course is structured, and how to follow along.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-01-25",
    "categories": [],
    "contents": "\nThis course explores the role of visualization within data science workflows. We’ll get hands-on experience how visualization can be used to support,\nData cleaning and tidying\nExploratory analysis\nModel inspection\nScientific reporting\nThe course mostly uses R, though occasionally we will use the vega-lite library in javascript. Prior experience with R, but not javascript, is expected.\nIn terms of content, we’ll begin with a three week crash course on the fundamental principles of data visualization. This will provide a vocabulary for designing and critiquing visualizations throughout the semester. Weeks 4 - 5 are focused on the connections between visualization and tidy data. Rearranging your data in the right way can make it much easier to visualize; conversely, a good interface can immediately highlight data quality issues.\nFrom here on out, we’ll focus on exploratory analysis and model visualization. Weeks 6 - 8 consider data with temporal, geographic, or network structure – each comes with its own visual conventions. Weeks 9 - 11 focus on visualization of high-dimensional data, drawing from ideas in unsupervised learning. Weeks 12 and 13 describe the recent use of visualization to support inspection of complex supervised models. We close with a discussion about the possibilities of visualization in the broader intellectual landscape.\nFollowing along\nYou can really only learn visualization by practicing. For this reason, most of the homeworks are technical exercises in building pre-defined visualizations (I do hope you can exercise more creativity through your course project, though). The homeworks will be much more approachable if you have already run all the code accompanying the recordings. I recommend you have a window open for running code while watching the recordings, pausing whenever you want to tinker with how a particular line works.\nThe .Rmd files discussed in each recording are linked at the top of the post. You should be able to run the .Rmd file directly on your computer – if you encounter any issues, don’t hesitate to reach out. Observable notebooks that reproduces figures discussed in the recordings are also linked at the top of each post. See the Introduction to Vega-Lite lecture for a brief description of how to reproduce examples in your own Observable notebooks.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-25T10:41:34-06:00",
    "input_file": {}
  }
]
