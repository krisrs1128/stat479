<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Interpretable Machine Learning   STAT 479 - Spring 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="main_files/libs/clipboard/clipboard.min.js"></script>
<script src="main_files/libs/quarto-html/quarto.js"></script>
<script src="main_files/libs/quarto-html/popper.min.js"></script>
<script src="main_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="main_files/libs/quarto-html/anchor.min.js"></script>
<link href="main_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="main_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="main_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="main_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="main_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="main_files/libs/quarto-diagram/mermaid.min.js"></script>
<script src="main_files/libs/quarto-diagram/mermaid-init.js"></script>
<link href="main_files/libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="main.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Interpretable Machine Learning <br> STAT 479 - Spring 2025</h1>
<p class="subtitle lead">Lecture Slides</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RefManageR)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>bib <span class="ot">&lt;-</span> <span class="fu">ReadBib</span>(<span class="st">"references.bib"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Ignoring entry 'angus2017'  (line100) because:
    A bibentry of bibtype 'Article' has to specify the field: c("journaltitle", "journal")</code></pre>
</div>
</div>
<section id="introduction" class="level1">
<h1>1. Introduction</h1>
<ul>
<li><p>course logistics</p></li>
<li><p>overall motivation</p></li>
<li><p>decomposing interpretability</p></li>
</ul>
<hr>
<section id="course-logistics" class="level2">
<h2 class="anchored" data-anchor-id="course-logistics">Course Logistics</h2>
<hr>
<section id="weekly-rhythm" class="level3">
<h3 class="anchored" data-anchor-id="weekly-rhythm">Weekly rhythm</h3>
<p>Tuesday</p>
<ul>
<li>New methods</li>
<li>Exercises [Conceptual]</li>
</ul>
<p>Thursday</p>
<ul>
<li>Case study using method</li>
<li>Exercises [Technical]</li>
</ul>
<p>Friday (every ~ 3 weeks) * Homework due</p>
<hr>
</section>
<section id="learning-outcommes" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcommes">Learning outcommes</h3>
<ul>
<li><strong>Compare</strong> the competing definitions of interpretable machine learning, the motivations behind them, and metrics that can be used to quantify whether they have been met.</li>
<li>Within a specific application context, <strong>evaluate</strong> the trade-offs associated with competing interpretable machine learning techniques.</li>
<li><strong>Describe</strong> the theoretical foundation of intrinsically interpretable models like sparse regression, gaussian processes, and classification and regression trees, and <strong>apply</strong> them to realistic case studies with appropriate validation checks.</li>
</ul>
<hr>
</section>
<section id="learning-outcommes-1" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcommes-1">Learning outcommes</h3>
<ul>
<li><strong>Describe</strong> the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and <strong>apply</strong> them to realistic case studies with appropriate validation checks</li>
<li><strong>Analyze</strong> large-scale foundation models using methods like sparse autoencoders and <strong>describe</strong> their relevance to problems of model control and AI-driven design.</li>
</ul>
<hr>
</section>
</section>
<section id="materials" class="level2">
<h2 class="anchored" data-anchor-id="materials">Materials</h2>
<p>materials</p>
<hr>
<section id="expectations" class="level3">
<h3 class="anchored" data-anchor-id="expectations">Expectations</h3>
<hr>
</section>
<section id="evaluation" class="level3">
<h3 class="anchored" data-anchor-id="evaluation">Evaluation</h3>
<hr>
</section>
</section>
<section id="what-is-interpretability" class="level2">
<h2 class="anchored" data-anchor-id="what-is-interpretability">What is interpretability?</h2>
<hr>
</section>
<section id="reading" class="level2">
<h2 class="anchored" data-anchor-id="reading">Reading</h2>
<ul>
<li><p>Lipton, Z. C. (2018). The Mythos of Model Interpretability. ACM Queue: Tomorrow’s Computing Today, 16(3), 31–57. https://doi.org/10.1145/3236386.3241340</p></li>
<li><p>Murdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R., &amp; Yu, B. (2019). Definitions, methods, and applications in interpretable machine learning. Proceedings of the National Academy of Sciences of the United States of America, 116(44), 22071–22080. https://doi.org/10.1073/pnas.1900654116</p></li>
</ul>
<hr>
<section id="what-can-go-wrong" class="level3">
<h3 class="anchored" data-anchor-id="what-can-go-wrong">What can go wrong?</h3>
<p><img src="figures/asthma.png" width="1000" class="center"></p>
<p>Example from .</p>
<hr>
</section>
<section id="what-can-go-wrong-1" class="level3">
<h3 class="anchored" data-anchor-id="what-can-go-wrong-1">What can go wrong?</h3>
<p><img src="figures/adversarial-stopsign.webp" width="600" class="center"></p>
<p>Example from .</p>
<hr>
</section>
<section id="what-can-go-wrong-2" class="level3">
<h3 class="anchored" data-anchor-id="what-can-go-wrong-2">What can go wrong?</h3>
<p><img src="figures/bard-hallucination.webp" width="900/"></p>
<hr>
</section>
<section id="what-can-go-wrong-3" class="level3">
<h3 class="anchored" data-anchor-id="what-can-go-wrong-3">What can go wrong?</h3>
<p><img src="figures/spurious_xray.png" width="700" class="center"></p>
<hr>
</section>
<section id="problems" class="level3">
<h3 class="anchored" data-anchor-id="problems">Problems</h3>
<ul>
<li>Fairness</li>
<li>Safety</li>
<li>Misinformation</li>
</ul>
<hr>
</section>
<section id="formulation-gaps" class="level3">
<h3 class="anchored" data-anchor-id="formulation-gaps">Formulation gaps</h3>
<ul>
<li><p>In machine learning, most effort is directed towards ensuring models have good performance performance metrics on external benchmark data sets.</p></li>
<li><p>Models learning this way can be very accurate according to easily measured criteria like accuracy or computational efficiency but inappropriate with respect to properties that are harder to measure.</p></li>
<li><p>The gap between what we want our models to achieve and what we can easily encode in performance metrics is called a formulation gap.</p></li>
</ul>
<hr>
</section>
<section id="formulation-gaps-trust" class="level3">
<h3 class="anchored" data-anchor-id="formulation-gaps-trust">Formulation gaps: Trust</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Would you be willing to relinquish control to the model?</p></li>
<li><p>The answer depends on how it manages individual cases, not just than overall accuracy.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img src="figures/aircanada.png"></p>
</div>
</div>
<hr>
</section>
<section id="formulation-gaps-transferability" class="level3">
<h3 class="anchored" data-anchor-id="formulation-gaps-transferability">Formulation gaps: Transferability</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Typical benchmarks randomly split data into training vs.&nbsp;test sets.</p></li>
<li><p>Models are often used in settings that don’t match those original training/test splits.</p></li>
<li><p>The use of models might themselves change the distribution of the data (pneumonia example from before).</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img src="figures/zillow.png"><br> See (Vern L. Glaser and Safavi, 2023).</p>
</div>
</div>
<hr>
</section>
<section id="formulation-gaps-informativeness" class="level3">
<h3 class="anchored" data-anchor-id="formulation-gaps-informativeness">Formulation gaps: Informativeness</h3>
<ul>
<li><p>Models are often used to support discovery. This is a different task than automation.</p></li>
<li><p>While this is often an argument for using “white box” models, black boxes can still support discovery, e.g., by identifying similar cases in a medical diagnosis system.</p></li>
</ul>
<hr>
</section>
<section id="formulation-gaps-ethics" class="level3">
<h3 class="anchored" data-anchor-id="formulation-gaps-ethics">Formulation gaps: Ethics</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Models might amplify existing biases if only test accuracy is considered.</p></li>
<li><p>Fairness metrics have been defined to help guard against this risk, but there is no universal metric for fairness. Interpretability can help address broader demands for transparency.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img src="figures/propublica_compas.png" width="500/"></p>
</div>
</div>
<hr>
</section>
<section id="exercise-past-experience" class="level3">
<h3 class="anchored" data-anchor-id="exercise-past-experience">Exercise: Past Experience</h3>
<p>Introduce yourself to your neighbors. What is your name and degree program? What are your areas of interest? How might interpretability or explainability be helpful in the work that you do?</p>
<p>Then respond to [Past Experience] in the exercise sheet.</p>
<hr>
</section>
<section id="pdr-framework" class="level3">
<h3 class="anchored" data-anchor-id="pdr-framework">PDR Framework</h3>
<ul>
<li><p>Reference (Murdoch, Singh, Kumbier, Abbasi-Asl, and Yu, 2019) also breaks the vague concept of “interpretability” down into precise elements which can be more formally evaluated.</p></li>
<li><p>Together, this helps establish trust in the reliability of the results, which is important in interdisciplinary work.</p></li>
<li><p>It also helps protect against unintended consequences that can arise after model deployment.</p></li>
</ul>
<hr>
</section>
<section id="pdr-framework-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="pdr-framework-accuracy">PDR Framework: Accuracy</h3>
<ul>
<li><p><strong>Predictive Accuracy</strong>: The model-to-explain has to be accurate. There is not point “interpreting” a model that gives a poor approximation of reality.</p></li>
<li><p><strong>Descriptive Accuracy</strong>: The interpretation should be faithful to the model. This is the extent to which the explanation reflects what the black box actually learned, which is not necessarily the same as what it was designed to learn.</p></li>
</ul>
<hr>
</section>
<section id="the-pdr-desiderata-relevancy" class="level3">
<h3 class="anchored" data-anchor-id="the-pdr-desiderata-relevancy">The PDR Desiderata: Relevancy</h3>
<ul>
<li><p>Interpretations don’t exist in a vacuum. Like data visualizations, their complexity needs to be suitable to their audience.</p></li>
<li><p>For example, we might give three different explanations of the same model depending on whether we are communicating with biologists, clinicians, or statisticians.</p></li>
<li><p>Whether the outputs from interpretability outputs are relevant to their audience can be gauged by their adoption in specific scientific settings or how they are actually used by participants in user studies.</p></li>
</ul>
<hr>
</section>
<section id="spectrum-of-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="spectrum-of-evaluation">Spectrum of evaluation</h3>
<p>Reference (Doshi-Velez and Kim, 2017) notes that new interpretability techniques can be evaluated at several levels.</p>
<ul>
<li><p>Functionally-grounded: Define computational proxy tasks that can be measured without studying real users.</p></li>
<li><p>Human-grounded: Consider simplified tasks that can be solved by general audience members. This can involve crowdsourcing.</p></li>
<li><p>Application grounded: Evaluate in the field with representative experts in a concrete end-use case.</p></li>
</ul>
<hr>
</section>
<section id="phases-of-method-development" class="level3">
<h3 class="anchored" data-anchor-id="phases-of-method-development">Phases of method development</h3>
<ul>
<li><p>These different types of evaluation can inform one another. For example, we can define new proxy tasks based on the most challenging steps for experts.</p></li>
<li><p>New methods that do well in computational proxies are worth investigating through user studies.</p></li>
</ul>
<hr>
</section>
<section id="benchmarking" class="level3">
<h3 class="anchored" data-anchor-id="benchmarking">Benchmarking</h3>
<ul>
<li><p>Each of these types fo evaluation will come with their own performance metrics.</p></li>
<li><p>We will revisit this question periodically as we introduce new methods and study the contexts in which they are worth applying.</p></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="definitions" class="level1">
<h1>2. Definitions</h1>
<ul>
<li><p>intrinsic vs.&nbsp;post-hoc</p></li>
<li><p>global vs.&nbsp;local explanation</p></li>
<li><p>workflow</p></li>
</ul>
<hr>
<section id="learning-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes">Learning outcomes</h3>
<ol type="1">
<li>Compare the competing definitions of interpretable machine learning, the motivations behind them, and metrics that can be used to quantify whether they have been met.</li>
</ol>
<hr>
</section>
<section id="reading-1" class="level2">
<h2 class="anchored" data-anchor-id="reading-1">Reading</h2>
<ul>
<li><p>Lipton, Z. C. (2018). The Mythos of Model Interpretability. ACM Queue: Tomorrow’s Computing Today, 16(3), 31–57. https://doi.org/10.1145/3236386.3241340</p></li>
<li><p>Murdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R., &amp; Yu, B. (2019). Definitions, methods, and applications in interpretable machine learning. Proceedings of the National Academy of Sciences of the United States of America, 116(44), 22071–22080. https://doi.org/10.1073/pnas.1900654116</p></li>
</ul>
<hr>
</section>
<section id="intrinsic-interpretability-vs.-post-hoc-explanations" class="level2">
<h2 class="anchored" data-anchor-id="intrinsic-interpretability-vs.-post-hoc-explanations">Intrinsic Interpretability vs.&nbsp;Post-hoc Explanations</h2>
<ul>
<li><p>Intrinsically interpretable: Build a “glass box” from the start. The model is interpretable by design—its structure allows us to understand how it works.</p></li>
<li><p>Post Hoc: Inspect an already trained “black box” model, which can be chosen simply to maximize accuracy without regard to interpretability. Post-hoc methods extract explanations from models that weren’t designed to be understood</p></li>
</ul>
<p><img src="figures/black_box_flashlight.png" width="900" class="center"></p>
<hr>
<section id="intrinsic-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="intrinsic-interpretability">Intrinsic interpretability</h3>
<p>Some properties that make a model intrinsically interpretable are:</p>
<ul>
<li>Sparsity</li>
<li>Simulatability</li>
<li>Modularity</li>
</ul>
<p>We define these on the next few slides.</p>
<hr>
</section>
<section id="sparsity" class="level3">
<h3 class="anchored" data-anchor-id="sparsity">Sparsity</h3>
<ul>
<li><p>A model is sparse if the number of non-zero parameters is small relative to the total number of available parameters.</p></li>
<li><p>This is interpretability property is motivated by Occam’s razor: The simplest explanation is likely closest to the truth.</p></li>
<li><p>Sparsity enhances interpretability only when it correctly captures the structure of the true data-generating process. If the true relationship depends on many features, imposing sparsity introduces bias.</p></li>
</ul>
<hr>
</section>
<section id="simulatability" class="level3">
<h3 class="anchored" data-anchor-id="simulatability">Simulatability</h3>
<p>A model is simulatable if it’s possible to manually compute its output for any input within a reasonable time. Both the number of model parameters and the inference complexity factor in.</p>
<p><a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/"> <img src="figures/simulatability.gif" class="center" width="650/"> </a></p>
<hr>
</section>
<section id="modularity" class="level3">
<h3 class="anchored" data-anchor-id="modularity">Modularity</h3>
<ul>
<li><p>A model is modular if its prediction function <span class="math inline">\(f(x)\)</span> can be decomposed into interpretable components, each of which can be analyzed independently.</p></li>
<li><p>One example is an additive decomposition, where the function can be written as,</p>
<p><span class="math display">\[\begin{align*}
  f\left(x\right) = b_{0} + \sum_{j = 1}^{J}f_{j}\left(x_{j}\right)
  \end{align*}\]</span></p>
<p>and each <span class="math inline">\(f_{i}\)</span> operates on only a single coordinate of the input <span class="math inline">\(x\)</span>.</p></li>
<li><p>More generally, a model is modular if subsets of its parameters or computations can be viewed as separate, interpretable units.</p></li>
</ul>
<hr>
</section>
<section id="exercise-linear-model-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="exercise-linear-model-interpretability">Exercise: Linear Model Interpretability</h3>
<hr>
</section>
<section id="post-hoc-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="post-hoc-interpretability">Post-hoc interpretability</h3>
<p>Some common strategies for explaining black box models include,</p>
<ul>
<li><p>Feature importances</p></li>
<li><p>Feature attributions</p></li>
<li><p>Model distillations</p></li>
</ul>
<p>A few examples are given on the next few slides, but first we should distinguish between global and local explanations.</p>
<hr>
</section>
<section id="global-explanation" class="level3">
<h3 class="anchored" data-anchor-id="global-explanation">Global explanation</h3>
<ul>
<li><p>A global explanation characterizes how a model behaves across all possible inputs.</p></li>
<li><p>These explanations are valuable in scientific studies, where we usually look for universal rules relating sets of variables.</p></li>
</ul>
<p><img src="figures/explanation_types.png" width="500" class="center"></p>
<hr>
</section>
<section id="example-variable-importance" class="level3">
<h3 class="anchored" data-anchor-id="example-variable-importance">Example: Variable importance</h3>
<p>According to this plot, the variables X4, X2, and X1 seem most important across three separate tree-based models (weeks 4 - 5).</p>
<p><img src="figures/vip_plots.png" width="800" class="center"></p>
<hr>
</section>
<section id="local-explanation" class="level3">
<h3 class="anchored" data-anchor-id="local-explanation">Local explanation</h3>
<ul>
<li><p>A local explanation describes why a model made a specific prediction for a particular input. The relationships it finds may be unique to that example.</p></li>
<li><p>These are especially helpful in auditing high-stakes decisions made in specific cases, e.g.&nbsp;loan approvals, medical diagnoses, parole decisions.</p></li>
</ul>
<p><img src="figures/explanation_types.png" width="500" class="center"></p>
<hr>
</section>
<section id="example-saliency-map" class="level3">
<h3 class="anchored" data-anchor-id="example-saliency-map">Example: saliency map</h3>
<p>In vision models, one use case of local explanations is to identify the parts of an image that are “most important” for particular predicted classes.</p>
<p><img src="figures/saliency_example.png" width="850" class="center"></p>
<hr>
</section>
<section id="example-shap-attributions" class="level3">
<h3 class="anchored" data-anchor-id="example-shap-attributions">Example: SHAP attributions</h3>
<p>For this sample, the most “important” feature is <code>Capital Gain</code>. It is responsible for much of the negative prediction <span class="math inline">\(f\left(x\right)\)</span>, in a sense that will be made precise when we discuss SHAP.</p>
<p><img src="figures/shap_waterfall.png" class="center"></p>
<hr>
</section>
<section id="interpretability-accuracy-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="interpretability-accuracy-trade-offs">Interpretability-accuracy trade-offs</h3>
<ul>
<li><p>For many datasets, simple models (like decision trees) offer high descriptive accuracy but have lower predictive accuracy compared to more complex models (like random forests).</p></li>
<li><p>Similarly, deep neural networks might predict the future well but not be amenable to accurate descriptions in PDR sense.</p></li>
</ul>
<hr>
</section>
<section id="bias-and-variance" class="level3">
<h3 class="anchored" data-anchor-id="bias-and-variance">Bias and Variance</h3>
<p>One intuition for this tradeoff comes from the bias variance tradeoff. More complex models have lower bias, and if we have lots of data, variance is less of a concern.</p>
<p><img src="figures/bias_variance.png" class="center"></p>
<hr>
</section>
<section id="does-it-exist" class="level3">
<h3 class="anchored" data-anchor-id="does-it-exist">Does it exist?</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>Unlike the bias-variance tradeoff, which has a precise mathematical foundation, the interpretability-accuracy tradeoff is more of a heuristic, and some have argued that it can be misleading (Rudin, 2019).</p>
</div><div class="column" style="width:50%;">
<p><a href="https://research-vp.tau.ac.il/sites/resauth.tau.ac.il/files/DARPA-BAA-16-53_Explainable_Artificial_Intelligence.pdf"> <img src="figures/tradeoff_darpa.png" width="400/"> </a></p>
</div>
</div>
<hr>
</section>
<section id="exercise" class="level3">
<h3 class="anchored" data-anchor-id="exercise">Exercise:</h3>
<hr>
</section>
<section id="recommended-workflow" class="level3">
<h3 class="anchored" data-anchor-id="recommended-workflow">Recommended workflow</h3>
<p>If applied systematically, techniques from interpretability can be used to check assumptions, identify data and model quality issues, and uncover surprising relationships. The steps below expand on ideas in (Murdoch, Singh, Kumbier et al., 2019).</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">%%{init: {'theme':'forest', 'themeVariables': {'fontSize':'36px', 'fontFamily':'arial'}, 'flowchart': {'padding': 80}}}%%
graph LR
    A[Design] --&gt; B[Predictive &lt;br/&gt; Accuracy]
    B --&gt; C[Stability]
    C --&gt; D[Explanation &lt;br/&gt; Comparisons]
    D --&gt; E[External &lt;br/&gt; Checks]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<hr>
</section>
<section id="data-collection" class="level3">
<h3 class="anchored" data-anchor-id="data-collection">Data collection</h3>
<ol type="1">
<li><p>First consider the underlying data collection mechanism. This is essential context for any downstream interpretation.</p></li>
<li><p>Were the data purely observational, and if so, what led to a sample being observed (or missed)?</p></li>
<li><p>If it is from an experiment, which aspects were controlled? In what ways is the experimental system representative of the end-use?</p></li>
</ol>
<hr>
</section>
<section id="predictive-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="predictive-accuracy">Predictive accuracy</h3>
<ol type="1">
<li><p>Measure the model’s fit to the data using some measure of performance accuracy on a test set.</p></li>
<li><p>The test set should mimic what would be encountered during deployment as closely as possible, even though this is impossible in situations where the environment is always changing.</p></li>
</ol>
<hr>
</section>
<section id="interpretationexplanation" class="level3">
<h3 class="anchored" data-anchor-id="interpretationexplanation">Interpretation/Explanation</h3>
<p>Next we can apply our interpretability technique. We should pay attention to <strong>descriptive accuracy</strong>. For example,</p>
<ul>
<li><p>How many features do we need before the SHAP approximation matches the full model prediction (Week 6 - 7)?</p></li>
<li><p>How closely does the distilled model match the original model’s predictions (Week 10)?</p></li>
<li><p>If using permutation importance, are the permuted data plausible, or do they lie outside the training data (Week 4)?</p></li>
</ul>
<hr>
</section>
<section id="stability" class="level3">
<h3 class="anchored" data-anchor-id="stability">Stability</h3>
<ol type="1">
<li><p>Are the interpretations robust to perturbations?</p></li>
<li><p>Data perturbations: Do the same interpretations appear when using different subsamples?</p></li>
<li><p>Hyperparameter perturbations: Do the same interpretations across similar hyperparameters?</p></li>
<li><p>Model perturbations: If using a post-hoc explanation, we can see whether the same features arise across multiple models?</p></li>
</ol>
<hr>
</section>
<section id="compare-interpretations" class="level3">
<h3 class="anchored" data-anchor-id="compare-interpretations">Compare interpretations</h3>
<ol type="1">
<li><p>Apply multiple interpretation methods to the same model.</p></li>
<li><p>If methods give contradictory results, then at least one has low descriptive accuracy.</p></li>
</ol>
<hr>
</section>
<section id="external-checks" class="level3">
<h3 class="anchored" data-anchor-id="external-checks">External checks</h3>
<p>Test interpretations against domain knowledge or experimental results. If the interpretation claims feature <span class="math inline">\(j\)</span> is important, then can be checked by</p>
<ul>
<li>comparing with existing scientific results</li>
<li>measuring performance when feature <span class="math inline">\(j\)</span> is removed or held constant</li>
<li>running new experiments that manipulate feature <span class="math inline">\(j\)</span>.</li>
</ul>
<hr>
</section>
<section id="explanations-for-communication" class="level3">
<h3 class="anchored" data-anchor-id="explanations-for-communication">Explanations for Communication</h3>
<p>The talk (Kim, 2022) describes how interpretability bridges human and machine “concepts.”</p>
<p><img src="figures/kim_iclr.png" class="center" width="700/"></p>
<p>This bridge is especially important in collaborative work! Data science never exists in a vacuum.</p>
<hr>
</section>
</section>
</section>
<section id="linear-models" class="level1">
<h1>3. Linear Models</h1>
<ul>
<li><p>motivating case study</p></li>
<li><p>linear models</p></li>
<li><p>sparse linear models</p></li>
<li><p>bias variance trade-off</p></li>
</ul>
<hr>
<section id="reading-2" class="level2">
<h2 class="anchored" data-anchor-id="reading-2">Reading</h2>
<ul>
<li>Sections 10.4 - 10.6. Yu, B., &amp; Barter, R. L. (2024). Veridical data science. London, England: MIT Press. https://vdsbook.com/10-ls_continued</li>
</ul>
<hr>
<section id="learning-outcomes-1" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-1">Learning outcomes</h3>
<ol type="1">
<li>Describe the theoretical foundation of intrinsically interpretable models like sparse regression, gaussian processes, and classification and regression trees, and apply them to realistic case studies with appropriate validation checks.</li>
<li>Compare the competing definitions of interpretable machine learning, the motivations behind them, and metrics that can be used to quantify whether they have been met.</li>
</ol>
<hr>
</section>
<section id="drug-response-prediction" class="level3">
<h3 class="anchored" data-anchor-id="drug-response-prediction">Drug response prediction</h3>
<ul>
<li><p>Patients with the same diagnosed cancer often respond very differently to the same drug. How can we figure out which drugs any particular patient will respond to?</p></li>
<li><p>If we imagine <code>drug effectiveness = f(gene activity)</code>, then one approach is to measure activity of genetic pathways in biopsies of that patient’s cancer tissue.</p></li>
<li><p>The most important features in that model can be used to stratify patients into different responder/non-responder subtypes.</p></li>
</ul>
<hr>
</section>
<section id="study-design" class="level3">
<h3 class="anchored" data-anchor-id="study-design">Study design</h3>
<ul>
<li><p>To learn this relationship, previous studies used “immortalized” cancer cell lines. These doesn’t fully represent the complexity of cancer in real populations.</p></li>
<li><p>The study (Dietrich, Oleś, Lu, Sellner, Anders, Velten, Wu, Hüllein, da Silva Liberio, Walther, Wagner, Rabe, Ghidelli-Disse, Bantscheff, Oleś, Słabicki, Mock, Oakes, Wang, Oppermann, Lukas, Kim, Sill, Benner, Jauch, Sutton, Young, Rosenquist, Liu, Jethwa, Lee, Lewis, Putzker, Lutz, Rossi, Mokhir, Oellerich, Zirlik, Herling, Nguyen-Khac, Plass, Andersson, Mustjoki, von Kalle, Ho, Hensel, Dürig, Ringshausen, Zapatka, Huber, and Zenz, 2017) measured drug responses in samples from primary patients who were being treated for blood cancer (CLL). They simultaneously measured gene expression and DNA methylation activity (we will skip over some the biology, but happy to discuss this with anyone who’s interested).</p></li>
</ul>
<hr>
</section>
<section id="drug-response-data" class="level3">
<h3 class="anchored" data-anchor-id="drug-response-data">Drug response data</h3>
<p><em>Features</em> - 121 samples from CLL patients - 61 drugs, 5 dosages per drug - 9553 features total</p>
<p><em>Outcome</em> - Cell viability: Percentage of surviving cells after drug exposure</p>
<hr>
</section>
<section id="main-question" class="level3">
<h3 class="anchored" data-anchor-id="main-question">Main Question</h3>
<p><br> <br></p>
<p><strong>For a given drug treatment profile, which RNA or methylation features differentiate between drug sensitivity vs.&nbsp;resistance?</strong></p>
<hr>
</section>
<section id="review-code-example" class="level3">
<h3 class="anchored" data-anchor-id="review-code-example">Review Code Example</h3>
<ul>
<li><code>01-linear_model.qmd</code>: “Exploration”</li>
</ul>
<hr>
</section>
</section>
<section id="sparse-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="sparse-linear-regression">Sparse Linear Regression</h2>
<ul>
<li><p>Viability can be viewed as a <strong>response variable</strong> <span class="math inline">\(y \in \mathbb{R}^{N}\)</span>, and the molecular variables can be treated as <strong>features</strong> <span class="math inline">\(X \in \mathbb{R}^{N  \times J}\)</span>. Here</p></li>
<li><p>The setting is <strong>high-dimensional</strong> with few samples compared to features. Without some sort of regularization (like a sparsity assumption), the problem is underdetermined, since <span class="math inline">\(N = 121, J = 9553\)</span>.</p></li>
<li><p><strong>Sparsity</strong> will help us focus on the most important pathways out of thousands of candidates.</p></li>
</ul>
<p>This was a real study! Linear models are not just for class work…they are a practical tool in science.</p>
<hr>
<section id="single-continuous-predictor" class="level3">
<h3 class="anchored" data-anchor-id="single-continuous-predictor">Single continuous predictor</h3>
<p>Imagine modeling viability <span class="math inline">\(y_{i}\)</span>​ for sample <span class="math inline">\(i\)</span> as a linear function of a single gene’s expression level <span class="math inline">\(x_{i} \in \mathbb{R}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
y_i=\beta_0+x_{i 1} \beta_1+\epsilon_i
\end{align*}\]</span></p>
<p>The least-squares estimate <span class="math inline">\(\hat{\beta} := \left(\hat{\beta}_{0}, \hat{\beta}_{1}\right)\)</span>​ is found by minimizing</p>
<p><span class="math display">\[\begin{align*}
\min_{\beta_0, \beta_1} \sum_{i = 1}^{N}\left(y_i-\beta_0-x_{i} \beta_1\right)^2
\end{align*}\]</span></p>
<hr>
</section>
<section id="visualization" class="level3">
<h3 class="anchored" data-anchor-id="visualization">Visualization</h3>
<hr>
</section>
<section id="single-categorical-predictor" class="level3">
<h3 class="anchored" data-anchor-id="single-categorical-predictor">Single categorical predictor</h3>
<p>That visualization considered <span class="math inline">\(x_{i}\)</span> to be continuous, but the same idea applies to categorical predictors. For example, <span class="math inline">\(x_{i} \in \{0, 1\}\)</span> could record whether the patient has a mutation in a particular gene.</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span>​: The expected viability for the reference group (<span class="math inline">\(x_{i} = 0\)</span>).</p></li>
<li><p><span class="math inline">\(\beta_1\)</span>: The difference in expected viability between the mutated <span class="math inline">\(x_{i} = 1\)</span> and reference groups</p></li>
</ul>
<hr>
</section>
<section id="visualization-1" class="level3">
<h3 class="anchored" data-anchor-id="visualization-1">Visualization</h3>
<hr>
</section>
</section>
<section id="multiple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression">Multiple Linear Regression</h2>
<p>Assumed model form:</p>
<p><span class="math display">\[\begin{align*}
y_{i} &amp;= \sum_{j = 1}^{n}x_{ij}\beta_{j} + \epsilon_{i} \\
&amp;:= \mathbf{x}_{i}^\top \beta_{j} + \epsilon_{i}
\end{align*}\]</span></p>
<ul>
<li><p><span class="math inline">\(\epsilon_{i}\)</span> represents random variation due to unmeasured factors.</p></li>
<li><p>This model assumes the effect of feature <span class="math inline">\(j\)</span> on <span class="math inline">\(y\)</span> is constant regardless of the value of <span class="math inline">\(x_{k}\)</span> for any other feature <span class="math inline">\(k\)</span>.</p></li>
</ul>
<hr>
<section id="fitting-multiple-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="fitting-multiple-linear-regression">Fitting Multiple Linear Regression</h3>
<ul>
<li>We can estimate <span class="math inline">\(\hat{\beta} \in \mathbf{R}^{J}\)</span> by minimizing the residual sum of squares (RSS) loss,</li>
</ul>
<p><span class="math display">\[\begin{align*}
\min_{\beta} \sum_{i=1}^N \left( y_i - \mathbf{x}_i^\top \beta \right)^2
\end{align*}\]</span></p>
<hr>
</section>
<section id="visualization-two-continuous-predictors" class="level3">
<h3 class="anchored" data-anchor-id="visualization-two-continuous-predictors">Visualization: Two continuous predictors</h3>
<p>We can imagine how viability changes when changing the expression levels for two genes simultaneously.</p>
<hr>
</section>
<section id="visualization-one-continuous-one-categorical-predictor" class="level3">
<h3 class="anchored" data-anchor-id="visualization-one-continuous-one-categorical-predictor">Visualization: One continuous + one categorical predictor</h3>
<p>If we include a continuous pathway measurement and a binary mutation, the model generates one line per category, and the lines are parallel.</p>
<hr>
</section>
<section id="ceteris-paribus" class="level3">
<h3 class="anchored" data-anchor-id="ceteris-paribus">Ceteris paribus</h3>
<ul>
<li><p>“All other things being equal”.</p></li>
<li><p><span class="math inline">\(\beta_j\)</span>​ gives the impact of changing <span class="math inline">\(x_j\)</span> while every other feature <span class="math inline">\(k\)</span> in the model is fixed.</p></li>
<li><p>Warning: In our application, genes are often highly correlated. Holding other genes “fixed” is not realistic. This should be kept in mind whenever working with correlated features.</p></li>
</ul>
<hr>
</section>
<section id="interaction-terms" class="level3">
<h3 class="anchored" data-anchor-id="interaction-terms">Interaction terms</h3>
<ul>
<li><p>If we had <span class="math inline">\(J = 2\)</span>, we can consider two-way interactions,</p>
<p><span class="math display">\[\begin{align*}
  y_{i} = \beta_{0} + x_{1}\beta_{1} + x_{2}\beta_{2} + x_{1}x_{2}\beta_{3} + \epsilon_{i}
  \end{align*}\]</span></p>
<p>the coefficient <span class="math inline">\(\beta_{3}\)</span> represents the interaction.</p></li>
<li><p>The slope of for one gene now depends on the value of another (what does this mean algebraically?)</p></li>
<li><p>In our example, a gene might be important only for a particular drug.</p></li>
</ul>
<hr>
</section>
<section id="irrelevant-predictors" class="level3">
<h3 class="anchored" data-anchor-id="irrelevant-predictors">Irrelevant predictors?</h3>
<ul>
<li><p>If we had many noise features (unrelated to response), least squares will still try finding coefficients for each of them. This causes overfitting: our predictions would depend on variables that don’t actually matter.</p></li>
<li><p>We don’t expect all genes to be relevant to cell viability in this experiment. It’s more likely that a few key pathways are driving resistance.</p></li>
<li><p><strong>Variable selection</strong>: If the noise variables do not reduce the SSE, then the Lasso sets their coefficients <span class="math inline">\(\beta_{j}\)</span> to exactly zero. The <span class="math inline">\(\ell^{1}\)</span> penalty “induces sparsity.”</p></li>
</ul>
<hr>
</section>
</section>
<section id="ell1-regularization" class="level2">
<h2 class="anchored" data-anchor-id="ell1-regularization"><span class="math inline">\(\ell^{1}\)</span> Regularization</h2>
<p>The Lasso objective is <span class="math display">\[\begin{align*}
\min_{\beta \in \mathbb{R}^J} \left[ \frac{1}{2N} \sum_{i=1}^N \left(y_i - \mathbf{x}_i^\top \beta\right)^2 + \lambda \lVert \beta \rVert_1 \right].
\end{align*}\]</span> This is the same loss as linear regression, but with a new <span class="math inline">\(\ell^{1}\)</span> penalty</p>
<p><span class="math display">\[\begin{align*}
\|\beta\|_{1} := \sum_{j = 1}^{J} \left|\beta_{j}\right|
\end{align*}\]</span></p>
<hr>
</section>
<section id="ell1-regularization-1" class="level2">
<h2 class="anchored" data-anchor-id="ell1-regularization-1"><span class="math inline">\(\ell^{1}\)</span> Regularization</h2>
<p>It’s not obvious, but the minimizers often set coordinates <span class="math inline">\(\beta_{j}\)</span> to <em>exactly</em> zero. The “selected” features are those where <span class="math inline">\(\beta_{j} \neq 0\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\min_{\beta \in \mathbb{R}^J} \left[ \frac{1}{2N} \sum_{i=1}^N \left(y_i - \mathbf{x}_i^\top \beta\right)^2 + \lambda \lVert \beta \rVert_1 \right].
\end{align*}\]</span></p>
<hr>
<section id="visualization-2" class="level3">
<h3 class="anchored" data-anchor-id="visualization-2">Visualization</h3>
<hr>
</section>
<section id="lasso-paths" class="level3">
<h3 class="anchored" data-anchor-id="lasso-paths">Lasso Paths</h3>
<ul>
<li><p><span class="math inline">\(\lambda\)</span> is a hyperparameter that controls model complexity.</p></li>
<li><p>As <span class="math inline">\(\lambda \uparrow \infty\)</span>, all coefficients shrink toward zero. As <span class="math inline">\(\lambda \downarrow 0\)</span>, we return to OLS.</p></li>
<li><p>We can study the “order” that variables enter the model as we gradually decrease <span class="math inline">\(\lambda\)</span>. We expect the most important predictors (e.g., drug response-related genes) to enter first.</p></li>
</ul>
<hr>
</section>
<section id="choosing-lambda" class="level3">
<h3 class="anchored" data-anchor-id="choosing-lambda">Choosing <span class="math inline">\(\lambda\)</span></h3>
<ul>
<li><p>What is an appropriate “budget” for model complexity?</p></li>
<li><p>Bias-Variance Trade-off: Decreasing <span class="math inline">\(\lambda\)</span> reduces bias but increases the variance of the predictions.</p></li>
<li><p>We need to tune <span class="math inline">\(\lambda\)</span> to balance these competing issues and achieve good performance on holdout samples.</p></li>
</ul>
<hr>
</section>
<section id="cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="cross-validation">Cross-validation</h3>
<p>Split the data into <span class="math inline">\(K\)</span> folds (e.g., <span class="math inline">\(K = 5\)</span>). Fit the model on <span class="math inline">\(K−1\)</span> folds and tested on the remaining fold. This mimics the setting of gathering new data and testing the model on that data.</p>
<hr>
</section>
<section id="hyperparameter-selection" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-selection">Hyperparameter Selection</h3>
<p>(show the glmnet curve)</p>
<ul>
<li><p><span class="math inline">\(\lambda_{\text{min}}\)</span>: Minimizes the cross-validation error.</p></li>
<li><p><span class="math inline">\(\lambda_{\text{1se}}\)</span>: The simplest model, i.e., largest <span class="math inline">\(\lambda\)</span>, whose error is within one standard error of the minimum. This is a sparser model with comparable performance to the best.</p></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="linear-models-1" class="level1">
<h1>4. Linear Models</h1>
<ul>
<li><p>interpreting sparse linear models</p></li>
<li><p>application to case study</p></li>
<li><p>interpretation challenges</p></li>
</ul>
<hr>
<section id="learning-outcomes-2" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-2">Learning outcomes</h3>
<ol type="1">
<li>Describe the theoretical foundation of intrinsically interpretable models like sparse regression, gaussian processes, and classification and regression trees, and apply them to realistic case studies with appropriate validation checks.</li>
<li>Compare the competing definitions of interpretable machine learning, the motivations behind them, and metrics that can be used to quantify whether they have been met.</li>
</ol>
<hr>
</section>
<section id="reminder" class="level3">
<h3 class="anchored" data-anchor-id="reminder">Reminder</h3>
<p>These were the properties of intrinsically interpretable models we introduced earlier.</p>
<ul>
<li><strong>Sparsity</strong>: The model uses only a few of the many candidate features.</li>
<li><strong>Simulatability</strong>: The ability to manually calculate the outcome from the inputs.</li>
<li><strong>Modularity</strong>: Each component can be understood independently from the others.</li>
</ul>
<hr>
</section>
<section id="parsimony" class="level3">
<h3 class="anchored" data-anchor-id="parsimony">Parsimony</h3>
<ul>
<li><p>The <span class="math inline">\(\ell^{1}\)</span>-penalized linear model focuses our attention on a small subset of selected features where <span class="math inline">\(\hat{\beta}_{j} = 0\)</span>.</p></li>
<li><p>The larger the <span class="math inline">\(\lambda\)</span>, the more interpretable the model becomes, since the support set <span class="math inline">\(S = \{j : \hat{\beta}_{j} \neq 0\}\)</span> shrinks.</p></li>
</ul>
<hr>
</section>
<section id="simulatability-1" class="level3">
<h3 class="anchored" data-anchor-id="simulatability-1">Simulatability</h3>
<ul>
<li><p>A linear model <span class="math inline">\(\hat{y} = \hat{\beta}_{0} + \sum_{j \in S} x_j \hat{\beta}_{j}\)</span> is reasonably simulatable when <span class="math inline">\(|S|\)</span> is small.</p></li>
<li><p>But for even moderate to large <span class="math inline">\(|S|\)</span>, simulatability becomes a challenge even for linear regression (Slack, Friedler, Scheidegger, and Roy, 2019)!</p></li>
</ul>
<hr>
</section>
<section id="modularity-1" class="level3">
<h3 class="anchored" data-anchor-id="modularity-1">Modularity</h3>
<ul>
<li><p>The model is additive: <span class="math inline">\(f(x) = \sum_{j = 1}^{J}f_j(x_j)\)</span>.</p></li>
<li><p>We can inspect the influence of gene <span class="math inline">\(j\)</span> using a single coefficient <span class="math inline">\(\beta_{j}\)</span>.</p></li>
<li><p>But even in linear regression, ceteris paribus means each coefficient has to be understood relative to all other features in the model.</p></li>
</ul>
<hr>
</section>
<section id="global-local-interpretations" class="level3">
<h3 class="anchored" data-anchor-id="global-local-interpretations">Global &amp; local interpretations</h3>
<ul>
<li><p><strong>Global</strong>: <span class="math inline">\(\hat{\beta}\)</span> helps us identify the most influential genes across the study.</p></li>
<li><p><strong>Local</strong>: For cell <span class="math inline">\(i\)</span>, the <span class="math inline">\(x_{ij}\hat{\beta}_{j}\)</span> terms help us understand its particular viability prediction.</p></li>
</ul>
<hr>
</section>
<section id="case-study-formulation" class="level3">
<h3 class="anchored" data-anchor-id="case-study-formulation">Case study formulation</h3>
<ul>
<li><p><strong>Response</strong> (<span class="math inline">\(y\)</span>): Cell viability after drug treatment.</p></li>
<li><p><strong>Predictors</strong> (<span class="math inline">\(X\)</span>): High-dimensional molecular profiles <span class="math inline">\(N = 121, J = 9553\)</span>.</p></li>
<li><p><strong>Goal</strong>: Identify a sparse set of features that distinguish drug sensitive vs.&nbsp;resistant cells.</p></li>
</ul>
<hr>
</section>
<section id="model-fitting" class="level3">
<h3 class="anchored" data-anchor-id="model-fitting">Model fitting</h3>
<p>We use the <code>glmnet</code> package in R:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the lasso path and perform 10-fold CV</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>cv_fit <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This solves the optimization problem for a sequence of <span class="math inline">\(\lambda\)</span> values using cyclical coordinate descent.</p>
<hr>
</section>
<section id="evaluation-criteria" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-criteria">Evaluation criteria</h3>
<p>These boxplots show the performance on the holdout folds across a range of <span class="math inline">\(\lambda\)</span> complexity parameters. For each <span class="math inline">\(k = 1, \dots, 10\)</span>, we compute</p>
<p><span class="math display">\[\begin{align*}
CV_{k}\left(\lambda\right) = \frac{1}{N} \sum_{i \in I_{k}} \left(y_{i} - \hat{y}_{i}^{-k}\left(\lambda\right)\right)^2
\end{align*}\]</span> where <span class="math display">\[\begin{align*}
y_{i}^{-k}\left(\lambda\right) := x_{i}^\top \hat{\beta}^{-k}\left(\lambda\right)
\end{align*}\]</span> and <span class="math inline">\(\hat{\beta}^{-k}\left(\lambda\right)\)</span> solves the lasso optimization at hyperparameter <span class="math inline">\(\lambda\)</span> using data from all folds except <span class="math inline">\(I_{k}\)</span>.</p>
<hr>
</section>
<section id="evaluation-criteria-1" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-criteria-1">Evaluation criteria</h3>
<ul>
<li><p>The initial decrease is the phase where adding predictors rapidly improves performance. Usually the boxplots increase again for small values of <span class="math inline">\(\lambda\)</span>, at which point the model is overfitting.</p></li>
<li><p>We use <span class="math inline">\(\lambda_{1\text{se}}\)</span> to choose the simplest model whose error is within one standard error of the minimum MSE. This is the point where adding new genes doesn’t significantly add to the holdout prediction accuracy.</p></li>
</ul>
<hr>
</section>
<section id="data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation">Data preparation</h3>
<ul>
<li><p>It’s important to transform <span class="math inline">\(x_{ij} \to \frac{x_{ij} - \bar{x}_j}{\hat{\sigma}_j}\)</span>.</p></li>
<li><p>The penalty <span class="math inline">\(\lambda \sum |\beta_j|\)</span> treats all <span class="math inline">\(\beta_j\)</span> equally. If <span class="math inline">\(x_j\)</span> has a large scale, its <span class="math inline">\(\beta_j\)</span> will be small, making it “cheaper” for the penalty to keep it in the model.</p></li>
</ul>
<hr>
</section>
<section id="coefficient-paths" class="level3">
<h3 class="anchored" data-anchor-id="coefficient-paths">Coefficient paths</h3>
<ul>
<li><p>We can visualize how <span class="math inline">\(\hat{\beta}_j\)</span> evolves as <span class="math inline">\(\lambda\)</span> decreases.</p></li>
<li><p>Each line represents a gene (RNA or methylation feature). The order in which they “emerge” from zero indicates their relative importance in predicting drug response.</p></li>
</ul>
<hr>
</section>
<section id="coefficient-paths-1" class="level3">
<h3 class="anchored" data-anchor-id="coefficient-paths-1">Coefficient paths</h3>
<ul>
<li><p>To make it easier to see the feature names, we can use a heatmap instead.</p></li>
<li><p>In this problem, all the coefficients were positive, so we log-transformed to more clearly see differences in coefficient value.</p></li>
<li><p>This helps us focus on a much more reasonable set of important genomic features. We can also find extenral evidence that these genes are involved in important oncogenic pathways.</p></li>
</ul>
<hr>
</section>
<section id="takeaways" class="level3">
<h3 class="anchored" data-anchor-id="takeaways">Takeaways</h3>
<ul>
<li><p>Sparse linear models narrowed us down from 9K+ features to 37 that are enough to predict drug sensitivity with relatively high accuracy.</p></li>
<li><p>In this problem, standardization was essential. We are lucky that it is the default in <code>glmnet</code>, but this isn’t always the case.</p></li>
</ul>
<hr>
</section>
<section id="possible-improvements" class="level3">
<h3 class="anchored" data-anchor-id="possible-improvements">Possible Improvements</h3>
<ul>
<li><p>The original scale of the features might be informative. By standardizing features, we might be giving undue weight to noise features with low variability.</p></li>
<li><p>Are there distinct gene pathway signatures for different drugs? In this case, we should consider either drug-pathway interaction terms or drug-specific classifiers.</p></li>
</ul>
<hr>
</section>
<section id="instability" class="level3">
<h3 class="anchored" data-anchor-id="instability">Instability</h3>
<ul>
<li><p><strong>The Problem</strong>: When predictors are highly correlated, the <span class="math inline">\(\hat{\beta}\)</span> estimated by lasso becomes unstable.</p></li>
<li><p>Small changes in the training data can cause the lasso to switch between two highly correlated genes, leading to conflicting interpretations of the same underlying signal.</p></li>
</ul>
<hr>
<p>(todo)</p>
</section>
<section id="simulation-setup" class="level3">
<h3 class="anchored" data-anchor-id="simulation-setup">Simulation setup</h3>
<ul>
<li><p><strong>Design</strong>: Take two highly correlated genes (<span class="math inline">\(r &gt; 0.9\)</span>) and use them to predict viability.</p></li>
<li><p><strong>Experiment</strong>: Generate 100 bootstrap resamples of the CLL data and fit a lasso model to each.</p></li>
</ul>
<hr>
<p>(todo)</p>
</section>
<section id="simulation-results" class="level3">
<h3 class="anchored" data-anchor-id="simulation-results">Simulation results</h3>
<ul>
<li><p>Observe how often each gene is selected.</p></li>
<li><p>In many cases, the lasso will pick Gene A in 50% of the trials and Gene B in the other 50%, even if both are equally important.</p></li>
</ul>
<hr>
</section>
<section id="takeaway" class="level3">
<h3 class="anchored" data-anchor-id="takeaway">Takeaway</h3>
<ul>
<li><p>Selection <span class="math inline">\(\neq\)</span> Importance</p>
<ul>
<li>Just because a gene was not selected by the lasso does not mean it is not biologically relevant – it could be correlated with a gene that’s part of the true biological mechanism.</li>
</ul></li>
<li><p>More generally, we careful applying sparse models on correlated data. There may be many plausible, equally predictive explanations based on different subsets of features.</p></li>
</ul>
<hr>
</section>
<section id="interpretability-vs.-accuracy-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="interpretability-vs.-accuracy-trade-offs">Interpretability vs.&nbsp;accuracy trade-offs</h3>
<ul>
<li><p>Often, people assume that to get higher accuracy, we need “black-box” models (like deep learning or Random Forests) and sacrifice interpretability.</p></li>
<li><p>But in many scientific problems (like today’s case study, see also (, )), sparse linear models are surprisingly competitive, while remaining more interpretable to more audiences. It depends on the true relationship between predicators and response in the data.</p></li>
</ul>
<hr>
</section>
</section>
<section id="nonlinear-models" class="level1">
<h1>5. Nonlinear Models</h1>
<ul>
<li><p>motivating case study</p></li>
<li><p>kernels</p></li>
<li><p>GP regression</p></li>
<li><p>tuning</p></li>
</ul>
<hr>
<section id="learning-outcomes-3" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-3">Learning outcomes</h3>
<ol type="1">
<li>Describe the theoretical foundation of intrinsically interpretable models like sparse regression, gaussian processes, and classification and regression trees, and apply them to realistic case studies with appropriate validation checks.</li>
</ol>
<hr>
</section>
<section id="reading-3" class="level2">
<h2 class="anchored" data-anchor-id="reading-3">Reading</h2>
<ul>
<li><p>Görtler, J., Kehlbeck, R., &amp; Deussen, O. (2019). A visual exploration of Gaussian processes. Distill, 4(4). doi:10.23915/distill.00017</p></li>
<li><p>Deisenroth, M., Luo, Y., &amp; van der Wilk, M. (2020). A practical guide to Gaussian processes. https://infallible-thompson-49de36.netlify.app/.</p></li>
</ul>
<hr>
</section>
<section id="motivating-case-study" class="level2">
<h2 class="anchored" data-anchor-id="motivating-case-study">Motivating Case Study</h2>
<p><strong>Question:</strong> How quickly does a star rotate around its axis? This has to be answered accurately before we can search for exoplanets around the star.</p>
<p><strong>Data:</strong> Telescopes make it possible to measure the brightness of a star over time. We can look for periodic patterns in the brightness.</p>
<p><img src="figures/kepler_telescope.jpg"></p>
<hr>
<section id="intuition" class="level3">
<h3 class="anchored" data-anchor-id="intuition">Intuition</h3>
<p>Ideally the brighness would be a perfect sine wave. But brightness changes in more complex ways both on the surface of the star and in the space from the star to the telescope.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img src="figures/lighthouse.gif"></p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.1     ✔ stringr   1.5.2
✔ ggplot2   4.0.0     ✔ tibble    3.3.0
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.1.0     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<hr>
</section>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<p>How does flux vary as the star rotates?</p>
<p><img src="figures/stellar-variability_3_0.png" class="center" width="700/"></p>
<hr>
</section>
<section id="statistical-motivation" class="level3">
<h3 class="anchored" data-anchor-id="statistical-motivation">Statistical Motivation</h3>
<p><strong>Formulation.</strong></p>
<ul>
<li>Period <span class="math inline">\(P\)</span> ≈ rotation period of star</li>
<li>Variations related to evolving spots</li>
</ul>
<p><strong>Goal.</strong> Infer <span class="math inline">\(P\)</span> to understand stellar activity and improve exoplanet detection</p>
<p><strong>Modeling</strong>. We need a statistical model of functions that:</p>
<ul>
<li>Allows for nonlinearity.</li>
<li>Allows interpretation of quantities like <span class="math inline">\(P\)</span>.</li>
</ul>
<hr>
</section>
</section>
<section id="kernels" class="level2">
<h2 class="anchored" data-anchor-id="kernels">Kernels</h2>
<hr>
<section id="linear-regression-and-similarity" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression-and-similarity">Linear Regression and Similarity</h3>
<p><strong>Linear regression</strong> makes predictions using</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[y | \mathbf{x}] = \mathbf{x}^\top \boldsymbol{\beta}.
\end{align*}\]</span></p>
<p>Geometrically, predictions at nearby points should be similar.</p>
<p>What does “nearby” mean? In linear regression, <span class="math display">\[\begin{align*}
\mathbf{x}^\top \mathbf{x}'
\end{align*}\]</span> measures closeness (show the covariance calculation).</p>
<hr>
</section>
<section id="linear-regression-and-similarity-1" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression-and-similarity-1">Linear Regression and Similarity</h3>
<p><strong>Kernel answer:</strong> Define closeness explicitly.</p>
<p><span class="math display">\[\begin{align*}
k(\mathbf{x}, \mathbf{x}') = \text{how much } f\left(\mathbf{x}\right) \text{ and } f\left(\mathbf{x'}\right) \text{should covary}
\end{align*}\]</span></p>
<p>The function <span class="math inline">\(k\)</span> is called a “kernel”.</p>
<hr>
</section>
<section id="radial-basis-function-rbf-kernel" class="level3">
<h3 class="anchored" data-anchor-id="radial-basis-function-rbf-kernel">Radial Basis Function (RBF) Kernel</h3>
<p><span class="math display">\[k_{\text{RBF}}(x, x') = \sigma_f^2 \exp\left(-\frac{\|x - x'\|^2}{2\ell^2}\right)\]</span></p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>At distance <span class="math inline">\(|x - x'| = 0\)</span>: perfect correlation</li>
<li>At distance <span class="math inline">\(|x - x'| = \ell\)</span>: decays to <span class="math inline">\(\sigma_{f}e^{-\frac{1}{2}} \approx 0.6 \sigma_{f}\)</span></li>
<li>At distance <span class="math inline">\(|x - x'| = 3\ell\)</span>: nearly zero</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img src="figures/rbf_kernel_matrix.gif"></p>
</div>
</div>
<hr>
</section>
<section id="lengthscale-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="lengthscale-interpretation">Lengthscale Interpretation</h3>
<p><span class="math display">\[\text{Correlation} = \exp\left(-\frac{\tau^2}{2\ell^2}\right), \quad \tau = |x - x'|\]</span></p>
<p><strong>Small <span class="math inline">\(\ell\)</span> (relative to data range)</strong></p>
<ul>
<li>Allows rapid oscillation</li>
<li>Can fit data with high-frequency variation</li>
</ul>
<p><strong>Long lengthscale:</strong></p>
<ul>
<li>Enforces slower variation</li>
<li>Smooths over local fluctuations</li>
</ul>
<hr>
</section>
<section id="lengthscale-interpretation-1" class="level3">
<h3 class="anchored" data-anchor-id="lengthscale-interpretation-1">Lengthscale Interpretation</h3>
<p><span class="math display">\[\text{Correlation} = \exp\left(-\frac{\tau^2}{2\ell^2}\right), \quad \tau = |x - x'|\]</span></p>
<p><img src="figures/gp_length_scale_interpretation.gif" class="center" width="1000/"></p>
<hr>
</section>
<section id="periodic-kernel" class="level3">
<h3 class="anchored" data-anchor-id="periodic-kernel">Periodic Kernel</h3>
<p><span class="math display">\[k_{\text{per}}(x, x') = \sigma_f^2 \exp\left(-\frac{2\sin^2(\pi|x-x'|/p)}{\ell^2}\right)\]</span></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Parameters:</strong></p>
<ul>
<li><span class="math inline">\(p\)</span>: period</li>
<li><span class="math inline">\(\ell\)</span>: lengthscale (similar to RBF)</li>
<li><span class="math inline">\(\sigma_f^2\)</span>: signal variance</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img src="figures/periodic_kernel_matrix.gif" width="200/"></p>
</div>
</div>
<p>This will help us model stellar rotation.</p>
<hr>
</section>
<section id="matérn-kernels" class="level3">
<h3 class="anchored" data-anchor-id="matérn-kernels">Matérn Kernels</h3>
<p><span class="math display">\[k_{\text{Matérn-3/2}}(x, x') = \sigma_f^2\left(1 + \frac{\sqrt{3}\tau}{\ell}\right)\exp\left(-\frac{\sqrt{3}\tau}{\ell}\right)\]</span></p>
<p><span class="math display">\[k_{\text{Matérn-5/2}}(x, x') = \sigma_f^2\left(1 + \frac{\sqrt{5}\tau}{\ell} + \frac{5\tau^2}{3\ell^2}\right)\exp\left(-\frac{\sqrt{5}\tau}{\ell}\right)\]</span></p>
<p><strong>Unlike RBF:</strong> Finite differentiability</p>
<ul>
<li>Matérn-3/2: once differentiable</li>
<li>Matérn-5/2: twice differentiable</li>
</ul>
<p>(You don’t need to memorize these formulas!)</p>
<hr>
</section>
<section id="matérn-kernels-1" class="level3">
<h3 class="anchored" data-anchor-id="matérn-kernels-1">Matérn Kernels</h3>
<p>We haven’t yet explained how these samples are generated, but samples that use a Matérn kernel are often “rougher” are more realistic.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img src="figures/gp_samples.png"></p>
</div><div class="column" style="width:50%;">
<p><img src="figures/matern_samples.png"></p>
</div>
</div>
<hr>
</section>
<section id="combining-kernels" class="level3">
<h3 class="anchored" data-anchor-id="combining-kernels">Combining Kernels</h3>
<p>If <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> are valid kernels, so are</p>
<p><span class="math display">\[\begin{align*}
k_{\text{sum}}(x, x') = k_1(x, x') + k_2(x, x') (\text{   superposition})\\
k_{\text{prod}}(x, x') = k_1(x, x') \times k_2(x, x') (\text{   modulation})
\end{align*}\]</span></p>
<p>Addition is like two independent phenomena occuring at once.</p>
<ul>
<li><span class="math inline">\(k_{RBF} + k_{\text{periodic}}\)</span>: smooth trend + rotation</li>
<li>Like hearing a melody + rhythm</li>
</ul>
<hr>
</section>
<section id="combining-kernels-1" class="level3">
<h3 class="anchored" data-anchor-id="combining-kernels-1">Combining Kernels</h3>
<p>If <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> are valid kernels, so are</p>
<p><span class="math display">\[\begin{align*}
k_{\text{sum}}(x, x') = k_1(x, x') + k_2(x, x') (\text{   superposition})\\
k_{\text{prod}}(x, x') = k_1(x, x') \times k_2(x, x') (\text{   modulation})
\end{align*}\]</span></p>
<p>Multiplication is like where one pattern modulates another.</p>
<ul>
<li><span class="math inline">\(k_{RBF} \times k_{\text{periodic}}\)</span>: periodic signal with evolving magnitude</li>
<li>Like a volume knob controlling a repetitive signal</li>
</ul>
<hr>
</section>
<section id="combining-kernels-2" class="level3">
<h3 class="anchored" data-anchor-id="combining-kernels-2">Combining Kernels</h3>
<p>Since kernels are closed using these algebraic operations, they can be tailored to the specific problem of interest.</p>
<p><img src="figures/combining_kernels.png" class="center"></p>
<hr>
</section>
</section>
<section id="gaussian-process-regression" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-process-regression">Gaussian Process Regression</h2>
<hr>
<section id="gaussian-process-prior" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-process-prior">Gaussian Process Prior</h3>
<p><strong>Definition.</strong> A Gaussian Process (GP) is a collection of random variables, any finite subset of which is jointly Gaussian</p>
<p><span class="math display">\[f \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))\]</span></p>
<p>This can be used to define a prior distribution over functions. <span class="math display">\[\mathbf{f} = [f(x_1), \ldots, f(x_N)]^\top \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{K})\]</span></p>
<p>where <span class="math inline">\(\mu_i = m(x_i)\)</span> and <span class="math inline">\(K_{ij} = k(x_i, x_j)\)</span> WE often set <span class="math inline">\(m(\mathbf{x}) = 0\)</span>.</p>
<hr>
</section>
<section id="function-space-perspective" class="level3">
<h3 class="anchored" data-anchor-id="function-space-perspective">Function space perspective</h3>
<p><strong>Classical statistics</strong> Estimate fixed-dimensional parameters <span class="math inline">\(\theta\)</span> of a function.</p>
<p><strong>GPs</strong> Estimate the entire function <span class="math inline">\(f\)</span>.</p>
<p><strong>Observation</strong> This seems impossible, since functions are infinite-dimensional. But we only observe <span class="math inline">\(f\)</span> at finitely many points, <span class="math display">\[\begin{align*}
\mathbf{f}_{N} = \left(f\left(x_1\right), \dots, f(x_N)\right)
\end{align*}\]</span> and a GP can give us a prior for <em>any</em> choice of <span class="math inline">\(x_{1}, \dots, x_{N}\)</span>.</p>
<p>This is what it means to define a “distribution over functions.”</p>
<hr>
</section>
<section id="prior-covariance-structure" class="level3">
<h3 class="anchored" data-anchor-id="prior-covariance-structure">Prior Covariance Structure</h3>
<p><strong>Covariance matrix <span class="math inline">\(\mathbf{K}\)</span>:</strong></p>
<ul>
<li>Diagonal: <span class="math inline">\(K_{ii} = \sigma_f^2\)</span> is the prior variance at each point.</li>
<li>Off-diagonal: <span class="math inline">\(K_{ij}\)</span> is the covariance between <span class="math inline">\(f(x_i)\)</span> and <span class="math inline">\(f(x_j)\)</span></li>
</ul>
<p>Kernel encodes our assumptions about function smoothness, periodicity, trends, etc.</p>
<hr>
</section>
<section id="sampling-from-the-prior" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-the-prior">Sampling from the Prior</h3>
<p><img src="figures/gp_prior_trimmed.gif" class="center"></p>
<hr>
</section>
<section id="sampling-from-the-prior-1" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-the-prior-1">Sampling from the Prior</h3>
<ol type="1">
<li>Choose test locations <span class="math inline">\(\mathbf{x}_* = [x_1, \ldots, x_N]^\top\)</span></li>
<li>Compute <span class="math inline">\(\mathbf{K}_* = k(\mathbf{x}_*, \mathbf{x}_*)\)</span></li>
<li>Sample <span class="math inline">\(\mathbf{f}_* \sim \mathcal{N}(\mathbf{0}, \mathbf{K}_*)\)</span></li>
</ol>
<p>Each sample is one plausible function from our prior</p>
<p>Different kernels → different classes of functions</p>
<hr>
</section>
<section id="adding-observations" class="level3">
<h3 class="anchored" data-anchor-id="adding-observations">Adding Observations</h3>
<p>So far we have only described a prior over functions. How does that prior relate to the observed data <span class="math inline">\(y_{i}\)</span>?</p>
<p>We imagine <span class="math display">\[\begin{align*}
y_i = f(x_i) + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma_n^2)
\end{align*}\]</span> conditional on a draw <span class="math inline">\(f\)</span> from the GP prior.</p>
<p>This results in the joint distribution, <span class="math display">\[\begin{bmatrix} \mathbf{f}_* \\ \mathbf{y} \end{bmatrix} \sim \mathcal{N}\left(\mathbf{0}, \begin{bmatrix} \mathbf{K}_{**} &amp; \mathbf{K}_{*\text{train}} \\ \mathbf{K}_{\text{train}*} &amp; \mathbf{K}_{\text{train}} + \sigma_n^2\mathbf{I} \end{bmatrix}\right)\]</span></p>
<hr>
</section>
<section id="gaussian-process-posterior" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-process-posterior">Gaussian Process Posterior</h3>
<p>Now we condition on observed <span class="math inline">\(\mathbf{y}\)</span> to get a posterior for the actual <span class="math inline">\(f\)</span> that generated the data. The Gaussian conditioning formula gives</p>
<p><span class="math display">\[p(\mathbf{f}_* | \mathbf{y}, \mathbf{X}, \mathbf{x}_*) = \mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\Sigma}_*)\]</span> where</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol{\mu}_* &amp;= \mathbf{K}_{*\text{train}}(\mathbf{K}_{\text{train}} + \sigma_n^2\mathbf{I})^{-1}\mathbf{y}\\
\boldsymbol{\Sigma}_* &amp;= \mathbf{K}_{**} - \mathbf{K}_{*\text{train}}(\mathbf{K}_{\text{train}} + \sigma_n^2\mathbf{I})^{-1}\mathbf{K}_{\text{train}*}
\end{align*}\]</span></p>
<p>The next few slides interpret this formula.</p>
<hr>
</section>
<section id="posterior-mean" class="level3">
<h3 class="anchored" data-anchor-id="posterior-mean">Posterior Mean</h3>
<p><span class="math display">\[\begin{align*}
\boldsymbol{\mu}_*=\mathbf{K}_{* \text { train }}\left(\mathbf{K}_{\text {train }}+\sigma_n^2 \mathbf{I}\right)^{-1} \mathbf{y}
\end{align*}\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{K}_{\text{train}}\)</span> -&gt; how much the test point <span class="math inline">\(x_{\ast}\)</span> correlates with the training data</li>
<li><span class="math inline">\(\left(\mathbf{K}_{\text {train }}+\sigma_n^2 \mathbf{I}\right)^{-1} \mathbf{y}\)</span> -&gt; reweight the training data according to measurement precision</li>
</ul>
<hr>
</section>
<section id="posterior-mean-1" class="level3">
<h3 class="anchored" data-anchor-id="posterior-mean-1">Posterior Mean</h3>
<p><img src="figures/gp_posterior_mean.gif" class="center"></p>
<hr>
</section>
<section id="posterior-variance" class="level3">
<h3 class="anchored" data-anchor-id="posterior-variance">Posterior Variance</h3>
<p><span class="math display">\[\begin{align*}
\boldsymbol{\Sigma}_* = \mathbf{K}_{**} - \mathbf{K}_{*\text{train}}(\mathbf{K}_{\text{train}} + \sigma_n^2\mathbf{I})^{-1}\mathbf{K}_{\text{train}*}
\end{align*}\]</span></p>
<ul>
<li>We reduce uncertainty when we observe data</li>
<li>If <span class="math inline">\(x_{\ast}\)</span> is far from the data, then <span class="math inline">\(K_{\ast \text{train}}\)</span> is small and we revert to the prior <span class="math inline">\(\Sigma_{\ast} \to \mathbf{K}_{\ast\ast}\)</span>.</li>
</ul>
<hr>
</section>
<section id="posterior-variance-1" class="level3">
<h3 class="anchored" data-anchor-id="posterior-variance-1">Posterior Variance</h3>
<p><img src="figures/posterior_covariance.gif" class="center"></p>
<hr>
</section>
<section id="implementation-preview" class="level3">
<h3 class="anchored" data-anchor-id="implementation-preview">Implementation Preview</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Periodic × Decay kernel</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>k_periodic <span class="ot">&lt;-</span> <span class="fu">periodic</span>(<span class="at">period =</span> <span class="fu">exp</span>(logperiod),</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">lengthscale =</span> <span class="dv">1</span>,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">variance =</span> <span class="fu">exp</span>(logamp))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>k_decay <span class="ot">&lt;-</span> <span class="fu">mat52</span>(<span class="at">lengthscale =</span> <span class="fu">exp</span>(logdecay),</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">variance =</span> <span class="dv">1</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>full_kernel <span class="ot">&lt;-</span> k_periodic <span class="sc">*</span> k_decay <span class="sc">+</span> <span class="fu">white</span>(<span class="fu">exp</span>(logs2))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define GP</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>gp_model <span class="ot">&lt;-</span> <span class="fu">gp</span>(data<span class="sc">$</span>time, full_kernel)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="fu">distribution</span>(data<span class="sc">$</span>flux) <span class="ot">&lt;-</span> <span class="fu">normal</span>(gp_model <span class="sc">+</span> mean_flux, data<span class="sc">$</span>flux_err)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
</section>
<section id="exercise-1" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1">Exercise</h3>
<p>[Posterior and prior predictive distributions].</p>
<ol type="1">
<li><p>Fix a point <span class="math inline">\(x_{\ast}\)</span> of interest. Before we observe any data <span class="math inline">\(y_{1}, \dots,
y_{n}\)</span>, what would your best guess of <span class="math inline">\(y_{\ast}\)</span> be? How much uncertainty would you have? How does this differ from your best guess for <span class="math inline">\(f_{\ast}\)</span>?</p></li>
<li><p>Suppose you have now observed data <span class="math inline">\(y_{1}, \dots, y_{n}\)</span> at <span class="math inline">\(x_{1}, \dots,
x_{n}\)</span>. We again ask you for a guess of <span class="math inline">\(y_{\ast}\)</span> at <span class="math inline">\(x_{\ast}\)</span>. How has it changed?</p></li>
<li><p>[Optional]. The quantity in the previous part has a Gaussian distribution. Can you derive formulas for <span class="math inline">\(\mu_{*}\)</span> and <span class="math inline">\(\sigma_{*}^2\)</span>?</p></li>
</ol>
<p><span class="math display">\[p(y_* | \mathbf{y}, \mathbf{X}, x_*) = \mathcal{N}(\mu_*, \sigma_*^2 + \sigma_n^2)\]</span></p>
<hr>
</section>
</section>
</section>
<section id="using-nonlinear-models" class="level1">
<h1>6. Using Nonlinear Models</h1>
<ul>
<li>Hyperparameter tuning</li>
<li>Application to case study</li>
<li>Posterior predictives</li>
<li>Interpretability evaluation</li>
</ul>
<div class="cell">
<div class="cell-output cell-output-stderr">
<pre><code>Ignoring entry 'angus2017'  (line100) because:
    A bibentry of bibtype 'Article' has to specify the field: c("journaltitle", "journal")</code></pre>
</div>
</div>
<hr>
<section id="learning-outcomes-4" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-4">Learning outcomes</h3>
<ol type="1">
<li>Describe the theoretical foundation of intrinsically interpretable models like sparse regression, gaussian processes, and classification and regression trees, and apply them to realistic case studies with appropriate validation checks.</li>
</ol>
<hr>
</section>
<section id="reading-4" class="level2">
<h2 class="anchored" data-anchor-id="reading-4">Reading</h2>
<ul>
<li><p>Görtler, J., Kehlbeck, R., &amp; Deussen, O. (2019). A visual exploration of Gaussian processes. Distill, 4(4). doi:10.23915/distill.00017</p></li>
<li><p>Deisenroth, M., Luo, Y., &amp; van der Wilk, M. (2020). A practical guide to Gaussian processes. https://infallible-thompson-49de36.netlify.app/.</p></li>
</ul>
<hr>
</section>
<section id="hyperparameter-optimization" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-optimization">Hyperparameter Optimization</h2>
<hr>
<section id="marginal-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="marginal-likelihood">Marginal Likelihood</h3>
<p>Q: How do we choose hyperparameters <span class="math inline">\(\theta = \left(\ell, \sigma_f, \dots, \right)\)</span></p>
<p>A: Find <span class="math inline">\(\theta\)</span> that makes the observed data <span class="math inline">\(\mathbf{y}\)</span> most probably:</p>
<p><strong>Marginal likelihood</strong></p>
<p><span class="math display">\[p(\mathbf{y} | \mathbf{X}, \theta) = \int p(\mathbf{y} | \mathbf{f}, \mathbf{X}) p(\mathbf{f} | \mathbf{X}, \theta) d\mathbf{f}\]</span></p>
<p>This “marginalizes” over all possible functions <span class="math inline">\(\mathbf{f}\)</span>.</p>
<hr>
</section>
<section id="marginal-likelihood-1" class="level3">
<h3 class="anchored" data-anchor-id="marginal-likelihood-1">Marginal Likelihood</h3>
<p>For the GP model, this has a closed form:</p>
<p><span class="math display">\[\log p(\mathbf{y} | \mathbf{X}, \theta) = -\frac{1}{2}\mathbf{y}^\top(\mathbf{K} + \sigma_n^2\mathbf{I})^{-1}\mathbf{y} - \frac{1}{2}\log|\mathbf{K} + \sigma_n^2\mathbf{I}| + \text{const}\]</span></p>
<p>This has two competing terms – data fit vs.&nbsp;model complexity.</p>
<hr>
</section>
<section id="automatic-occams-razor" class="level3">
<h3 class="anchored" data-anchor-id="automatic-occams-razor">Automatic Occam’s Razor</h3>
<p>Consider fitting a sine wave with noise. How would you choose the RBF <span class="math inline">\(\ell\)</span>?</p>
<p>Very small <span class="math inline">\(\ell\)</span> (underfitting)</p>
<ul>
<li>Will interpolate the noise, not just the cuve</li>
<li>Spreads probability over too many datasets</li>
<li>Low marginal likelihood</li>
</ul>
<p>Very large <span class="math inline">\(\ell\)</span> (overfitting)</p>
<ul>
<li>Will oversmooth the sine component</li>
<li>Low marginal likelihood</li>
</ul>
<p>The right choice of <span class="math inline">\(\ell\)</span> will be flexible enough to fit the sine curve but not the noise. The marginal likelihood finds this automatically without any cross-validation.</p>
<hr>
</section>
<section id="marginal-likelihood-surface" class="level3">
<h3 class="anchored" data-anchor-id="marginal-likelihood-surface">Marginal Likelihood Surface</h3>
<p><img src="figures/marginal_likelihood_surface.gif"></p>
<ul>
<li>Non-convex: multiple local optima</li>
<li>Each optimum corresponds to different explanation of data</li>
<li>Gradient-based optimization sensitive to initialization</li>
</ul>
<hr>
</section>
<section id="initializing-signal-variance" class="level3">
<h3 class="anchored" data-anchor-id="initializing-signal-variance">Initializing Signal Variance</h3>
<p>Observation = Signal + Noise</p>
<p><span class="math display">\[\begin{align*}
y = f(x) + \epsilon
\end{align*}\]</span></p>
<p><strong>Variance Decomposition</strong></p>
<p><span class="math display">\[\begin{align*}
\underbrace{\operatorname{Var}[y]}_{\text {what we see }}=\underbrace{\sigma_f^2}_{\text {signal }}+\underbrace{\sigma_n^2}_{\text {noise }}
\end{align*}\]</span></p>
<hr>
</section>
<section id="initializing-signal-variance-1" class="level3">
<h3 class="anchored" data-anchor-id="initializing-signal-variance-1">Initializing Signal Variance</h3>
<p><strong>Heuristic 1:</strong> If the instrument’s precision is known (e.g., Kepler telescope noise <span class="math inline">\(\sigma_n \approx 10^-4\)</span>), then set <span class="math display">\[\begin{align*}
\sigma_f^2=\operatorname{Var}[y]-\sigma_n^2
\end{align*}\]</span></p>
<p><strong>Heuristic 2:</strong> Suppose the signal dominates by some assumed SNR factor <span class="math inline">\(\kappa\)</span> (e.g., 2 - 100). Then set <span class="math display">\[\begin{align*}
\sigma_{f}^{2} \approx  \operatorname{Var}[y]\\
\sigma_{n}^{2} \approx \frac{\sigma_f^2}{\kappa^2}
\end{align*}\]</span></p>
<hr>
</section>
<section id="initializing-lengthscale" class="level3">
<h3 class="anchored" data-anchor-id="initializing-lengthscale">Initializing Lengthscale</h3>
<p>Lengthscale controls how far we travel before <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f(x')\)</span> decorrelate</p>
<p><strong>Heuristic:</strong> Set lengthscale relative to spread of input data <span class="math display">\[\ell \approx \lambda \cdot \text{SD}[\mathbf{X}], \quad \lambda \in [0.2, 10]\]</span></p>
<p><strong>Alternative:</strong> Use median distance from mean <span class="math display">\[\ell \approx \text{median}\{|x_i - \bar{x}|\}\]</span></p>
<hr>
</section>
</section>
<section id="case-study-steller-variability" class="level2">
<h2 class="anchored" data-anchor-id="case-study-steller-variability">Case Study: Steller Variability</h2>
<hr>
<section id="data-characteristics" class="level3">
<h3 class="anchored" data-anchor-id="data-characteristics">Data Characteristics</h3>
<div class="columns">
<div class="column" style="width:50%;">
<div class="sourceCode" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">dim</span>(data)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>] <span class="dv">3822</span>    <span class="dv">3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column" style="width:50%;">
<p><img src="figures/stellar_variability.png"></p>
</div>
</div>
<hr>
</section>
<section id="periodogram" class="level3">
<h3 class="anchored" data-anchor-id="periodogram">Periodogram</h3>
<div class="columns">
<div class="column" style="width:50%;">
<div class="sourceCode" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>spec <span class="ot">&lt;-</span> <span class="fu">spectrum</span>(data<span class="sc">$</span>flux, <span class="at">pad =</span> <span class="dv">6</span>, <span class="at">plot =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column" style="width:50%;">
<p><img src="figures/stellar_periodogram.png"></p>
</div>
</div>
<p>There seem to be two periodicities that dominate.</p>
<hr>
</section>
<section id="rotation-kernel-for-stellar-variability" class="level3">
<h3 class="anchored" data-anchor-id="rotation-kernel-for-stellar-variability">Rotation Kernel for Stellar Variability</h3>
<p><span class="math display">\[k_{\text{rot}} = k_{\text{periodic}} \times k_{\text{decay}}\]</span></p>
<p><strong>Physical interpretation:</strong></p>
<ul>
<li>Periodic component: rotation</li>
<li>Decay component: timescale for spot changes</li>
<li>Product: spots appear/disappear as star rotates</li>
</ul>
<p>The original source (, ) used a slightly more involved periodoc kernel. It used a similar domain-informed kernel construction though.</p>
<hr>
</section>
<section id="kernel-specification" class="level3">
<h3 class="anchored" data-anchor-id="kernel-specification">Kernel Specification</h3>
<p>This is how we can implement the kernel in R.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>k_periodic <span class="ot">&lt;-</span> <span class="fu">periodic</span>(<span class="at">period =</span> <span class="fu">exp</span>(logperiod),</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">lengthscale =</span> <span class="dv">1</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">variance =</span> <span class="fu">exp</span>(logamp))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>k_decay <span class="ot">&lt;-</span> <span class="fu">mat52</span>(<span class="at">lengthscale =</span> <span class="fu">exp</span>(logdecay),</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">variance =</span> <span class="dv">1</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>full_kernel <span class="ot">&lt;-</span> k_periodic <span class="sc">*</span> k_decay <span class="sc">+</span> <span class="fu">white</span>(<span class="fu">exp</span>(logs2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Hyperparameters</p>
<ul>
<li><code>logperiod</code>: <span class="math inline">\(\log p\)</span> (rotation period)</li>
<li><code>logdecay</code>: <span class="math inline">\(\log \ell_{\text{decay}}\)</span> (spot lifetime)</li>
<li><code>logamp</code>: <span class="math inline">\(\log \sigma_f^2\)</span> (signal variance)</li>
<li><code>logs2</code>: <span class="math inline">\(\log \sigma_n^2\)</span> (noise variance)</li>
</ul>
<hr>
</section>
<section id="prior-specification" class="level3">
<h3 class="anchored" data-anchor-id="prior-specification">Prior Specification</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>mean_flux <span class="ot">&lt;-</span> <span class="fu">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>logs2 <span class="ot">&lt;-</span> <span class="fu">normal</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>logamp <span class="ot">&lt;-</span> <span class="fu">normal</span>(<span class="fu">log</span>(<span class="fu">var</span>(data<span class="sc">$</span>flux)), <span class="dv">2</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>logperiod <span class="ot">&lt;-</span> <span class="fu">normal</span>(<span class="fu">log</span>(<span class="dv">25</span>), <span class="dv">2</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>logdecay <span class="ot">&lt;-</span> <span class="fu">normal</span>(<span class="fu">log</span>(<span class="dv">25</span> <span class="sc">*</span> <span class="dv">3</span>), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Rationale</p>
<ul>
<li><code>logperiod ~ N(log(25), 2)</code>: Centered at periodogram guess, but wide</li>
<li><code>logdecay ~ N(log(75), 2)</code>: Lasts ~3 rotation periods</li>
<li><code>logamp ~ N(log(Var[y]), 2)</code>: Signal variance ~ data variance</li>
<li><code>logs2 ~ N(-1, 2)</code>: Relatively small noise</li>
</ul>
<hr>
</section>
<section id="maximum-a-posteriori-map-estimate" class="level3">
<h3 class="anchored" data-anchor-id="maximum-a-posteriori-map-estimate">Maximum A Posteriori (MAP) Estimate</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>gp_model <span class="ot">&lt;-</span> <span class="fu">gp</span>(data<span class="sc">$</span>time, full_kernel)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">distribution</span>(data<span class="sc">$</span>flux) <span class="ot">&lt;-</span> <span class="fu">normal</span>(gp_model <span class="sc">+</span> mean_flux,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                                   data<span class="sc">$</span>flux_err)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">model</span>(logperiod, logdecay, logamp, logs2, mean_flux)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>map_soln <span class="ot">&lt;-</span> <span class="fu">opt</span>(m, <span class="at">initial_values =</span> start_vals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This helps us find a mode of the posterior as starting point for MCMC.</p>
<hr>
</section>
<section id="posterior-distribution-rotation-period" class="level3">
<h3 class="anchored" data-anchor-id="posterior-distribution-rotation-period">Posterior Distribution: Rotation Period</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>period_samples <span class="ot">&lt;-</span> <span class="fu">exp</span>(draws[, <span class="st">"logperiod"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="figures/stellar_posterior_rotation.png" class="center"></p>
<hr>
</section>
<section id="posterior-predictive-samples" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-samples">Posterior Predictive Samples</h3>
<p>Each draw here is a plausible function <span class="math inline">\(f(t)\)</span> consistent with data and kernel</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>time_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(data<span class="sc">$</span>time), <span class="fu">max</span>(data<span class="sc">$</span>time), <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>gp_array <span class="ot">&lt;-</span> <span class="fu">project</span>(gp_model, time_grid)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>gp_samples <span class="ot">&lt;-</span> <span class="fu">calculate</span>(gp_array, <span class="at">values =</span> draws)[[<span class="dv">1</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>project</code> evaluates the GP posterior distribution at the specified <code>time_grid</code>.</p>
<hr>
</section>
<section id="posterior-predictive-visualization" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-visualization">Posterior Predictive Visualization</h3>
<p><strong>Observations</strong></p>
<ul>
<li>Periodic structure well-captured</li>
<li>Uncertainty narrow near data (compare with later excercise)</li>
</ul>
<p><img src="figures/stellar_gp_posterior.png" class="center"></p>
<hr>
</section>
<section id="case-study-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="case-study-takeaways">Case Study Takeaways</h3>
<p>The kernel is a hypothesis about the data generating process.</p>
<ul>
<li>Quasiperiodicity -&gt; rotating star with evolving spots</li>
<li>Not an arbitrary computational choice</li>
</ul>
<p>What we learned</p>
<ul>
<li>Rotation period</li>
<li>Spot evolution</li>
<li>Subtract fitted stellar variability -&gt; potential exoplanet detection</li>
</ul>
<p>The GP is not just fitting curves, it’s extracting astrophysics.</p>
<hr>
</section>
</section>
<section id="numerical-stability" class="level2">
<h2 class="anchored" data-anchor-id="numerical-stability">Numerical Stability</h2>
<hr>
<section id="condition-number" class="level3">
<h3 class="anchored" data-anchor-id="condition-number">Condition Number</h3>
<p>Problem: Computing <span class="math inline">\(\left(\mathbf{K} + \sigma_n^2 I\right)^{-1}\)</span> on a computer.</p>
<p>The condition number measures is the ratio of the largest and smallest eigenvalues of this matrix:</p>
<p><span class="math display">\[\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}\]</span></p>
<p><strong>Problem:</strong> Large <span class="math inline">\(\kappa\)</span> → numerical instability in matrix inversion</p>
<hr>
</section>
<section id="sources-of-ill-conditioning" class="level3">
<h3 class="anchored" data-anchor-id="sources-of-ill-conditioning">Sources of Ill-Conditioning</h3>
<p>For GPs, we can bound <span class="math inline">\(\kappa \leq N\alpha^2 + 1\)</span> where <span class="math inline">\(\alpha = \frac{\sigma_f}{\sigma_n}\)</span>.</p>
<p>Paradoxically, high-quality data and larger sample sizes create numerical problems!</p>
<hr>
</section>
<section id="sources-of-ill-conditioning-1" class="level3">
<h3 class="anchored" data-anchor-id="sources-of-ill-conditioning-1">Sources of Ill-Conditioning</h3>
<p>When does <span class="math inline">\(\left(K + \sigma_n^2\right)\)</span> become nearly singular?</p>
<p>Scenario 1: Nearly identical rows</p>
<ul>
<li>Happens when <span class="math inline">\(x_i \approx x_j\)</span> (points too close).</li>
<li>Also when <span class="math inline">\(\ell \gg\)</span> data range (everythign correlated).</li>
</ul>
<p>Scenarios 2: Noise-free limit</p>
<ul>
<li>When <span class="math inline">\(\sigma_{n} \downarrow 0\)</span>, we are asking for interpolation</li>
<li>The term <span class="math inline">\(\sigma_{n}^{2}\)</span> regularizes</li>
</ul>
<hr>
</section>
<section id="sources-of-ill-conditioning-2" class="level3">
<h3 class="anchored" data-anchor-id="sources-of-ill-conditioning-2">Sources of Ill-Conditioning</h3>
<p><img src="figures/gp_condition_number.gif" class="center"></p>
<hr>
</section>
<section id="jitter-heuristic" class="level3">
<h3 class="anchored" data-anchor-id="jitter-heuristic">Jitter Heuristic</h3>
<p><strong>Heuristic.</strong> Add small “jitter” to the diagonal</p>
<p><span class="math display">\[\begin{align*}
\mathbf{K} + \sigma_n^2\mathbf{I} \to \mathbf{K} + (\sigma_n^2 + \epsilon^2)\mathbf{I}
\end{align*}\]</span></p>
<ul>
<li>Increases <span class="math inline">\(\lambda_{\text{min}}\)</span></li>
<li>Caps maximum achievable SNR at <span class="math inline">\(\frac{\sigma_f}{\epsilon}\)</span></li>
<li>Prevents numerical catastrophe.</li>
</ul>
<p>Typically <span class="math inline">\(\epsilon ~ 10^{-6} - 10^{-4}\)</span>.</p>
<hr>
</section>
<section id="data-normalization" class="level3">
<h3 class="anchored" data-anchor-id="data-normalization">Data Normalization</h3>
<p><strong>Input standardization</strong> <span class="math display">\[\tilde{x} = \frac{x - \bar{x}}{\text{SD}[x]}\]</span></p>
<p>Also have to rescale <span class="math inline">\(\ell \to \ell / \text{SD}[x]\)</span></p>
<p><strong>Output centering</strong> <span class="math display">\[\tilde{y} = y - \bar{y}\]</span></p>
<ul>
<li>Moves data away from “meaningful zeros”</li>
<li>Makes default priors (e.g., <span class="math inline">\(m = 0\)</span>) more appropriate</li>
</ul>
<hr>
</section>
<section id="kepler-analysis" class="level3">
<h3 class="anchored" data-anchor-id="kepler-analysis">Kepler Analysis</h3>
<p>We used this in the stellar variability study.</p>
<ol type="1">
<li>Remove median flux (centering)</li>
<li>Convert to parts per thousand (scaling)</li>
<li>Thin data by factor of 10 (reduce <span class="math inline">\(N\)</span>)</li>
</ol>
<p>See <code>03-prep.ipynb</code>.</p>
<hr>
</section>
</section>
<section id="interpretability" class="level2">
<h2 class="anchored" data-anchor-id="interpretability">Interpretability</h2>
<hr>
<section id="interpretability-criteria" class="level3">
<h3 class="anchored" data-anchor-id="interpretability-criteria">Interpretability Criteria</h3>
<ul>
<li>Predictive accuracy: Can we approximate complex functions?</li>
<li>Parameter meaning: Do parameters correspond to scientific quantities?</li>
<li>Uncertainty: Does model know what it doesn’t know?</li>
<li>Visualization: Can you draw what the model believes?</li>
<li>Falsifiability: Can you identify when the model is wrong?</li>
</ul>
<p>GPs do well on all five… deep learning fails 2 - 5.</p>
<hr>
</section>
<section id="comparison-with-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="comparison-with-linear-regression">Comparison with Linear Regression</h3>
<p><strong>Linear regression</strong></p>
<ul>
<li>Parametric: <span class="math inline">\(y = \beta_0 + x^\top \beta\)</span>.</li>
<li>Interpretable: “<span class="math inline">\(\beta_{1}\)</span> is the effect of <span class="math inline">\(x_{1}\)</span> all else held equal”</li>
<li>Global: same <span class="math inline">\(\beta\)</span> everywhere</li>
<li>Limited: cannot capture nonlinearity without feature engineering</li>
</ul>
<p><strong>Gaussian processes</strong></p>
<ul>
<li>Nonparametric: no fixed functional form</li>
<li>Interpretable Hyperparameters: <span class="math inline">\(\ell\)</span> (scale), <span class="math inline">\(p\)</span> (period), …</li>
<li>Local: predictions weighted by kernel</li>
<li>Flexible: captures complex patterns</li>
</ul>
<hr>
</section>
<section id="nonparametric-but-interpretable" class="level3">
<h3 class="anchored" data-anchor-id="nonparametric-but-interpretable">Nonparametric but Interpretable</h3>
<ul>
<li>Nonparametric -&gt; complexity grows as the number of samples increases</li>
<li>Not “effect sizes” but “characteristic scales”</li>
<li>“How quickly does <span class="math inline">\(f\)</span> vary?” not “How much does <span class="math inline">\(x\)</span> affect <span class="math inline">\(y\)</span>?”</li>
</ul>
<p>Both GPs and linear regression are interpertable, but in different senses.</p>
<hr>
</section>
<section id="limitations-and-extensions" class="level3">
<h3 class="anchored" data-anchor-id="limitations-and-extensions">Limitations and Extensions</h3>
<p><strong>Limitations:</strong></p>
<ul>
<li>1D time series only</li>
<li>Observed data are Gaussian</li>
<li>Stationary/periodic assumptions</li>
</ul>
<p><strong>Extensions:</strong></p>
<ul>
<li>Non-Gaussian observations: Maybe we want a Student-t likelihood for robustness or softmax layer for classification.</li>
<li>Multidimensional inputs or outputs: We might have other relvant measurements for each star, and may want to model several stars at once.</li>
</ul>
<hr>
</section>
<section id="exercise-2" class="level3">
<h3 class="anchored" data-anchor-id="exercise-2">Exercise</h3>
<p>[GP Missing Data] Rerun the case study notebook with a window of observations missing. How does it affect the posterior on the period? What do the posterior predictive samples look like?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>data_model <span class="ot">&lt;-</span> data <span class="sc">|&gt;</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(time) <span class="sc">|&gt;</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(time <span class="sc">&lt;</span> <span class="dv">1210</span> <span class="sc">|</span> time <span class="sc">&gt;</span> <span class="dv">1220</span>) <span class="sc">|&gt;</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">slice</span>(<span class="fu">seq</span>(<span class="dv">1</span>, <span class="fu">n</span>(), <span class="at">by =</span> <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
</section>
</section>
</section>
<section id="classification-and-regression-trees" class="level1">
<h1>7. Classification and Regression Trees</h1>
<ul>
<li><p>motivating case study</p></li>
<li><p>trees and partitions</p></li>
<li><p>growing and pruning</p></li>
<li><p>tuning and stability</p></li>
</ul>
<hr>
<section id="learning-outcomes-5" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-5">Learning outcomes</h3>
<ol type="1">
<li>Describe the theoretical foundation of intrinsically interpretable models like sparse regression, gaussian processes, and classification and regression trees, and apply them to realistic case studies with appropriate validation checks.</li>
<li>Compare the competing definitions of interpretable machine learning, the motivations behind them, and metrics that can be used to quantify whether they have been met.</li>
</ol>
<hr>
</section>
<section id="reading-5" class="level2">
<h2 class="anchored" data-anchor-id="reading-5">Reading</h2>
<ul>
<li><strong>Section 8.1</strong>. James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). An introduction to statistical learning (2nd edn). doi:10.1007/978-1-0716-1418-1
<ul>
<li>PDF available through library: https://search.library.wisc.edu/catalog/9913885290202121</li>
</ul></li>
<li><strong>Sections 3 and 4.1</strong>. Agarwal, A., Kenney, A. M., Tan, Y. S., Tang, T. M., &amp; Yu, B. (2023). Integrating random forests and generalized linear models for improved accuracy and interpretability. doi:10.48550/ARXIV.2307.01932
<ul>
<li>Will not appear on exam</li>
</ul></li>
</ul>
<hr>
<section id="voter-turnout" class="level3">
<h3 class="anchored" data-anchor-id="voter-turnout">Voter turnout</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Campaigns spend significant effort trying to get their voter base to turn out during elections</li>
<li>Researchers run randomized trials to see which interventions work</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img src="figures/Vote_button-1024x1005.png"></p>
</div>
</div>
<hr>
</section>
<section id="study-design-1" class="level3">
<h3 class="anchored" data-anchor-id="study-design-1">Study design</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>The study (GERBER, GREEN, and LARIMER, 2008) were interested in how social pressure influences voter turnout. They defined four types of mailers and sent them randomly to potential voters in the Michigan 2006 general election.</p>
</div><div class="column" style="width:50%;">
<p><img src="figures/mailer-1.png" center="true"></p>
</div>
</div>
<hr>
<div class="columns">
<div class="column" style="width:50%;">
<p><img src="figures/mailer-2.png" center="true"></p>
</div><div class="column" style="width:50%;">
<p><img src="figures/mailer-3.png" center="true"></p>
</div>
</div>
<hr>
<p><img src="figures/mailer-4.png" center="true"></p>
<hr>
</section>
<section id="treatment-effect-heterogeneity" class="level3">
<h3 class="anchored" data-anchor-id="treatment-effect-heterogeneity">Treatment Effect Heterogeneity</h3>
<ul>
<li><p>On average, those who received the neighbor intervention were more likely to vote.</p></li>
<li><p>But are some groups more strongly influenced than others? The statistical term for this is “treatment effect heterogeneity.”</p></li>
</ul>
<hr>
</section>
<section id="treatment-effect-heterogeneity-1" class="level3">
<h3 class="anchored" data-anchor-id="treatment-effect-heterogeneity-1">Treatment Effect Heterogeneity</h3>
<p>Intuitively, there are some people who are already so jaded (or motivated) that the intervention is unlikely to change their behavior.</p>
<hr>
</section>
<section id="statistical-motivation-1" class="level3">
<h3 class="anchored" data-anchor-id="statistical-motivation-1">Statistical Motivation</h3>
<p><strong>Formulation.</strong></p>
<ul>
<li>Background variables <span class="math inline">\(x \in \mathcal{X}\)</span> (age, previous voting record, …)</li>
<li>Treatment assignment <span class="math inline">\(w \in \{0, 1\}\)</span> (mailer vs.&nbsp;control)</li>
<li>Voting probability <span class="math inline">\(\mu\left(x, w\right)\)</span></li>
</ul>
<p><strong>Goal.</strong> Find subsets of <span class="math inline">\(\mathcal{X}\)</span> where <span class="math inline">\(\tau\left(x\right)\)</span> is large: <span class="math display">\[\begin{align*}
\tau\left(x\right) := \mu\left(x, 1\right) - \mu\left(x, 0\right)
\end{align*}\]</span></p>
<p><strong>Modeling</strong>. We need a statistical model that partitions <span class="math inline">\(\mathcal{X}\)</span> into interpretable subgroups where <span class="math inline">\(\tau\left(x\right)\)</span> is large.</p>
<hr>
</section>
</section>
<section id="cart" class="level2">
<h2 class="anchored" data-anchor-id="cart">CART</h2>
<hr>
<section id="the-partition-structure" class="level3">
<h3 class="anchored" data-anchor-id="the-partition-structure">The Partition Structure</h3>
<p>A tree is a partition <span class="math inline">\(\mathcal{X} = \cup_{j=1}^J R_j\)</span> where each region <span class="math inline">\(R_j\)</span> carries constant prediction <span class="math inline">\(\hat{y}_j\)</span>.</p>
<p><strong>Geometry</strong>: Axis-aligned recursive bisection. Each split: <span class="math inline">\(\{x: X_j &lt; s\} \cup \{x: X_j \geq s\}\)</span>.</p>
<p><strong>Prediction in <span class="math inline">\(R_j\)</span></strong>:</p>
<ul>
<li>Regression: <span class="math inline">\(\hat{y}_j = \frac{1}{n_j}\sum_{i \in R_j} y_i\)</span></li>
<li>Classification: <span class="math inline">\(\hat{y}_j = \arg\max_k \hat{p}_{jk}\)</span> where <span class="math inline">\(\hat{p}_{jk} = \frac{1}{n_j}\sum_{i \in R_j} \mathbb{1}(y_i = k)\)</span></li>
</ul>
<hr>
</section>
<section id="loss-functions" class="level3">
<h3 class="anchored" data-anchor-id="loss-functions">Loss Functions</h3>
<p><strong>Regression</strong>: Residual sum of squares <span class="math display">\[\text{RSS}(T) = \sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_j)^2\]</span></p>
<p><strong>Classification</strong>: Impurity measures (entropy-based)</p>
<ul>
<li>Gini: <span class="math inline">\(G = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk})\)</span></li>
<li>Cross-entropy: <span class="math inline">\(D = -\sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}\)</span></li>
</ul>
<p>These measure class mixing. Small values indicate node purity—observations from predominantly a single class.</p>
<hr>
</section>
<section id="growing-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="growing-algorithm">Growing Algorithm</h3>
<p>Greedy algorithm: At each step, select predictor <span class="math inline">\(X_j\)</span> and cutpoint <span class="math inline">\(s\)</span> to minimize RSS (regression) or impurity (classification).</p>
<p>Define half-planes: <span class="math display">\[R_1(j,s) = \{X|X_j &lt; s\}, \quad R_2(j,s) = \{X|X_j \geq s\}\]</span></p>
<p>Choose <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> minimizing: <span class="math display">\[\sum_{i: x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 + \sum_{i: x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2\]</span></p>
<hr>
</section>
<section id="growing-algorithm-1" class="level3">
<h3 class="anchored" data-anchor-id="growing-algorithm-1">Growing Algorithm</h3>
<p>Repeat: split one of the existing regions until stopping criterion (minimum node size) reached.</p>
<p>Split gain: <span class="math display">\[\Delta I = I_{\text{parent}} - \left(\frac{n_L}{n} I_L + \frac{n_R}{n} I_R\right)\]</span></p>
<hr>
</section>
<section id="data-types" class="level3">
<h3 class="anchored" data-anchor-id="data-types">Data Types</h3>
<p><strong>Numerical</strong>: Split at <span class="math inline">\(X_j &lt; s\)</span>. Search over observed values.</p>
<p><strong>Categorical</strong>: For <span class="math inline">\(K\)</span> levels, partition into two subsets. There are <span class="math inline">\(2^{K-1}-1\)</span> possible binary splits.</p>
<p><strong>Missing values</strong>: Use surrogate splits—alternate predictors producing similar partitions.</p>
<hr>
</section>
</section>
<section id="tuning" class="level2">
<h2 class="anchored" data-anchor-id="tuning">Tuning</h2>
<hr>
<section id="hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters">Hyperparameters</h3>
<ul>
<li><strong>Complexity parameter</strong> <span class="math inline">\(\alpha\)</span> (or <span class="math inline">\(cp\)</span>): Penalty per leaf</li>
<li><strong>Minimum split size</strong>: Observations required to attempt split</li>
<li><strong>Maximum depth</strong>: Limit on tree height</li>
</ul>
<p>Use <span class="math inline">\(K\)</span>-fold cross-validation to select optimal <span class="math inline">\(\alpha\)</span>.</p>
<hr>
</section>
<section id="pruning" class="level3">
<h3 class="anchored" data-anchor-id="pruning">Pruning</h3>
<p>Large trees overfit. For a given <span class="math inline">\(\alpha \geq 0\)</span>, find a subtree <span class="math inline">\(T \subset T_0\)</span> minimizing: <span class="math display">\[\sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|\]</span> where <span class="math inline">\(|T|\)</span> counts terminal nodes.</p>
<p><span class="math inline">\(\alpha\)</span> penalizes complexity. When <span class="math inline">\(\alpha = 0\)</span>, optimal subtree is <span class="math inline">\(T_0\)</span>. As <span class="math inline">\(\alpha\)</span> increases, optimal subtree shrinks.</p>
<hr>
</section>
<section id="pruning-1" class="level3">
<h3 class="anchored" data-anchor-id="pruning-1">Pruning</h3>
<ul>
<li><p><strong>Weakest link pruning.</strong> As <span class="math inline">\(\alpha\)</span> increases from zero, branches are removed in nested fashion. Computing the entire sequence of optimal subtrees is efficient.</p></li>
<li><p>Select <span class="math inline">\(\alpha\)</span> by cross-validation. Standard lasso-type tradeoff.</p></li>
</ul>
<hr>
</section>
<section id="algorithm-summary" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-summary">Algorithm Summary</h3>
<ul>
<li>Grow large tree <span class="math inline">\(T_0\)</span> via recursive binary splitting (stop at minimum node size)</li>
<li>Apply cost complexity pruning to obtain sequence of subtrees as function of <span class="math inline">\(\alpha\)</span></li>
<li>Use <span class="math inline">\(K\)</span>-fold cross-validation: for each fold, repeat steps 1-2, evaluate MSE as function of <span class="math inline">\(\alpha\)</span></li>
<li>Return subtree from step 2 corresponding to the <span class="math inline">\(\alpha\)</span> that minimizes CV error</li>
</ul>
<hr>
</section>
<section id="geometric-limitations" class="level3">
<h3 class="anchored" data-anchor-id="geometric-limitations">Geometric Limitations</h3>
<p><strong>Axis-alignment</strong>: Splits perpendicular to coordinate axes. Capturing diagonal boundaries requires many nodes – have to approximat a smooth functions with a “staircase.”</p>
<p><strong>Instability</strong>: Small data perturbations can change the root split, hence the entire tree structure. High variance estimator.</p>
<p><strong>Correlated predictors</strong>: Among <span class="math inline">\(X_i \approx X_j\)</span>, choice is arbitrary. Importance measures become meaningless.</p>
<hr>
</section>
<section id="comparison" class="level3">
<h3 class="anchored" data-anchor-id="comparison">Comparison</h3>
<p>Piecewise constant on axis-aligned rectangles vs.&nbsp;global plane.</p>
<p><strong>Tree model</strong>: <span class="math display">\[f(X) = \sum_{m=1}^M c_m \cdot \mathbb{1}_{(X \in R_m)}\]</span></p>
<p><strong>Linear model</strong>: <span class="math display">\[f(X) = \beta_0 + \sum_{j=1}^p X_j \beta_j\]</span></p>
<hr>
</section>
<section id="what-trees-represent" class="level3">
<h3 class="anchored" data-anchor-id="what-trees-represent">What Trees Represent</h3>
<p><img src="figures/cart_v_lr.png" class="center"></p>
<hr>
</section>
<section id="takeaways-1" class="level3">
<h3 class="anchored" data-anchor-id="takeaways-1">Takeaways</h3>
<ul>
<li>Handle heterogeneous data and missing values naturally (+)</li>
<li>If/then logic is easily communicated to non-statisticians (+)</li>
<li>Geometric restrictions create fundamental approximation limits. Ensembles (boosting, random forest, etc.) address this. We’ll discuss them a bit next week. (-)</li>
</ul>
<div class="columns">
<div class="column" style="width:55%;">
<p>The original motivation were trees that doctor’s had created informally (Breiman, Friedman, Olshen, and Stone, 2017).</p>
</div><div class="column" style="width:45%;">
<p><img src="figures/brieman_medical.png" class="center"></p>
</div>
</div>
<hr>
</section>
<section id="exercise-split-choice-visual-explanation" class="level3">
<h3 class="anchored" data-anchor-id="exercise-split-choice-visual-explanation">Exercise: Split Choice Visual Explanation</h3>
<p>Respond to [Split Choice Visual Explanation] in the exercise sheet.</p>
</section>
</section>
</section>
<section id="applying-cart" class="level1">
<h1>8. Applying CART</h1>
<ul>
<li><p>application to case study</p></li>
<li><p>data-driven basis interpretation</p></li>
<li><p>MDI+</p></li>
<li><p>comparing importance measures</p></li>
</ul>
<hr>
<section id="learning-outcomes-6" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-6">Learning outcomes</h3>
<ol type="1">
<li>Describe the theoretical foundation of intrinsically interpretable models like sparse regression, gaussian processes, and classification and regression trees, and apply them to realistic case studies with appropriate validation checks.</li>
<li>Compare the competing definitions of interpretable machine learning, the motivations behind them, and metrics that can be used to quantify whether they have been met.</li>
</ol>
<hr>
</section>
<section id="reading-6" class="level3">
<h3 class="anchored" data-anchor-id="reading-6">Reading</h3>
<ul>
<li><strong>Section 8.1</strong>. James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). An introduction to statistical learning (2nd edn). doi:10.1007/978-1-0716-1418-1
<ul>
<li>PDF available through library: https://search.library.wisc.edu/catalog/9913885290202121</li>
</ul></li>
<li><strong>Sections 3 and 4.1</strong>. Agarwal, A., Kenney, A. M., Tan, Y. S., Tang, T. M., &amp; Yu, B. (2023). Integrating random forests and generalized linear models for improved accuracy and interpretability. doi:10.48550/ARXIV.2307.01932
<ul>
<li>Will not appear on the exam but will appear on HW</li>
</ul></li>
</ul>
<hr>
</section>
<section id="cart-case-study" class="level2">
<h2 class="anchored" data-anchor-id="cart-case-study">CART Case Study</h2>
<hr>
<section id="the-voter-turnout-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-voter-turnout-problem">The Voter Turnout Problem</h3>
<p><strong>Data</strong>. 344,084 observations - Control group: 191,243 - Treatment groups (civic duty, hawthorne, household, neighbor): ~38,200 each</p>
<p><strong>Predictors</strong>. Voting history (5 binary indicators for past elections) and age</p>
<p><strong>Question</strong>. Which voters respond most to social pressure treatments?</p>
<hr>
</section>
<section id="strategy" class="level3">
<h3 class="anchored" data-anchor-id="strategy">Strategy</h3>
<p>Learn partition <span class="math inline">\(\{R_j\}\)</span> of predictor space using only non-treatment variables.</p>
<p>Estimate treatment effect <span class="math inline">\(\tau(x)\)</span> within each region <span class="math inline">\(R_j\)</span>: <span class="math display">\[\hat{\tau}_j = \bar{y}_j^{\text{treated}} - \bar{y}_j^{\text{control}}\]</span></p>
<p>This identifies <strong>treatment effect heterogeneity</strong>.</p>
<hr>
</section>
<section id="tuning-1" class="level3">
<h3 class="anchored" data-anchor-id="tuning-1">Tuning</h3>
<p>Cross-validate complexity parameter <span class="math inline">\(cp \in [0.0001, 0.1]\)</span>.</p>
<p>Optimal tree balances partition refinement against generalization.</p>
<hr>
</section>
<section id="trained-tree-structure" class="level3">
<h3 class="anchored" data-anchor-id="trained-tree-structure">Trained Tree Structure</h3>
<p>[Include visualization of fitted tree]</p>
<hr>
</section>
<section id="partition-wise-treatment-effects" class="level3">
<h3 class="anchored" data-anchor-id="partition-wise-treatment-effects">Partition-wise Treatment Effects</h3>
<p>[Include figure showing <span class="math inline">\(\hat{\tau}_j\)</span> across regions]</p>
<p><strong>Target group</strong>. Treatment effects concentrated among occasional voters within specific age groups.</p>
<p><strong>Zero-effect regions</strong>. “Always voters” and “never voters” show minimal treatment response.</p>
<hr>
</section>
<section id="mean-decrease-in-impurity-mdi" class="level3">
<h3 class="anchored" data-anchor-id="mean-decrease-in-impurity-mdi">Mean Decrease in Impurity (MDI)</h3>
<p>For feature <span class="math inline">\(X_k\)</span>, add the impurity reductions across all splits on <span class="math inline">\(X_k\)</span>: <span class="math display">\[\text{MDI}_k = \sum_{s \in S^{(k)}} \frac{N(t(s))}{n} \hat{\Delta}(s)\]</span></p>
<p>where <span class="math inline">\(S^{(k)}\)</span> are splits using <span class="math inline">\(X_k\)</span>, <span class="math inline">\(N(t(s))\)</span> is node size, and: <span class="math display">\[\hat{\Delta}(s) = \frac{1}{N(t)} \left[\sum_{i \in t} (y_i - \bar{y}_t)^2 - \sum_{i \in t_L} (y_i - \bar{y}_{t_L})^2 - \sum_{i \in t_R} (y_i - \bar{y}_{t_R})^2\right]\]</span></p>
<p><strong>Problem</strong>. MDI has a bias towards high-cardinality features and is unstable when features are correlated.</p>
<hr>
</section>
</section>
<section id="mdi" class="level2">
<h2 class="anchored" data-anchor-id="mdi">MDI+</h2>
<hr>
<section id="trees-as-linear-models" class="level3">
<h3 class="anchored" data-anchor-id="trees-as-linear-models">Trees as Linear Models</h3>
<p><strong>Observation</strong> (Klusowski and Tian, 2023). A decision tree with splits <span class="math inline">\(S = \{s_1, \ldots, s_m\}\)</span> is equivalent to linear regression on transformed features.</p>
<p>For each split <span class="math inline">\(s\)</span> of node <span class="math inline">\(t\)</span> on feature <span class="math inline">\(X_k\)</span> at threshold <span class="math inline">\(\omega\)</span>, define: <span class="math display">\[\phi(x; s) = \frac{N(t_R) \mathbb{1}\{x \in t_L\} - N(t_L) \mathbb{1}\{x \in t_R\}}{\sqrt{N(t_L) N(t_R)}}\]</span></p>
<p><strong>Feature map</strong>. <span class="math inline">\(\Phi(x; S) = (\phi(x; s_1), \ldots, \phi(x; s_m))\)</span> Tree predictions <span class="math inline">\(\hat{f}(x)\)</span> equal OLS predictions on <span class="math inline">\(\Phi(X; S)\)</span>.</p>
<hr>
</section>
<section id="what-this-means-geometrically" class="level3">
<h3 class="anchored" data-anchor-id="what-this-means-geometrically">What This Means Geometrically</h3>
<p>Each split <span class="math inline">\(s\)</span> creates a decision stump, an indicator showing whether <span class="math inline">\(x\)</span> falls left, right, or outside node <span class="math inline">\(t\)</span>.</p>
<p>The matrix <span class="math inline">\(\Phi(X; S) \in \mathbb{R}^{n \times m}\)</span> is the learned basis from the tree structure.</p>
<hr>
</section>
<section id="what-this-means-geometrically-1" class="level3">
<h3 class="anchored" data-anchor-id="what-this-means-geometrically-1">What This Means Geometrically</h3>
<p>Tree model: <span class="math inline">\(f(x) = \beta_0 + \sum_{j=1}^m \beta_j \phi_j(x)\)</span></p>
<p>This is just linear regression in the <span class="math inline">\(\phi\)</span> coordinates.</p>
<hr>
</section>
<section id="mdi-as-r2" class="level3">
<h3 class="anchored" data-anchor-id="mdi-as-r2">MDI as <span class="math inline">\(R^2\)</span></h3>
<p>Partition splits by feature: <span class="math inline">\(S = S^{(1)} \cup \cdots \cup S^{(p)}\)</span></p>
<p>Let <span class="math inline">\(\Phi^{(k)}(X; S)\)</span> be stumps splitting on <span class="math inline">\(X_k\)</span>.</p>
<p><strong>Theorem</strong>. <span class="math display">\[\text{MDI}_k = \text{TSS} \cdot R^2(y, \hat{y}^{(k)})\]</span></p>
<p>where <span class="math inline">\(\hat{y}^{(k)}\)</span> are fitted values regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(\Phi^{(k)}(X; S)\)</span> alone.</p>
<p>MDI measures how much variance <span class="math inline">\(X_k\)</span> explains using only its splits.</p>
<hr>
</section>
<section id="the-overfitting-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-overfitting-problem">The Overfitting Problem</h3>
<p>MDI evaluates <span class="math inline">\(R^2\)</span> on the same data used to build the tree.</p>
<p>This is data snooping! Features that happened to reduce training RSS get inflated importance.</p>
<p>Worse: tree construction already biases toward high-cardinality features and is unstable under correlation.</p>
<hr>
</section>
<section id="mdi-ideas-section-4.1" class="level3">
<h3 class="anchored" data-anchor-id="mdi-ideas-section-4.1">MDI+ Ideas (section 4.1)</h3>
<p>Replace in-sample <span class="math inline">\(R^2\)</span> with proper out-of-sample evaluation.</p>
<ol type="1">
<li><strong>New features</strong>. Add raw <span class="math inline">\(X_k\)</span> to <span class="math inline">\(\Phi^{(k)}(X)\)</span> to capture smooth/additive structure trees miss</li>
<li><strong>Regularize</strong>. Fit penalized GLM (e.g., lasso) on augmented features</li>
<li><strong>Leave-one-out</strong>. Evaluate importance using LOO predictions, not training fit</li>
</ol>
<hr>
</section>
<section id="mdi-algorithm-section-4.1" class="level3">
<h3 class="anchored" data-anchor-id="mdi-algorithm-section-4.1">MDI+ Algorithm (section 4.1)</h3>
<p>Given tree <span class="math inline">\(S\)</span> trained on bootstrap sample <span class="math inline">\(D_n^*\)</span>:</p>
<p><strong>Step 1</strong>. Create augmented feature map <span class="math display">\[\tilde{\Phi}^{(k)}(X) = [\Phi^{(k)}(X), X_k]\]</span></p>
<p><strong>Step 2</strong>. Fit regularized GLM <span class="math inline">\(M\)</span> on full data using <span class="math inline">\(\tilde{\Phi}(X)\)</span>. Tune penalty <span class="math inline">\(\lambda\)</span> via approximate LOO.</p>
<p><strong>Step 3</strong>. For each feature <span class="math inline">\(k\)</span> and observation <span class="math inline">\(i\)</span>, compute LOO partial prediction: <span class="math display">\[\hat{y}_i^k = g^{-1}\left(\sum_{j \neq k} \bar{\tilde{\phi}}_j \hat{\beta}_j + \tilde{\Phi}^{(k)}(x_i) \hat{\beta}_{-i}^{(k)}\right)\]</span></p>
<p>where <span class="math inline">\(\hat{\beta}_{-i}^{(k)}\)</span> are coefficients fitted without observation <span class="math inline">\(i\)</span>.</p>
<p><strong>Step 4</strong>. Measure similarity between <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}^{(k)} = (\hat{y}_1^k, \ldots, \hat{y}_n^k)\)</span>: <span class="math display">\[\text{MDI}_k^+ = m(y, \hat{y}^{(k)})\]</span></p>
<hr>
</section>
<section id="justification" class="level3">
<h3 class="anchored" data-anchor-id="justification">Justification</h3>
<p><strong>Augmentation</strong>. Trees partition with axis-aligned rectangles. Adding raw <span class="math inline">\(X_k\)</span> captures linear/smooth effects the stumps miss.</p>
<p><strong>Regularization</strong>. High-dimensional, correlated features from stumps + raw variables need to be stabilized.</p>
<p><strong>LOO</strong>. Separates training from evaluation. Prevents overstating importance of features that fit training noise.</p>
<p><strong>GLM flexibility</strong>. Use logistic regression for classification, Huber loss for robustness, etc.</p>
<hr>
</section>
<section id="theoretical-guarantees" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-guarantees">Theoretical Guarantees</h3>
<p>Under certain conditions, MDI+ achieves:</p>
<ul>
<li><strong>Consistency</strong>. Recovers true importance ranking as <span class="math inline">\(n \to \infty\)</span></li>
<li><strong>Stability</strong>. Less sensitive to feature correlation than MDI or MDA</li>
<li><strong>Debiasing</strong>. Mitigates tree construction bias toward high-cardinality features</li>
</ul>
<p>See paper for formal statements.</p>
<hr>
</section>
<section id="mdi-stability" class="level3">
<h3 class="anchored" data-anchor-id="mdi-stability">MDI+ Stability</h3>
<p><img src="figures/mdi_stability.png" class="center" width="900/"></p>
<hr>
</section>
<section id="mdi-under-correlation" class="level3">
<h3 class="anchored" data-anchor-id="mdi-under-correlation">MDI+ Under Correlation</h3>
<p><img src="figures/mdi_correlation.png" class="center" width="900/"></p>
<hr>
</section>
<section id="practical-considerations" class="level3">
<h3 class="anchored" data-anchor-id="practical-considerations">Practical Considerations</h3>
<p><strong>Hyperparameters</strong></p>
<ul>
<li>Choice of augmentation (raw features? polynomials?)</li>
<li>GLM family (gaussian, binomial, etc.)</li>
<li>Penalty strength <span class="math inline">\(\lambda\)</span> (tuned via LOO CV)</li>
<li>Similarity metric <span class="math inline">\(m\)</span> (negative MSE, correlation, etc.)</li>
</ul>
<p><strong>Computational cost</strong>. Approximate LOO (Rad &amp; Maleki 2020) avoids refitting <span class="math inline">\(n\)</span> times. Nearly same cost as standard RF.</p>
<p><strong>Package</strong>. Available in the <code>imodels</code> package (https://csinva.io/imodels/).</p>
<hr>
</section>
<section id="takewaways" class="level3">
<h3 class="anchored" data-anchor-id="takewaways">Takewaways</h3>
<p>Decision trees are closely connected to linear regression through a learned piecewise constant basis <span class="math inline">\(\Phi(X; S)\)</span>. MDI is <span class="math inline">\(R^2\)</span> of partial regressions on this basis but suffers from overfitting.</p>
<p>MDI+ corrects this by:</p>
<ol type="1">
<li>Augmenting with features trees naturally miss</li>
<li>Regularizing high-dimensional basis</li>
<li>Evaluating out-of-sample via LOO</li>
</ol>
</section>
</section>
</section>
<section id="tree-ensembles" class="level1">
<h1>9. Tree Ensembles</h1>
<ul>
<li><p>motivating case study</p></li>
<li><p>random forest algorithm</p></li>
<li><p>random forest variable importance</p></li>
<li><p>critiques</p></li>
</ul>
<hr>
<section id="learning-outcome" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcome">Learning outcome</h3>
<ol type="1">
<li>Describe the theoretical foundation of intrinsically interpretable models like sparse regression, gaussian processes, and classification and regression trees, and apply them to realistic case studies with appropriate validation checks.</li>
<li>Compare the competing definitions of interpretable machine learning, the motivations behind them, and metrics that can be used to quantify whether they have been met.</li>
</ol>
<hr>
</section>
<section id="reading-7" class="level3">
<h3 class="anchored" data-anchor-id="reading-7">Reading</h3>
<ul>
<li><strong>Section 8.1</strong>. James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). An introduction to statistical learning (2nd edn). doi:10.1007/978-1-0716-1418-1
<ul>
<li>PDF available through library: https://search.library.wisc.edu/catalog/9913885290202121</li>
</ul></li>
<li><code>imodels</code> documentation.</li>
</ul>
<hr>
</section>
<section id="motivating-case-study-1" class="level2">
<h2 class="anchored" data-anchor-id="motivating-case-study-1">Motivating Case Study</h2>
<p>Two major oxygenation events occurred in Earth’s history: - Great Oxygenation Event: first atmospheric oxygen - Neoproterozoic Oxygenation Event (NOE): increase to current levels (~540 million years ago, coinciding with Cambrian Explosion)</p>
<hr>
<section id="scientific-goal" class="level3">
<h3 class="anchored" data-anchor-id="scientific-goal">Scientific Goal</h3>
<p><strong>Question</strong>. How rapidly did the NOE occur?</p>
<p><strong>Data</strong>. Measurements from ancient rocks:</p>
<ul>
<li>Age (millions of years)</li>
<li>Environment (deep ocean, lakes, etc.)</li>
<li>Trace elements and total organic carbon (TOC)</li>
</ul>
<hr>
</section>
<section id="statistical-formulation" class="level3">
<h3 class="anchored" data-anchor-id="statistical-formulation">Statistical Formulation</h3>
<p><strong>Observed</strong>. Mo-ppm level <span class="math inline">\(y \in \mathbb{R}\)</span></p>
<p><strong>Predictors</strong>. <span class="math inline">\(x = (x_1, x_{2:D}) = (\text{age}, \text{context})\)</span> where context includes environment, lithology, trace elements</p>
<p><strong>Model</strong>. <span class="math inline">\(y_i = f(x_i) + \epsilon_i\)</span></p>
<p><strong>Goal</strong>. Determine how <span class="math inline">\(f\)</span> depends on age at fixed context.</p>
<p><strong>Requirements</strong>.</p>
<ul>
<li>Capture sharp transitions in <span class="math inline">\(y\)</span></li>
<li>Isolate contribution of age from confounding context</li>
</ul>
<hr>
</section>
</section>
<section id="random-forests" class="level2">
<h2 class="anchored" data-anchor-id="random-forests">Random Forests</h2>
<hr>
<section id="variance-reduction-via-averaging" class="level3">
<h3 class="anchored" data-anchor-id="variance-reduction-via-averaging">Variance Reduction via Averaging</h3>
<p>Single decision trees have high variance: small changes in training data produce different splits.</p>
<p><strong>Bootstrap Aggregating (Bagging)</strong> reduces variance by averaging:</p>
<p><span class="math display">\[\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}^{*b}(x)\]</span></p>
<p>where <span class="math inline">\(\hat{f}^{*b}(x)\)</span> is the prediction from a tree grown on the <span class="math inline">\(b\)</span>-th bootstrap sample.</p>
<p>For regression: arithmetic mean. For classification: majority vote.</p>
<hr>
</section>
<section id="feature-subsampling" class="level3">
<h3 class="anchored" data-anchor-id="feature-subsampling">Feature Subsampling</h3>
<p><strong>Problem</strong>. If one predictor dominates, bagged trees correlate highly. Averaging correlated quantities provides less variance reduction than averaging independent quantities.</p>
<p><strong>Solution</strong>. At each split, consider only <span class="math inline">\(m\)</span> random predictors from the full set of <span class="math inline">\(p\)</span> predictors.</p>
<p>Typical choices:</p>
<ul>
<li>Classification: <span class="math inline">\(m \approx \sqrt{p}\)</span></li>
<li>Regression: <span class="math inline">\(m \approx p/3\)</span></li>
</ul>
<p>This decorrelates trees by ensuring <span class="math inline">\((p-m)/p\)</span> of splits exclude the dominant predictor.</p>
<hr>
</section>
<section id="algorithm" class="level3">
<h3 class="anchored" data-anchor-id="algorithm">Algorithm</h3>
<ol type="1">
<li>Draw bootstrap sample of size <span class="math inline">\(n\)</span> with replacement</li>
<li>Grow tree on bootstrap sample:
<ul>
<li>At each node, select <span class="math inline">\(m\)</span> predictors randomly</li>
<li>Choose best split from these <span class="math inline">\(m\)</span> predictors</li>
<li>Grow deep (no pruning)</li>
</ul></li>
<li>Repeat for <span class="math inline">\(B\)</span> trees</li>
<li>Aggregate: average (regression) or vote (classification)</li>
</ol>
<hr>
</section>
<section id="hyperparameters-1" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters-1">Hyperparameters</h3>
<ul>
<li><strong><span class="math inline">\(B\)</span></strong> (number of trees): Large <span class="math inline">\(B\)</span> does not overfit. Use <span class="math inline">\(B \geq 500\)</span> for stable error.</li>
<li><strong><span class="math inline">\(m\)</span></strong> (mtry): Number of features per tree.</li>
<li><strong>nodesize</strong>. Minimum observations in terminal nodes. Controls tree depth.</li>
</ul>
<hr>
</section>
</section>
<section id="boosting" class="level2">
<h2 class="anchored" data-anchor-id="boosting">Boosting</h2>
<hr>
<section id="sequential-learning" class="level3">
<h3 class="anchored" data-anchor-id="sequential-learning">Sequential Learning</h3>
<p>Unlike bagging (parallel trees), boosting grows trees sequentially. Each tree fits the residuals from previous trees.</p>
<ol type="1">
<li>Initialize <span class="math inline">\(\hat{f}(x) = 0\)</span> and <span class="math inline">\(r_i = y_i\)</span> for all <span class="math inline">\(i\)</span></li>
<li>For <span class="math inline">\(b = 1, 2, \ldots, B\)</span>:
<ul>
<li>Fit tree <span class="math inline">\(\hat{f}^b\)</span> with <span class="math inline">\(d\)</span> splits to data <span class="math inline">\((X, r)\)</span></li>
<li>Update: <span class="math inline">\(\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)\)</span></li>
<li>Update residuals: <span class="math inline">\(r_i \leftarrow r_i - \lambda \hat{f}^b(x_i)\)</span></li>
</ul></li>
<li>Output: <span class="math inline">\(\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x)\)</span></li>
</ol>
<p>The shrinkage parameter <span class="math inline">\(\lambda\)</span> (learning rate) controls update step size.</p>
<hr>
</section>
<section id="hyperparameters-2" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters-2">Hyperparameters</h3>
<ul>
<li><strong><span class="math inline">\(B\)</span></strong>. Unlike random forests, boosting can overfit if <span class="math inline">\(B\)</span> is too large. Use cross-validation.</li>
<li><strong><span class="math inline">\(\lambda\)</span></strong>. Typically 0.01 or 0.001. Smaller <span class="math inline">\(\lambda\)</span> requires larger <span class="math inline">\(B\)</span> but often improves performance.</li>
<li><strong><span class="math inline">\(d\)</span></strong> (interaction depth): Number of splits per tree. <span class="math inline">\(d=1\)</span> (stumps) yields additive model. <span class="math inline">\(d&gt;1\)</span> captures interactions.</li>
</ul>
<hr>
</section>
</section>
<section id="variable-importance" class="level2">
<h2 class="anchored" data-anchor-id="variable-importance">Variable Importance</h2>
<hr>
<section id="mean-decrease-in-impurity-mdi-1" class="level3">
<h3 class="anchored" data-anchor-id="mean-decrease-in-impurity-mdi-1">Mean Decrease in Impurity (MDI)</h3>
<p>For each predictor <span class="math inline">\(X_j\)</span> and component tree, compute the MDI. Then average over all <span class="math inline">\(B\)</span> trees.</p>
<p><span class="math display">\[\text{MDI}(X_j) = \frac{1}{B} \sum_{b=1}^B \sum_{t \in T_b: v(t)=X_j} p(t) \Delta_i(t)\]</span></p>
<p>where <span class="math inline">\(p(t)\)</span> is the proportion of samples at node <span class="math inline">\(t\)</span> and <span class="math inline">\(\Delta_i(t)\)</span> is the weighted impurity decrease.</p>
<hr>
</section>
<section id="permutation-importance" class="level3">
<h3 class="anchored" data-anchor-id="permutation-importance">Permutation Importance</h3>
<p>Measure accuracy drop when feature values are randomly shuffled.</p>
<ol type="1">
<li>Train forest on data <span class="math inline">\(D\)</span></li>
<li>Calculate OOB error <span class="math inline">\(E_{\text{orig}}\)</span></li>
<li>For each tree <span class="math inline">\(b\)</span>:
<ul>
<li>Take OOB observations</li>
<li>Shuffle values of <span class="math inline">\(X_j\)</span> in those observations</li>
<li>Calculate OOB error <span class="math inline">\(E_{\text{perm},b}\)</span></li>
</ul></li>
<li>Importance = Average<span class="math inline">\((E_{\text{perm},b} - E_{\text{orig},b})\)</span></li>
</ol>
<p>If <span class="math inline">\(X_j\)</span> is important, shuffling breaks its relationship with <span class="math inline">\(y\)</span>, increasing error. If <span class="math inline">\(X_j\)</span> is noise, shuffling has minimal effect.</p>
<hr>
</section>
</section>
<section id="partial-dependence" class="level2">
<h2 class="anchored" data-anchor-id="partial-dependence">Partial Dependence</h2>
<hr>
<section id="definition" class="level3">
<h3 class="anchored" data-anchor-id="definition">Definition</h3>
<p>The partial dependence of <span class="math inline">\(f(X)\)</span> on subset <span class="math inline">\(X_S\)</span> is:</p>
<p><span class="math display">\[f_S(x_S) = \mathbb{E}_{X_C}[f(x_S, X_C)]\]</span></p>
<p>where <span class="math inline">\(C\)</span> is the complement of <span class="math inline">\(S\)</span> in the full predictor set.</p>
<p>We estimate this by averaging predictions over all <span class="math inline">\(N\)</span> training observations:</p>
<p><span class="math display">\[\bar{f}_S(x_S) = \frac{1}{N} \sum_{i=1}^{N} f(x_S, x_{i,C})\]</span></p>
<p>This shows the marginal effect of <span class="math inline">\(X_S\)</span> on the response after accounting for average effects of other variables.</p>
<hr>
</section>
<section id="interpretation" class="level3">
<h3 class="anchored" data-anchor-id="interpretation">Interpretation</h3>
<p><span class="math inline">\(f_S(x_S)\)</span> represents the effect of <span class="math inline">\(X_S\)</span> <strong>after accounting for</strong> the average effects of <span class="math inline">\(X_C\)</span>. This differs from the conditional expectation <span class="math inline">\(\tilde{f}_S(x_S) = \mathbb{E}[f(X_S, X_C) | X_S]\)</span>, which ignores <span class="math inline">\(X_C\)</span>. <br></p>
<p>The two coincide only if <span class="math inline">\(X_S \perp X_C\)</span>. If <span class="math inline">\(f(X) = h_1(X_S) + h_2(X_C)\)</span> (additive), then <span class="math inline">\(f_S(x_S) = h_1(x_S)\)</span> up to an additive constant. If <span class="math inline">\(f(X) = h_1(X_S) \cdot h_2(X_C)\)</span> (multiplicative), then <span class="math inline">\(f_S(x_S) = h_1(x_S)\)</span> up to a multiplicative constant.</p>
<hr>
</section>
</section>
<section id="critiques" class="level2">
<h2 class="anchored" data-anchor-id="critiques">Critiques</h2>
<hr>
<section id="instability-efrons-experiment" class="level3">
<h3 class="anchored" data-anchor-id="instability-efrons-experiment">Instability (Efron’s Experiment)</h3>
<p><strong>Experiment</strong>. Remove the “most important” feature. The model often identifies a proxy feature with high correlation, achieving similar performance.</p>
<p><strong>Implication</strong>. Importance measures are unstable under perturbation when predictors correlate.</p>
<hr>
</section>
<section id="extrapolation-hookers-experiment" class="level3">
<h3 class="anchored" data-anchor-id="extrapolation-hookers-experiment">Extrapolation (Hooker’s Experiment)</h3>
<p><strong>Problem</strong>. PDPs evaluate <span class="math inline">\(f(x_S, x_{i,C})\)</span> at points that may not exist in the joint distribution <span class="math inline">\(P(X_S, X_C)\)</span>.</p>
<p><strong>Example</strong>. If age and carbon are highly correlated, a PDP might evaluate “1 billion years old” with “modern carbon levels”, even though this never occurs in training data.</p>
<p><strong>Check</strong>. - Check correlation matrix of predictors</p>
<hr>
</section>
</section>
<section id="takeaways-2" class="level2">
<h2 class="anchored" data-anchor-id="takeaways-2">Takeaways</h2>
<ul>
<li><strong>Random forests</strong>. Reduce variance via bagging and feature subsampling.</li>
<li><strong>Boosting</strong>. Reduce bias and variance via sequential residual fitting.</li>
<li><strong>Variable importance</strong>. Provides global feature contribution but is sensitive to correlation structure.</li>
<li><strong>Partial dependence</strong>. Shows average marginal effects but risks extrapolation when features correlate strongly.</li>
</ul>
<hr>
</section>
<section id="exercise-3" class="level2">
<h2 class="anchored" data-anchor-id="exercise-3">Exercise</h2>
<p><strong>Depth-1 Boosting</strong>. Prove that boosting with stumps (<span class="math inline">\(d=1\)</span>) produces an additive model <span class="math inline">\(f(X) = \sum_{j=1}^p f_j(X_j)\)</span>.</p>
</section>
</section>
<section id="applying-tree-ensembles" class="level1">
<h1>10. Applying Tree Ensembles</h1>
<ul>
<li>application to case study</li>
<li>PDP and MDI+</li>
<li>comparing importance measures</li>
</ul>
<hr>
<section id="learning-outcomes-7" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-7">Learning outcomes</h3>
<ol type="1">
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
<li>Describe the theoretical foundation of intrinsically interpretable models like sparse regression, gaussian processes, and classification and regression trees, and apply them to realistic case studies with appropriate validation checks.</li>
</ol>
<hr>
</section>
<section id="reading-8" class="level3">
<h3 class="anchored" data-anchor-id="reading-8">Reading</h3>
<ul>
<li><strong>Section 8.1</strong>. James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). An introduction to statistical learning (2nd edn).</li>
<li><strong>Section 10.13.2</strong>. Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning (2nd edn).</li>
</ul>
<hr>
</section>
<section id="data-summary" class="level3">
<h3 class="anchored" data-anchor-id="data-summary">Data summary</h3>
<ul>
<li><strong>Target (<span class="math inline">\(y\)</span>)</strong>. <code>mo_ppm</code> (Molybdenum concentration), proxy for ocean oxygenation</li>
<li><strong>Key predictor</strong>. <code>age_model</code> (geological age in millions of years)</li>
<li><strong>Context</strong>. <code>site_type</code> (restricted vs.&nbsp;open marine), lithology, trace elements</li>
</ul>
<hr>
</section>
<section id="model-random-forest-regression" class="level3">
<h3 class="anchored" data-anchor-id="model-random-forest-regression">Model: Random Forest Regression</h3>
<p>Fit <span class="math inline">\(B\)</span> trees to approximate <span class="math inline">\(\mathbb{E}[Y|X]\)</span> where <span class="math inline">\(Y\)</span> = <code>mo_ppm</code> and <span class="math inline">\(X\)</span> includes age, site type, and geochemical context.</p>
<p>This should capture the temporal shift (due to the oxygenation event) while controlling for heterogeneity in sample collection.</p>
<hr>
</section>
<section id="hyperparameter-tuning" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<p>Grid search over: - <strong>ntree</strong>. Number of trees <span class="math inline">\(\in [1, 256]\)</span> - <strong>mtry</strong>. Features per split <span class="math inline">\(\in [2, 10]\)</span></p>
<p><strong>Validation</strong>. 3-fold cross-validation minimizing Mean Squared Error (MSE).</p>
<p>Select <span class="math inline">\(B\)</span> and <span class="math inline">\(m\)</span> that minimize cross-validated MSE.</p>
<hr>
</section>
<section id="variable-importance-1" class="level2">
<h2 class="anchored" data-anchor-id="variable-importance-1">Variable Importance</h2>
<p>:::: {.columns} ::: {.column width=“50%”} This is the average of MDI across all component trees.</p>
<p><span class="math display">\[\text{MDI}(j) = \frac{1}{B} \sum_{b=1}^{B} \sum_{t \in \mathcal{T}_b, v(t)=j} \Delta_i(t)\]</span></p>
<p><span class="math inline">\(\Delta_i(t)\)</span> is the weighted RSS reduction at node <span class="math inline">\(t\)</span> in tree <span class="math inline">\(\mathcal{T}_{b}\)</span> ::: ::: {.column width=“50%”} <!-- <img src="figures/mdi_figure.png"/> --> :::</p>
<hr>
</section>
<section id="model-validation" class="level2">
<h2 class="anchored" data-anchor-id="model-validation">Model Validation</h2>
<p>Before interpretation, validate the fit:</p>
<p><strong>Observed vs.&nbsp;Predicted</strong>. How accurate are predictions? Any systematic bias?</p>
<p><strong>Residual Analysis</strong>. Any potentially missing covariates?</p>
<hr>
</section>
<section id="partial-dependence-1" class="level2">
<h2 class="anchored" data-anchor-id="partial-dependence-1">Partial Dependence</h2>
<hr>
<section id="implementation-dalex" class="level3">
<h3 class="anchored" data-anchor-id="implementation-dalex">Implementation (DALEX)</h3>
<ol type="1">
<li>Create explainer: <code>explain(model, data, y)</code></li>
<li>Generate model profile: <code>model_profile(explainer, variables = "age_model")</code></li>
</ol>
<p>We’re computing</p>
<p><span class="math display">\[\bar{f}_S(x_S) = \frac{1}{n} \sum_{i=1}^n \hat{f}(x_S, x_{i,C})\]</span></p>
<p>where <span class="math inline">\(S\)</span> = <code>age_model</code> and <span class="math inline">\(C\)</span> = all other features.</p>
<hr>
</section>
<section id="interpretation-1" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-1">Interpretation</h3>
<p>The PDP isolates the oxygenation signal from site-specific confounders. The transition appears to occur in several phase. This was the main point of .</p>
<!-- <img src="figures/07_pdp_overall.png"/> -->
<hr>
</section>
<section id="site-stratified-pdps" class="level3">
<h3 class="anchored" data-anchor-id="site-stratified-pdps">Site-Stratified PDPs</h3>
<p>Compute separate PDPs for each <code>site_type</code>:</p>
<p><span class="math display">\[\bar{f}_{\text{age}}(\text{age} \mid \text{site}) = \frac{1}{n_{\text{site}}} \sum_{i: \text{site}_i = \text{site}} \hat{f}(\text{age}, x_{i,C})\]</span></p>
<!-- <img src="figures/07_pdp_overall.png"/> -->
<hr>
</section>
</section>
<section id="mdi-1" class="level2">
<h2 class="anchored" data-anchor-id="mdi-1">MDI+</h2>
<hr>
<section id="implementation-imodels-package" class="level3">
<h3 class="anchored" data-anchor-id="implementation-imodels-package">Implementation (<code>imodels</code> package)</h3>
<p>MDI+ maps tree structure to a linear model of “stumps” and uses regularized regression for importance scores.</p>
<p><strong>Steps</strong>. 1. Extract all splits from random forest 2. Represent predictions as weighted sum of indicator functions 3. Apply Ridge or Lasso to assign coefficients 4. Rank features by absolute coefficient values</p>
<hr>
</section>
<section id="mdi-result" class="level3">
<h3 class="anchored" data-anchor-id="mdi-result">MDI+ Result</h3>
<!-- <img src="figures/mdi_plus_result.png"/> -->
<hr>
</section>
</section>
<section id="sanity-checks" class="level2">
<h2 class="anchored" data-anchor-id="sanity-checks">Sanity Checks</h2>
<hr>
<section id="predictive-accuracy-rf-vs.-rf" class="level3">
<h3 class="anchored" data-anchor-id="predictive-accuracy-rf-vs.-rf">Predictive Accuracy: RF vs.&nbsp;RF+</h3>
<ul>
<li><strong>Random Forest (RF)</strong>. Optimized for MSE, produces complex trees</li>
<li><strong>RF+ (via MDI+)</strong>. Seeks sparse influential feature set</li>
</ul>
<p><strong>Validation</strong>. Check whether RF+ maintains similar <span class="math inline">\(R^2\)</span> to RF. If yes, trust MDI+ rankings.</p>
<hr>
</section>
<section id="pairwise-correlations" class="level3">
<h3 class="anchored" data-anchor-id="pairwise-correlations">Pairwise Correlations</h3>
<p>(show pairwise correlations between pairs of predictors)</p>
<hr>
</section>
<section id="stability-analysis" class="level3">
<h3 class="anchored" data-anchor-id="stability-analysis">Stability Analysis</h3>
<p><strong>Efron’s Bootstrap Experiment</strong> on SGP data:</p>
<ol type="1">
<li>Train forest on full data</li>
<li>Identify top <span class="math inline">\(k\)</span> features by importance</li>
<li>Remove top feature, retrain</li>
<li>Measure accuracy drop</li>
</ol>
<hr>
</section>
<section id="hyperparameter-sensitivity" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-sensitivity">Hyperparameter Sensitivity</h3>
<p><strong>Effect of <span class="math inline">\(m\)</span> (mtry)</strong>.</p>
<p><strong>Effect of <span class="math inline">\(B\)</span> (ntree)</strong>.</p>
<hr>
</section>
</section>
<section id="takeaways-3" class="level2">
<h2 class="anchored" data-anchor-id="takeaways-3">Takeaways</h2>
<ol type="1">
<li><p><strong>Nonlinear modeling</strong>. Tree ensembles let us capture nonlinear transition in Mo as a function of sample age.</p></li>
<li><p><strong>Partial dependence</strong>. Isolates temporal oxygenation signal from environmental confounders.</p></li>
<li><p><strong>Limitations</strong>.</p>
<ul>
<li>PDPs risk extrapolation when features correlate strongly</li>
<li>Importance measures can be unstable, even with improvements like MDI+</li>
</ul></li>
<li><p><strong>Checks</strong>.</p>
<ul>
<li>Stability across bootstraps</li>
<li>Feature perturbation (removing top features)</li>
<li>Hyperparameter sensitivity</li>
</ul></li>
</ol>
<hr>
</section>
<section id="exercise-4" class="level2">
<h2 class="anchored" data-anchor-id="exercise-4">Exercise</h2>
<hr>
<section id="shap-importance" class="level3">
<h3 class="anchored" data-anchor-id="shap-importance">11. SHAP importance</h3>
<ul>
<li><p>motivating case study</p></li>
<li><p>game theoretic motivation</p></li>
<li><p>translation to ML</p></li>
<li><p>visualization</p></li>
</ul>
<hr>
</section>
<section id="learning-outcomes-8" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-8">Learning outcomes</h3>
<ol type="1">
<li>Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ol>
<hr>
</section>
<section id="motivating-case-study-2" class="level3">
<h3 class="anchored" data-anchor-id="motivating-case-study-2">Motivating Case Study</h3>
<p>Enhancers are stretches of the genome that coordinate the expression of many downstream genes.</p>
<ul>
<li>Their status is determined by combinations of binding transcription factors and chromatin marks</li>
</ul>
<p>Understanding these interactions explains genotype <span class="math inline">\(\to\)</span> phenotype.</p>
<hr>
</section>
<section id="motivating-case-study-3" class="level3">
<h3 class="anchored" data-anchor-id="motivating-case-study-3">Motivating Case Study</h3>
<p>This can be studied using developing fruit flies.</p>
<p><img src="figures/nikon-1.gif"></p>
<p>(<a href="https://www.thisiscolossal.com/2024/09/nikon-small-world-in-motion-2024/">source</a>)</p>
<hr>
</section>
<section id="question" class="level3">
<h3 class="anchored" data-anchor-id="question">Question</h3>
<p>Which features drive enhancer activity in each sequence?</p>
<p><strong>Data</strong></p>
<p>Genome-wide measurements from blastoderm (stage 5) <em>Drosophila</em> embryos:</p>
<ul>
<li>DNA occupancy for 23 transcription factors</li>
<li>Activity for 13 chromatin markers</li>
<li>Binary labels indicating enhancer activity for genomic regions</li>
</ul>
<p>Each observation: a genomic sequence with associated regulatory features.</p>
<hr>
</section>
<section id="statistical-formulation-1" class="level3">
<h3 class="anchored" data-anchor-id="statistical-formulation-1">Statistical Formulation</h3>
<p><strong>Enhancer status</strong>: <span class="math inline">\(y \in \{0,1\}\)</span></p>
<p><strong>Predictors</strong>: <span class="math inline">\(x = (x_1, \ldots, x_D)\)</span> (TF binding intensities, chromatin signals)</p>
<p><strong>Model</strong>: <span class="math inline">\(f(x_i) = \mathbb{P}(y_i = 1 \mid x_i)\)</span></p>
<p><strong>Goal.</strong> Quantify each feature’s contribution to <span class="math inline">\(f(x_i)\)</span>.</p>
<hr>
</section>
</section>
<section id="game-theory" class="level2">
<h2 class="anchored" data-anchor-id="game-theory">Game Theory</h2>
<hr>
<section id="credit-assignment" class="level3">
<h3 class="anchored" data-anchor-id="credit-assignment">Credit Assignment</h3>
<p>How to distribute profit across employees <span class="math inline">\(i\)</span> in a company if any team <span class="math inline">\(S\)</span> has profit <span class="math inline">\(v(S)\)</span>?</p>
<p><span style="font-size: 18px;"> <img width="600" src="figures/shapley_team_1.png" class="center"><br> Game theoretic analogy. </span></p>
<hr>
</section>
<section id="shapley-credit-assignment" class="level3">
<h3 class="anchored" data-anchor-id="shapley-credit-assignment">Shapley Credit Assignment</h3>
<p>Employee <span class="math inline">\(i\)</span>’s credit: average marginal contribution <span class="math inline">\(v(S) - v(S \setminus \{i\})\)</span> over all teams <span class="math inline">\(S \ni i\)</span>.</p>
<p><span class="math display">\[\begin{align}
\varphi(i) = \frac{1}{D} \sum_{d = 1}^{D} \frac{1}{\binom{D-1}{d-1}}\sum_{S \in S_{d}(i)} [v(S) - v(S \setminus \{i\})]
\end{align}\]</span></p>
<p>where <span class="math inline">\(S_{d}(i)\)</span> collects subsets of size <span class="math inline">\(d\)</span> containing <span class="math inline">\(i\)</span>.</p>
<p><img width="400" src="figures/shapley_team_2.png" class="center"></p>
<hr>
</section>
<section id="axioms" class="level3">
<h3 class="anchored" data-anchor-id="axioms">Axioms</h3>
<p>Shapley values are the unique credit assignment satisfying:</p>
<p><strong>Symmetry.</strong> Equal marginal contributions <span class="math inline">\(\to\)</span> equal credit. <span class="math display">\[\begin{align*}
\varphi(i) = \varphi(j) \;\text{ if }\; v(S \cup \{i\}) - v(S) = v(S \cup \{j\}) - v(S) \;\; \forall S
\end{align*}\]</span></p>
<p><strong>Dummy.</strong> Zero marginal contribution <span class="math inline">\(\to\)</span> zero credit. <span class="math display">\[\begin{align*}
\varphi(i) = 0 \;\text{ if }\; v(S \cup \{i\}) = v(S) \;\; \forall S
\end{align*}\]</span></p>
<p><strong>Additivity.</strong> For two games <span class="math inline">\(v_1, v_2\)</span>, attributions add. <span class="math display">\[\begin{align*}
\varphi_i(v_1 + v_2) = \varphi_i(v_1) + \varphi_i(v_2)
\end{align*}\]</span></p>
<hr>
</section>
<section id="efficiency" class="level3">
<h3 class="anchored" data-anchor-id="efficiency">Efficiency</h3>
<p><strong>Efficiency.</strong> All credit is distributed, and no double counting.</p>
<p><span class="math display">\[\sum_{i=1}^D \varphi(i) = v(D) - v(\emptyset)\]</span></p>
<hr>
</section>
</section>
<section id="feature-attribution" class="level2">
<h2 class="anchored" data-anchor-id="feature-attribution">Feature Attribution</h2>
<hr>
<section id="game-theory-to-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="game-theory-to-machine-learning">Game Theory <span class="math inline">\(\to\)</span> Machine Learning</h3>
<p>For each prediction <span class="math inline">\(f(x)\)</span>, define a game:</p>
<p><span class="math display">\[\begin{align}
v_{x}(S) = \mathbb{E}_{p(x'_{S^C} \mid x_{S})}[f(x_{S}, x'_{S^C})]
\end{align}\]</span></p>
<p>where <span class="math inline">\(x_S\)</span> denotes coordinates in <span class="math inline">\(S\)</span> fixed at their observed values, and <span class="math inline">\(x'_{S^C}\)</span> are the remaining coordinates drawn from their conditional distribution.</p>
<p>How to define <span class="math inline">\(v(S)\)</span>? This determines what “importance” means.</p>
<hr>
</section>
<section id="shapley-feature-attribution" class="level3">
<h3 class="anchored" data-anchor-id="shapley-feature-attribution">Shapley Feature Attribution</h3>
<p>Feature <span class="math inline">\(i\)</span>’s contribution to <span class="math inline">\(f(x)\)</span>:</p>
<p><span class="math display">\[\begin{align}
\varphi_{x}(f, i) = \frac{1}{D} \sum_{d = 1}^{D} \frac{1}{\binom{D-1}{d-1}}\sum_{S \in S_{d}(i)}[v_{x}(S) - v_{x}(S \setminus \{i\})]
\end{align}\]</span></p>
<p>These satisfy: <span class="math display">\[\sum_{i=1}^D \varphi_{x}(f, i) = f(x) - \mathbb{E}[f(X)]\]</span></p>
<p>Each <span class="math inline">\(\varphi_x(f,i)\)</span> explains deviation from baseline prediction.</p>
<hr>
</section>
<section id="geometric-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="geometric-interpretation">Geometric Interpretation</h3>
<p><span class="math display">\[\begin{align}
v_{x}(S) = \mathbb{E}_{p(x'_{S^C} \mid x_{S})}[f(x_{S}, x'_{S^C})]
\end{align}\]</span></p>
<p>.center[ <img width="800" src="figures/shapley_v2.png">]</p>
<p>Conditional expectation when features <span class="math inline">\(S\)</span> are fixed at <span class="math inline">\(x_S\)</span>.</p>
<hr>
</section>
<section id="exercise-shap-visual-explanation" class="level3">
<h3 class="anchored" data-anchor-id="exercise-shap-visual-explanation">Exercise: SHAP Visual Explanation</h3>
<p>Respond to [SHAP Visual Explanation] in the exercise sheet.</p>
<hr>
</section>
<section id="main-difficulty" class="level3">
<h3 class="anchored" data-anchor-id="main-difficulty">Main Difficulty</h3>
<p>The conditional expectation <span class="math inline">\(\mathbb{E}[f(x_S, X_{S^C}) \mid x_S]\)</span> is generally <strong>not computable</strong>.</p>
<p>This forces a choice:</p>
<ol type="1">
<li><strong>Marginal Shapley.</strong> Replace with <span class="math inline">\(\mathbb{E}[f(x_S, X_{S^C})]\)</span>
<ul>
<li>Ignores feature correlations</li>
<li>Reveals model’s functional form</li>
</ul></li>
<li><strong>Conditional Shapley.</strong> Model <span class="math inline">\(p(x_{S^C} \mid x_S)\)</span>
<ul>
<li>Respects correlations</li>
<li>Hard to estimate, often biased</li>
</ul></li>
</ol>
<p>We discuss this further next week.</p>
<hr>
</section>
<section id="visualization---one-sample" class="level3">
<h3 class="anchored" data-anchor-id="visualization---one-sample">Visualization - One Sample</h3>
<p>Attributions sum to <span class="math inline">\(f(x)\)</span>: visualize as stacked bar centered at prediction.</p>
<p><span style="font-size: 18px;"> <img width="900" src="figures/shapley_sample_level.png" class="center"> </span></p>
<hr>
</section>
<section id="visualization---many-samples" class="level3">
<h3 class="anchored" data-anchor-id="visualization---many-samples">Visualization - Many Samples</h3>
<p>Compact visualization identifies samples with similar explanations.</p>
<p><span style="font-size: 18px;"> <img width="900" src="figures/shapley_dataset_level.png" class="center"> </span></p>
<hr>
</section>
</section>
</section>
<section id="shap-computation" class="level1">
<h1>12. SHAP computation</h1>
<ul>
<li><p>KernelSHAP</p></li>
<li><p>amortization</p></li>
<li><p>application to case study</p></li>
</ul>
<hr>
<section id="learning-outcomes-9" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-9">Learning outcomes</h3>
<ol type="1">
<li>Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ol>
<hr>
</section>
<section id="computational-challenges" class="level3">
<h3 class="anchored" data-anchor-id="computational-challenges">Computational Challenges</h3>
<p>Computing Shapley values exactly is intractable except in trivial cases.</p>
<ol type="1">
<li><span class="math inline">\(v_{x}(S)\)</span> requires evaluating conditional expectations.</li>
<li>The sum runs over <span class="math inline">\(2^D\)</span> subsets.</li>
</ol>
<hr>
</section>
<section id="approximating-v_xs" class="level3">
<h3 class="anchored" data-anchor-id="approximating-v_xs">Approximating <span class="math inline">\(v_{x}(S)\)</span></h3>
<p>How to compute:</p>
<p><span class="math display">\[v_{x}(S) = \mathbb{E}_{p(x'_{S^C} \mid x_{S})}[f(x_{S}, x'_{S^C})]\]</span></p>
<p>?</p>
<hr>
</section>
<section id="reference-values" class="level3">
<h3 class="anchored" data-anchor-id="reference-values">Reference Values</h3>
<p>Replace <span class="math inline">\(x'_{S^C}\)</span> with fixed reference (zeros, mean <span class="math inline">\(\bar{x}_{S^C}\)</span>):</p>
<p><span class="math display">\[v_{x}(S) \approx f(x_{S}, \bar{x}_{S^C})\]</span></p>
<p>One evaluation per <span class="math inline">\(S\)</span>. Very rough approximation – ignores correlations.</p>
<hr>
</section>
<section id="independence-assumption" class="level3">
<h3 class="anchored" data-anchor-id="independence-assumption">Independence Assumption</h3>
<p>Assume <span class="math inline">\(x_{S} \perp x_{S^C}\)</span>:</p>
<p><span class="math display">\[v_{x}(S) = \mathbb{E}_{p(x'_{S^C})}[f(x_{S}, x'_{S^C})] \approx \frac{1}{N}\sum_{n=1}^{N} f(x_{S}, x'_{n,S^C})\]</span></p>
<p>Sample or subsample from marginal.</p>
<hr>
</section>
<section id="regression-perspective" class="level3">
<h3 class="anchored" data-anchor-id="regression-perspective">Regression Perspective</h3>
<p>Can be shown that Shapley values solve a weighted regression:</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><span style="font-size: 20px;"> <span class="math display">\[v_{x}(S) \approx \varphi_{x}(f, \emptyset) + \sum_{d=1}^{D} \mathbb{1}_{d \in S}\varphi_{x}(f, d)\]</span> </span></p>
</div><div class="column" style="width:50%;">
<p><span style="font-size: 18px;"> <img width="400" src="figures/shapley_kernel_regression.png"> <span class="math inline">\(\mathbb{1}_{d \in S}\)</span> is known. Linear regression with unknown <span class="math inline">\(\varphi_{x}(f, d)\)</span>. Compute <span class="math inline">\(v_{x}(S)\)</span> on sampled subsets. </span></p>
</div>
</div>
<hr>
</section>
<section id="kernel-reweighting" class="level3">
<h3 class="anchored" data-anchor-id="kernel-reweighting">Kernel Reweighting</h3>
<p>Each row: subset <span class="math inline">\(S\)</span> of features.</p>
<p><span class="math display">\[v_{x}(S) \approx \varphi_{x}(f, \emptyset) + \sum_{d=1}^{D} \mathbb{1}_{d \in S}\varphi_{x}(f, d)\]</span></p>
<p>Weight for subset <span class="math inline">\(S\)</span>: <span class="math display">\[\frac{D - 1}{\binom{D}{|S|} |S|(D - |S|)}\]</span></p>
<p>Weighted regression recovers exact Shapley values <span class="math inline">\(\varphi_{x}(f, d)\)</span>.</p>
<hr>
</section>
<section id="exercise-kernelshap-code" class="level3">
<h3 class="anchored" data-anchor-id="exercise-kernelshap-code">Exercise: KernelSHAP Code</h3>
<hr>
</section>
<section id="amortization" class="level2">
<h2 class="anchored" data-anchor-id="amortization">Amortization</h2>
<hr>
<section id="the-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-problem">The Problem</h3>
<p>KernelSHAP runs per-instance: thousands of model evaluations per data point.</p>
<p>For millions of samples, this cost dominates.</p>
<hr>
</section>
<section id="amortized-explainer" class="level3">
<h3 class="anchored" data-anchor-id="amortized-explainer">Amortized Explainer</h3>
<p>Train explainer model <span class="math inline">\(g_\theta(x)\)</span> to predict <span class="math inline">\(\phi(x)\)</span> directly.</p>
<p><strong>Speed</strong>. One forward pass after training.</p>
<p><strong>Generalization</strong>. Exploits similarities between data points.</p>
<hr>
</section>
<section id="training-with-noisy-labels" class="level3">
<h3 class="anchored" data-anchor-id="training-with-noisy-labels">Training with Noisy Labels</h3>
<p>Training requires labels <span class="math inline">\(\phi(x)\)</span>, which we want to avoid computing.</p>
<p><strong>Stochastic amortization</strong>. Use cheap, noisy estimates <span class="math inline">\(\tilde{a}(b)\)</span>:</p>
<p><span class="math display">\[\tilde{\mathcal{L}}_{reg}(\theta) = \mathbb{E}[\|a(b;\theta) - \tilde{a}(b)\|^{2}]\]</span></p>
<p>Use high-variance estimates (KernelSHAP with 1-5 samples) as targets.</p>
<hr>
</section>
<section id="why-noisy-labels-work" class="level3">
<h3 class="anchored" data-anchor-id="why-noisy-labels-work">Why Noisy Labels Work</h3>
<ul>
<li><p>If <span class="math inline">\(\mathbb{E}[\tilde{a}(b) \mid b] = a(b)\)</span>, the model converges to correct attributions.</p></li>
<li><p>Slows convergence (high variance) but is unbiased.</p></li>
</ul>
<hr>
</section>
<section id="when-to-apply" class="level3">
<h3 class="anchored" data-anchor-id="when-to-apply">When to Apply</h3>
<p>Amortization beats per-example computation for <span class="math inline">\(n &gt; 1000\)</span>.</p>
<p>(add a figure)</p>
<hr>
</section>
</section>
<section id="drosophila-development" class="level2">
<h2 class="anchored" data-anchor-id="drosophila-development">Drosophila Development</h2>
<hr>
<section id="setup" class="level3">
<h3 class="anchored" data-anchor-id="setup">Setup</h3>
<p>Analyze <strong>enhancers</strong>. DNA sequences regulating gene expression in <em>Drosophila</em> embryos.</p>
<p><strong>Target</strong>. Enhancer status (1 vs.&nbsp;0)?</p>
<p><strong>Features</strong>. Transcription factor and chromatin mark levels.</p>
<hr>
</section>
<section id="model" class="level3">
<h3 class="anchored" data-anchor-id="model">Model</h3>
<p><strong>Gradient Boosting</strong> using the <code>mboost</code> package. Tuned as in previous case studies.</p>
<p><strong>Tuning</strong>. 3-fold CV optimizes learning rate (<code>nu</code>) and iterations (<code>mstop</code>) by AUC.</p>
<hr>
</section>
<section id="model-performance" class="level3">
<h3 class="anchored" data-anchor-id="model-performance">Model Performance</h3>
<p>Before interpreting, verify model quality.</p>
<p><strong>AUC</strong>. High values indicate learned biological signals.</p>
<p><strong>Calibration</strong>. Histogram of predicted probabilities per class.</p>
<hr>
</section>
<section id="global-importance" class="level3">
<h3 class="anchored" data-anchor-id="global-importance">Global Importance</h3>
<p><code>sv_importance</code> identifies influential transcription factors.</p>
<p><strong>Bicoid (<code>bcd2</code>)</strong>. High values increase SHAP → major activator.</p>
<p><strong>Twist (<code>twi2</code>)</strong>. Strong positive impact, consistent with known role.</p>
<hr>
</section>
<section id="local-explanations" class="level3">
<h3 class="anchored" data-anchor-id="local-explanations">Local Explanations</h3>
<p><code>sv_force</code> explains individual sequences.</p>
<p>Each prediction: balance of “pushes” from different genes.</p>
<p>Sequence may be enhancer primarily due to high <code>bcd2</code>, even if <code>twi2</code> low.</p>
<hr>
</section>
<section id="exercise-marginal-vs.-conditional" class="level3">
<h3 class="anchored" data-anchor-id="exercise-marginal-vs.-conditional">Exercise: Marginal vs.&nbsp;Conditional</h3>
</section>
</section>
</section>
<section id="alternative-views-of-shap" class="level1">
<h1>12. Alternative Views of SHAP</h1>
<ul>
<li>the sensitivity paradox</li>
<li>computing <span class="math inline">\(v_x(S)\)</span> two ways</li>
<li>software implementations</li>
</ul>
<hr>
<section id="learning-outcomes-10" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-10">Learning outcomes</h3>
<ol type="1">
<li>Compare the competing definitions of interpretable machine learning, the motivations behind them, and metrics that can be used to quantify whether they have been met.</li>
<li>Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks</li>
</ol>
<hr>
</section>
<section id="recall-shap-formula" class="level3">
<h3 class="anchored" data-anchor-id="recall-shap-formula">Recall: SHAP Formula</h3>
<p>Previously we defined:</p>
<p><span class="math display">\[v_x(S) = \mathbf{E}_{p(x'_{S^C} \mid x_S)}[f(x_S, x'_{S^C})]\]</span></p>
<p><span class="math display">\[\varphi_x(f, i) = \frac{1}{D} \sum_{d=1}^D \frac{1}{\binom{D-1}{d-1}} \sum_{S \in S_d(i)} [v_x(S) - v_x(S \setminus \{i\})]\]</span></p>
<p><strong>Today.</strong> What is hidden behind <span class="math inline">\(p(x'_{S^C} \mid x_S)\)</span>?</p>
<hr>
</section>
<section id="motivation" class="level3">
<h3 class="anchored" data-anchor-id="motivation">Motivation</h3>
<p><strong>Setup.</strong> <span class="math inline">\(f(x_1, x_2) = x_1\)</span> where <span class="math inline">\(X_1 \equiv X_2\)</span> (perfectly correlated).</p>
<p><strong>Question.</strong> What is <span class="math inline">\(\varphi_x(f, 2)\)</span>?</p>
<p><strong>Intuition.</strong> <span class="math inline">\(\varphi_x(f, 2) = 0\)</span> since <span class="math inline">\(f\)</span> ignores <span class="math inline">\(x_2\)</span>.</p>
<p><strong>Reality.</strong> Two software packages disagree.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>shap_cond <span class="ot">&lt;-</span> shapr<span class="sc">::</span><span class="fu">explain</span>(model, x, <span class="at">approach =</span> <span class="st">"gaussian"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># phi_2 ≈ 0.25 ≠ 0</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>shap_marg <span class="ot">&lt;-</span> fastshap<span class="sc">::</span><span class="fu">explain</span>(model, <span class="at">X =</span> X_train, <span class="at">newdata =</span> x)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># phi_2 ≈ 0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is not a bug. They compute different quantities.</p>
<hr>
</section>
<section id="two-definitions" class="level3">
<h3 class="anchored" data-anchor-id="two-definitions">Two Definitions</h3>
<p><strong>Conditional.</strong> <span class="math display">\[v_x^{\text{cond}}(S) = \mathbf{E}[f(x_S, X_{S^C}) \mid X_S = x_S]\]</span></p>
<p>Sample <span class="math inline">\(X_{S^C}\)</span> from training data where <span class="math inline">\(X_S \approx x_S\)</span>.</p>
<p><strong>Marginal.</strong> <span class="math display">\[v_x^{\text{marg}}(S) = \mathbf{E}[f(x_S, X_{S^C})]\]</span></p>
<p>Sample <span class="math inline">\(X_{S^C}\)</span> from all training data, ignoring <span class="math inline">\(x_S\)</span>.</p>
<hr>
</section>
<section id="manual-calculation-binary-case" class="level3">
<h3 class="anchored" data-anchor-id="manual-calculation-binary-case">Manual Calculation: Binary Case</h3>
<p><strong>Setup.</strong></p>
<ul>
<li><span class="math inline">\(f(x_1, x_2) = x_1\)</span></li>
<li><span class="math inline">\(P(X_1=1, X_2=1) = 0.5\)</span>, <span class="math inline">\(P(X_1=0, X_2=0) = 0.5\)</span></li>
<li>Explain <span class="math inline">\((x_1, x_2) = (1, 1)\)</span></li>
</ul>
<hr>
</section>
<section id="conditional-approach" class="level3">
<h3 class="anchored" data-anchor-id="conditional-approach">Conditional Approach</h3>
<p><span class="math display">\[v^{\text{cond}}(\emptyset) = \mathbf{E}[X_1] = 0.5\]</span> <span class="math display">\[v^{\text{cond}}(\{2\}) = \mathbf{E}[X_1 \mid X_2 = 1] = 1\]</span> <span class="math display">\[v^{\text{cond}}(\{1\}) = 1\]</span> <span class="math display">\[v^{\text{cond}}(\{1,2\}) = 1\]</span></p>
<hr>
</section>
<section id="conditional-approach-1" class="level3">
<h3 class="anchored" data-anchor-id="conditional-approach-1">Conditional Approach</h3>
<p>Contributions: <span class="math display">\[C(2 \mid \emptyset) = 1 - 0.5 = 0.5\]</span> <span class="math display">\[C(2 \mid \{1\}) = 1 - 1 = 0\]</span></p>
<p>Result: <span class="math inline">\(\varphi_2^{\text{cond}} = 0.25 \neq 0\)</span></p>
<hr>
</section>
<section id="marginal-approach" class="level3">
<h3 class="anchored" data-anchor-id="marginal-approach">Marginal Approach</h3>
<p><span class="math display">\[v^{\text{marg}}(\emptyset) = 0.5\]</span> <span class="math display">\[v^{\text{marg}}(\{2\}) = \mathbf{E}[X_1] = 0.5\]</span> <span class="math display">\[v^{\text{marg}}(\{1\}) = 1\]</span> <span class="math display">\[v^{\text{marg}}(\{1,2\}) = 1\]</span></p>
<hr>
</section>
<section id="marginal-approach-1" class="level3">
<h3 class="anchored" data-anchor-id="marginal-approach-1">Marginal Approach</h3>
<p>Contributions: <span class="math display">\[C(2 \mid \emptyset) = 0\]</span> <span class="math display">\[C(2 \mid \{1\}) = 0\]</span></p>
<p>Result: <span class="math inline">\(\varphi_2^{\text{marg}} = 0\)</span></p>
<hr>
</section>
<section id="sensitivity-failure" class="level3">
<h3 class="anchored" data-anchor-id="sensitivity-failure">Sensitivity Failure</h3>
<p><strong>Lemma.</strong> Using conditional expectations, <span class="math inline">\(\varphi_i \neq 0\)</span> does not imply <span class="math inline">\(f\)</span> depends on <span class="math inline">\(x_i\)</span>.</p>
<p><strong>Reason.</strong> Observing <span class="math inline">\(X_2 = x_2\)</span> provides information about <span class="math inline">\(X_1\)</span>.</p>
<hr>
</section>
<section id="exercise-geometric-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="exercise-geometric-interpretation">Exercise: Geometric Interpretation</h3>
<p>Recall Lecture 11’s visualization of <span class="math inline">\(v_x(S)\)</span> as moving through feature space.</p>
<p>When computing <span class="math inline">\(v_x(\{1\})\)</span>:</p>
<ol type="1">
<li><strong>Conditional.</strong> Where do we sample <span class="math inline">\((x_1, X_2)\)</span> from?</li>
<li><strong>Marginal.</strong> Where do we sample <span class="math inline">\((x_1, X_2)\)</span> from?</li>
<li>Which produces “impossible” combinations like <span class="math inline">\((x_1=1, x_2=0)\)</span>?</li>
</ol>
<p>Sketch the sampling regions in 2D feature space.</p>
<hr>
</section>
<section id="linear-models-2" class="level3">
<h3 class="anchored" data-anchor-id="linear-models-2">Linear Models</h3>
<p>For <span class="math inline">\(f(x) = \alpha_0 + \sum_i \alpha_i x_i\)</span>:</p>
<p><span class="math display">\[\varphi_i = \alpha_i(x_i - \mathbf{E}[X_i])\]</span></p>
<p>This is the true attribution.</p>
<hr>
</section>
<section id="gaussian-features" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-features">Gaussian Features</h3>
<p><span class="math inline">\(X_1, X_2 \sim N(0, \Sigma)\)</span> with <span class="math inline">\(\Sigma_{12} = \rho\)</span></p>
<p><span class="math display">\[f(x_1, x_2) = \alpha_1 x_1 + \alpha_2 x_2\]</span></p>
<p>Set <span class="math inline">\(\alpha_1 = 0\)</span>, so <span class="math inline">\(X_1\)</span> is irrelevant. Since <span class="math display">\[\mathbf{E}[X_1 \mid X_2 = x_2] = \rho x_2\]</span></p>
<p>we have <span class="math display">\[v^{\text{cond}}(\{1\}) = \alpha_1 x_1 + \alpha_2 \mathbf{E}[X_2 \mid x_1] = 0 + \alpha_2 \rho x_1\]</span></p>
<p>which is not zero even though <span class="math inline">\(\alpha_1 = 0\)</span>. Attribution leaks from <span class="math inline">\(X_2\)</span> to <span class="math inline">\(X_1\)</span> through <span class="math inline">\(\rho\)</span>.</p>
<hr>
</section>
<section id="empirical-study-human-activity-data" class="level3">
<h3 class="anchored" data-anchor-id="empirical-study-human-activity-data">Empirical Study: Human Activity Data</h3>
<p>Dataset: UCI Human Activity Recognition (10,299 observations, 561 features)</p>
<ol type="1">
<li>Randomly select 4 features</li>
<li>Train linear model: 3 features to predict 4th</li>
<li>Compare SHAP to true coefficients <span class="math inline">\(\alpha_j(x_j - \mathbf{E}[X_j])\)</span></li>
<li>Repeat 1000 times</li>
</ol>
<p><img src="figures/janzing_figure5.png" width="700"></p>
<hr>
</section>
<section id="r-fastshap" class="level3">
<h3 class="anchored" data-anchor-id="r-fastshap">R fastshap</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fastshap)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>shap_vals <span class="ot">&lt;-</span> <span class="fu">explain</span>(model, <span class="at">X =</span> X_train, <span class="at">newdata =</span> X_test, <span class="at">nsim =</span> <span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li>For each feature <span class="math inline">\(j\)</span>, sample <span class="math inline">\(M\)</span> random coalitions <span class="math inline">\(S \not\ni j\)</span></li>
<li>For each coalition, compute:
<ul>
<li><span class="math inline">\(v_x(S)\)</span>: sample from <span class="math inline">\(X_{\bar{S}}\)</span> background</li>
<li><span class="math inline">\(v_x(S \cup \{j\})\)</span>: sample from <span class="math inline">\(X_{\bar{S} \setminus \{j\}}\)</span> background</li>
</ul></li>
<li>Average: <span class="math inline">\(\phi_j \approx \frac{1}{M} \sum_{m=1}^M [v_x(S_m \cup \{j\}) - v_x(S_m)]\)</span></li>
</ol>
<p>Direct Monte Carlo approximation of the Shapley formula. Uses marginal expectation.</p>
<hr>
</section>
<section id="python-kernelexplainer" class="level3">
<h3 class="anchored" data-anchor-id="python-kernelexplainer">python KernelExplainer</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.KernelExplainer(model.predict, X_background)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer.shap_values(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li>Sample subset <span class="math inline">\(S \subseteq \{1, \ldots, D\}\)</span></li>
<li>Create synthetic data: <span class="math inline">\((x_S, X_{\bar{S}}^{(k)})\)</span> for background samples</li>
<li>Compute <span class="math inline">\(v_x(S) \approx \frac{1}{B} \sum_{b=1}^B f(x_S, X_{\bar{S}}^{(b)})\)</span></li>
<li>Solve weighted regression</li>
</ol>
<p>Also uses marginal expectation.</p>
<hr>
</section>
<section id="r-shapr" class="level3">
<h3 class="anchored" data-anchor-id="r-shapr">R shapr</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(shapr)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>explanation <span class="ot">&lt;-</span> <span class="fu">explain</span>(model, x_explain, <span class="at">approach =</span> <span class="st">"gaussian"</span>, <span class="at">x_train =</span> X_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li>Estimate covariance <span class="math inline">\(\hat{\Sigma}\)</span> from training data</li>
<li>For subset <span class="math inline">\(S\)</span>, compute conditional distribution parameters:</li>
</ol>
<p><span class="math display">\[\mu_{\bar{S} \mid S} = \mu_{\bar{S}} + \Sigma_{\bar{S}S} \Sigma_{SS}^{-1} (x_S - \mu_S)\]</span> <span class="math display">\[\Sigma_{\bar{S} \mid S} = \Sigma_{\bar{S}\bar{S}} - \Sigma_{\bar{S}S} \Sigma_{SS}^{-1} \Sigma_{S\bar{S}}\]</span></p>
<ol start="3" type="1">
<li>Sample <span class="math inline">\(X_{\bar{S}}^{(k)} \sim N(\mu_{\bar{S} \mid S}, \Sigma_{\bar{S} \mid S})\)</span></li>
<li>Compute <span class="math inline">\(v_x(S) \approx \frac{1}{K} \sum_k f(x_S, X_{\bar{S}}^{(k)})\)</span></li>
</ol>
<p>This uses conditional expectation.</p>
<hr>
</section>
<section id="exercise-implement-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="exercise-implement-from-scratch">Exercise: Implement from Scratch</h3>
<p>Complete this implementation:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>compute_v <span class="ot">&lt;-</span> <span class="cf">function</span>(model, x, S, X_train, <span class="at">approach =</span> <span class="st">"marginal"</span>) {</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  n_samples <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  predictions <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_samples)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_samples) {</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    x_synthetic <span class="ot">&lt;-</span> x</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (approach <span class="sc">==</span> <span class="st">"marginal"</span>) {</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Sample X_{\bar{S}} from all training data</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>      idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(X_train), <span class="dv">1</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>      x_synthetic[<span class="sc">-</span>S] <span class="ot">&lt;-</span> X_train[idx, <span class="sc">-</span>S]</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> <span class="cf">if</span> (approach <span class="sc">==</span> <span class="st">"conditional"</span>) {</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Sample from conditional distribution</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>      <span class="co"># </span><span class="al">TODO</span><span class="co">: Compute μ_{\bar{S}|S} and Σ_{\bar{S}|S}</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>      <span class="co">#mu_cond &lt;- ?</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Sigma_cond &lt;-</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>      x_synthetic[<span class="sc">-</span>S] <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="dv">1</span>, mu_cond, Sigma_cond)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    predictions[k] <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x_synthetic)))</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(predictions)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Test on binary example</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) x[<span class="dv">1</span>]  <span class="co"># Ignores x[2]</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>X_train <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))  <span class="co"># Perfect correlation</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>v_marg_2 <span class="ot">&lt;-</span> <span class="fu">compute_v</span>(f, x, <span class="at">S =</span> <span class="fu">c</span>(<span class="dv">2</span>), X_train, <span class="st">"marginal"</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>v_cond_2 <span class="ot">&lt;-</span> <span class="fu">compute_v</span>(f, x, <span class="at">S =</span> <span class="fu">c</span>(<span class="dv">2</span>), X_train, <span class="st">"conditional"</span>)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a><span class="co"># v_marg_2 should equal E[X_1] = 0.5</span></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a><span class="co"># v_cond_2 should equal E[X_1 | X_2=1] = 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="shap-and-interventions" class="level1">
<h1>13. SHAP and Interventions</h1>
<ul>
<li>causal graphs and the do-operator</li>
<li>marginal SHAP as intervention</li>
<li>code examples</li>
</ul>
<hr>
<section id="learning-outcomes-11" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-11">Learning outcomes</h3>
<ol type="1">
<li>Compare the competing definitions of interpretable machine learning, the motivations behind them, and metrics that can be used to quantify whether they have been met.</li>
<li>Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks</li>
</ol>
<hr>
</section>
<section id="causal-view" class="level2">
<h2 class="anchored" data-anchor-id="causal-view">Causal View</h2>
<hr>
<section id="recall" class="level3">
<h3 class="anchored" data-anchor-id="recall">Recall</h3>
<p><strong>Setup.</strong> Algorithm <span class="math inline">\(f(x_1, x_2) = x_1\)</span> trained on data where <span class="math inline">\(X_1 \equiv X_2\)</span>.</p>
<p><strong>Conditional SHAP</strong> attributes non-zero importance to <span class="math inline">\(X_2\)</span>.</p>
<p><strong>Marginal SHAP</strong> correctly gives <span class="math inline">\(\varphi_2 = 0\)</span>.</p>
<p><strong>Question.</strong> Should we have preferred Marginal SHAP all along?</p>
<hr>
</section>
<section id="causal-view-of-post-hoc-explanation" class="level3">
<h3 class="anchored" data-anchor-id="causal-view-of-post-hoc-explanation">Causal View of Post-hoc Explanation</h3>
<p>The function-to-explain can be viewed as a causal graph.</p>
<p>(draw causal graph with X and <span class="math inline">\(\tilde{X}\)</span>)</p>
<p>This distinguishes between observed data <span class="math inline">\(\tilde{X}\)</span> and function inputs <span class="math inline">\(X\)</span>.</p>
<p>When we set <span class="math inline">\(X_1 = x_1\)</span> and let other inputs vary, how does <span class="math inline">\(Y\)</span> change? It depends on how we vary the other inputs.</p>
<hr>
</section>
<section id="observation-vs.-intervention" class="level3">
<h3 class="anchored" data-anchor-id="observation-vs.-intervention">Observation vs.&nbsp;Intervention</h3>
<p><strong>Observational (seeing).</strong> <span class="math inline">\(\mathbf{E}[Y \mid X_1 = x_1]\)</span></p>
<p>“Among data with <span class="math inline">\(X_1 = x_1\)</span>, what is average <span class="math inline">\(Y\)</span>?”</p>
<p><strong>Interventional (doing).</strong> <span class="math inline">\(\mathbf{E}[Y \mid \text{do}(X_1 = x_1)]\)</span></p>
<p>“If we force <span class="math inline">\(X_1 = x_1\)</span> for each of this function’s inputs, what is <span class="math inline">\(Y\)</span>?”</p>
<p>These differ when features are correlated in training data.</p>
<hr>
</section>
<section id="backdoor-criterion" class="level3">
<h3 class="anchored" data-anchor-id="backdoor-criterion">Backdoor Criterion</h3>
<p>Consider:</p>
<pre><code>       Z
      / \
     ↓   ↓
   X₁ → Y ← X₂</code></pre>
<p><strong>Backdoor formula.</strong> <span class="math display">\[\mathbf{E}[Y \mid \text{do}(X_1 = x_1)] = \int \mathbf{E}[Y \mid x_1, x_2] p(x_2) \, dx_2\]</span></p>
<p>The do-operator deletes incoming edges to <span class="math inline">\(X_1\)</span>. Edge <span class="math inline">\(Z \to X_1\)</span> is severed.</p>
<hr>
</section>
<section id="backdoor-criterion-1" class="level3">
<h3 class="anchored" data-anchor-id="backdoor-criterion-1">Backdoor Criterion</h3>
<p>Contrast this with the formula for usual conditional expectation.</p>
<p><span class="math display">\[\mathbf{E}[Y \mid \text{do}(X_1 = x_1)] = \int \mathbf{E}[Y \mid x_1, x_2] p(x_2) \, dx_2\]</span></p>
<p><span class="math display">\[\mathbf{E}[Y \mid X_1 = x_1] = \int \mathbf{E}[Y \mid x_1, x_2] p(x_2 \vert x_{1}) \, dx_2\]</span></p>
<hr>
</section>
<section id="application-to-post-hoc-explanation" class="level3">
<h3 class="anchored" data-anchor-id="application-to-post-hoc-explanation">Application to Post-hoc Explanation</h3>
<p>Algorithm <span class="math inline">\(f\)</span> has no confounders:</p>
<p>(draw causal graph with X and <span class="math inline">\(\tilde{X}\)</span>)</p>
<p><span class="math display">\[\mathbf{E}[Y \mid \text{do}(X_S = x_S)] = \int f(x_S, x_{\bar{S}}) p(x_{\bar{S}}) \, dx_{\bar{S}}\]</span></p>
<p>No backdoor paths exist, so no conditioning is needed and this is the marginal expectation.</p>
<hr>
</section>
<section id="interventional-view" class="level3">
<h3 class="anchored" data-anchor-id="interventional-view">Interventional View</h3>
<p><strong>Algebraic.</strong> <span class="math display">\[\mathbf{E}[f(x_S, X_{\bar{S}})] = \int f(x_S, x_{\bar{S}}) p(x_{\bar{S}}) \, dx_{\bar{S}}\]</span></p>
<p><strong>Causal.</strong></p>
<p>Intervention <span class="math inline">\(\text{do}(X_S = x_S)\)</span> deletes any edges into <span class="math inline">\(X_S\)</span>. Distribution <span class="math inline">\(p(x_{\bar{S}})\)</span> unchanged.</p>
<p><strong>Computational.</strong></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> []</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    x_complement <span class="op">=</span> sample_from_training_data()  <span class="co"># Independent of x_S</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    predictions.append(f(x_S, x_complement))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> mean(predictions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="conditional-view" class="level3">
<h3 class="anchored" data-anchor-id="conditional-view">Conditional View</h3>
<p><strong>Algebraic.</strong> <span class="math display">\[\mathbf{E}[f(x_S, X_{\bar{S}}) \mid X_S = x_S] = \int f(x_S, x_{\bar{S}}) p(x_{\bar{S}} \mid x_S) \, dx_{\bar{S}}\]</span></p>
<p><strong>Causal.</strong> Asks “what do we see when <span class="math inline">\(X_S = x_S\)</span>?” not “what happens when we set <span class="math inline">\(X_S = x_S\)</span>?”</p>
<p><strong>Computational.</strong></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> []</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    x_complement <span class="op">=</span> sample_conditional_on(x_S)  <span class="co"># Depends on x_S</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    predictions.append(f(x_S, x_complement))</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> mean(predictions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="two-causal-graphs" class="level3">
<h3 class="anchored" data-anchor-id="two-causal-graphs">Two Causal Graphs</h3>
<p>Janzig et al.&nbsp;(2020) argue that we should focus on the “Algorithm” causal structure.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Data generation (real world).</strong></p>
<pre><code>    Z̃ (confounders)
   / \
  ↓   ↓
 X̃₁  X̃₂</code></pre>
<p>How do real-world features relate?</p>
</div><div class="column" style="width:50%;">
<p><strong>Algorithm (computation).</strong></p>
<pre><code>X₁ → Y = f(X)
X₂ →</code></pre>
</div><p>How does the prediction function process inputs?</p>
</div>
<hr>
</section>
<section id="formal-separation" class="level3">
<h3 class="anchored" data-anchor-id="formal-separation">Formal Separation</h3>
<p>Janzing et al.&nbsp;(2020): - <span class="math inline">\(\tilde{X}_j\)</span>: real-world feature - <span class="math inline">\(X_j\)</span>: algorithm input</p>
<p>Usually <span class="math inline">\(X_j = \tilde{X}_j\)</span>, but they are distinct objects with distinct causal graphs.</p>
<p><strong>Reality.</strong> Cannot change <span class="math inline">\(\tilde{X}_1\)</span> without affecting <span class="math inline">\(\tilde{X}_2\)</span> (shared causes).</p>
<p><strong>Algorithm.</strong> Can evaluate <code>f(x_1, x_2)</code> for any values.</p>
<p>SHAP explains <span class="math inline">\(f\)</span>, so arguably should use the algorithm’s causal structure.</p>
<hr>
</section>
<section id="binary-data-example" class="level3">
<h3 class="anchored" data-anchor-id="binary-data-example">Binary Data Example</h3>
<p><strong>Setup.</strong> <span class="math inline">\(f(x_1, x_2) = x_1\)</span> where <span class="math inline">\(X_1 \equiv X_2\)</span> in data.</p>
<p><strong>Marginal.</strong> <span class="math display">\[v^{\text{marg}}(\{2\}) = \int x_1 p(x_1) dx_1 = \mathbf{E}[X_1]\]</span></p>
<p><strong>Conditional.</strong> <span class="math display">\[v^{\text{cond}}(\{2\}) = \int x_1 p(x_1 \mid x_2) dx_1 = x_2\]</span></p>
<p>Conditional approach “sees” correlation. Marginal approach intervenes in the algorithm.</p>
<hr>
</section>
</section>
<section id="code-examples" class="level2">
<h2 class="anchored" data-anchor-id="code-examples">Code Examples</h2>
<hr>
<section id="binary-example" class="level3">
<h3 class="anchored" data-anchor-id="binary-example">Binary Example</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df_binary <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">sample</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>, n_obs, <span class="at">replace =</span> <span class="cn">TRUE</span>),</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> x1 <span class="sc">+</span> <span class="fu">rnorm</span>(n_obs, <span class="dv">0</span>, <span class="fl">0.1</span>),</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> x1</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>X_binary <span class="ot">&lt;-</span> df_binary <span class="sc">|&gt;</span> dplyr<span class="sc">::</span><span class="fu">select</span>(x1, x2)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>fit_binary <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> df_binary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Algorithm: <span class="math inline">\(\hat{y} = \hat{\alpha}_0 + \hat{\alpha}_1 x_1 + \hat{\alpha}_2 x_2\)</span></p>
<p>True causal effect of <span class="math inline">\(X_2\)</span> on <span class="math inline">\(Y\)</span>: zero.</p>
<hr>
</section>
<section id="marginal-shap" class="level3">
<h3 class="anchored" data-anchor-id="marginal-shap">Marginal SHAP</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>shap_marg <span class="ot">&lt;-</span> fastshap<span class="sc">::</span><span class="fu">explain</span>(</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  fit,</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">X =</span> X,</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> predict</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(shap_marg[, <span class="st">"x2"</span>]))  <span class="co"># ≈ 0.01</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Changing <code>x2</code> (while sampling <code>x1</code> independently) does not affect predictions.</p>
<p>Correctly identifies <span class="math inline">\(X_2\)</span> as irrelevant.</p>
<hr>
</section>
<section id="conditional-shap" class="level3">
<h3 class="anchored" data-anchor-id="conditional-shap">Conditional SHAP</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>shap_cond <span class="ot">&lt;-</span> shapr<span class="sc">::</span><span class="fu">explain</span>(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  fit,</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_explain =</span> X,</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_train =</span> X,</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">approach =</span> <span class="st">"gaussian"</span>,</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">phi0 =</span> <span class="fu">mean</span>(df_binary<span class="sc">$</span>y)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>)<span class="sc">$</span>shapley_values_est</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(shap_cond<span class="sc">$</span>x2))  <span class="co"># ≈ 0.12</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Observing <code>x2 = 1</code> implies <code>x1 ≈ 1</code>, so <code>x2</code> appears predictive.</p>
<p>Correlation <span class="math inline">\(\neq\)</span> causal relevance in the algorithm.</p>
<hr>
</section>
<section id="gaussian-example" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-example">Gaussian Example</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># covariates</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>X_gauss <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(n_obs_g, <span class="at">mu =</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_features), <span class="at">Sigma =</span> sigma_mat) <span class="sc">|&gt;</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>(<span class="at">.name_repair =</span> <span class="sc">~</span><span class="fu">paste0</span>(<span class="st">"X"</span>, <span class="dv">1</span><span class="sc">:</span>n_features))</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co"># generate response</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>true_alphas <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_features)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>true_alphas[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>y_gauss <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(X_gauss) <span class="sc">%*%</span> true_alphas</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
</section>
<section id="true-attribution" class="level3">
<h3 class="anchored" data-anchor-id="true-attribution">True Attribution</h3>
<p>Linear models: <span class="math inline">\(\varphi_j = \alpha_j (x_j - \mathbf{E}[X_j])\)</span></p>
<p>Since <span class="math inline">\(\alpha_1 = 0\)</span>, we have <span class="math inline">\(\varphi_1 = 0\)</span> for all observations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>fit_gauss <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> df_gauss)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit_gauss)[<span class="sc">-</span><span class="dv">1</span>]  <span class="co"># X1 coefficient ≈ 0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Algorithm does not use <span class="math inline">\(X_1\)</span>.</p>
<hr>
</section>
<section id="marginal-shap-gaussian" class="level3">
<h3 class="anchored" data-anchor-id="marginal-shap-gaussian">Marginal SHAP (Gaussian)</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>shap_marg_gauss <span class="ot">&lt;-</span> fastshap<span class="sc">::</span><span class="fu">explain</span>(</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  fit_gauss,</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">X =</span> X_gauss,</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">pred_wrapper =</span> predict</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(shap_marg_gauss[, <span class="st">"X1"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Small error due to finite sampling. Correctly near zero.</p>
<hr>
</section>
<section id="conditional-shap-gaussian" class="level3">
<h3 class="anchored" data-anchor-id="conditional-shap-gaussian">Conditional SHAP (Gaussian)</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>shap_cond_gauss <span class="ot">&lt;-</span> shapr<span class="sc">::</span><span class="fu">explain</span>(</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  fit_gauss,</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_explain =</span> X_gauss,</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_train =</span> X_gauss,</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">approach =</span> <span class="st">"gaussian"</span>,</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">phi0 =</span> <span class="fu">mean</span>(df_gauss<span class="sc">$</span>y)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>)<span class="sc">$</span>shapley_values_est</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(shap_cond_gauss<span class="sc">$</span>X1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Larger error. Attributes importance to <span class="math inline">\(X_1\)</span> due to correlation with other features.</p>
<hr>
</section>
<section id="sampling" class="level3">
<h3 class="anchored" data-anchor-id="sampling">Sampling</h3>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Marginal sampling will extrapolate – we test the algorithm on all possible inputs.</p>
<hr>
</section>
<section id="attributions-histogram" class="level3">
<h3 class="anchored" data-anchor-id="attributions-histogram">Attributions Histogram</h3>
<p><img src="figures/intervention_histogram_attrs.svg"></p>
<hr>
</section>
<section id="exercise-5" class="level3">
<h3 class="anchored" data-anchor-id="exercise-5">Exercise</h3>
</section>
</section>
</section>
<section id="deep-learning-setup" class="level1">
<h1>15. Deep Learning Setup</h1>
<ul>
<li><p>motivating case study</p></li>
<li><p>perceptron</p></li>
<li><p>data structures</p></li>
</ul>
<hr>
<section id="learning-outcomes-12" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-12">Learning outcomes</h3>
<ul>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="reading-9" class="level2">
<h2 class="anchored" data-anchor-id="reading-9">Reading</h2>
<ul>
<li><strong>Chapter 12 (excluding 12.5)</strong>.</li>
<li><strong>Getting Started with PyTorch</strong></li>
</ul>
<hr>
<section id="problem" class="level3">
<h3 class="anchored" data-anchor-id="problem">Problem</h3>
<p>Identify regions of rapid land use conversion (important in climate change-related work).</p>
<p>Data: 27K Sentinel-2 images (10m/pixel) - 34 European countries, 2015-2017 - Low cloud cover only - 10 land use classes</p>
<hr>
</section>
<section id="statistical-formulation-2" class="level3">
<h3 class="anchored" data-anchor-id="statistical-formulation-2">Statistical Formulation</h3>
<p><strong>Label</strong>. <span class="math inline">\(y \in \{0,1\}^{10}\)</span> (one-hot encoding of 10 classes)</p>
<p><strong>Input</strong>. <span class="math inline">\(x \in \mathbf{R}^{3 \times 64 \times 64}\)</span> (RGB patch)</p>
<p><strong>Model</strong>. <span class="math inline">\(f(x; \theta) \to \Delta^9\)</span> (probability simplex)</p>
<p><strong>Objective</strong>. <span class="math inline">\(\min_\theta \mathbf{E}[\mathcal{L}(f(x; \theta), y)]\)</span></p>
<hr>
</section>
</section>
<section id="perceptrons" class="level2">
<h2 class="anchored" data-anchor-id="perceptrons">Perceptrons</h2>
<hr>
<section id="single-neuron" class="level3">
<h3 class="anchored" data-anchor-id="single-neuron">Single Neuron</h3>
<p>Perceptron = Linear map + Nonlinearity</p>
<p><span class="math display">\[z = w^\mathsf{T} x + b \quad \text{(affine transformation)}\]</span> <span class="math display">\[y = g(z) \quad \text{(activation)}\]</span></p>
<p>Common <span class="math inline">\(g\)</span>: step function, ReLU, sigmoid</p>
<hr>
</section>
<section id="linear-separation" class="level3">
<h3 class="anchored" data-anchor-id="linear-separation">Linear Separation</h3>
<p>Decision boundary: <span class="math inline">\(\{x : w^\mathsf{T} x + b = 0\}\)</span></p>
<p>Classification: <span class="math inline">\(\text{sign}(w^\mathsf{T} x + b)\)</span></p>
<p>Limitation: Can only separate linearly separable classes</p>
<hr>
</section>
<section id="objective-function" class="level3">
<h3 class="anchored" data-anchor-id="objective-function">Objective Function</h3>
<p>Learning = Optimization</p>
<p><span class="math display">\[\theta^* = \mathop{\mathrm{arg\,min}}_\theta \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f(x_i; \theta), y_i)\]</span></p>
<p>Update rule: <span class="math inline">\(\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}\)</span></p>
<p>where <span class="math inline">\(\eta\)</span> is step size (learning rate)</p>
<hr>
</section>
<section id="multilayer-perceptron" class="level3">
<h3 class="anchored" data-anchor-id="multilayer-perceptron">Multilayer Perceptron</h3>
<p>Composition of affine maps and nonlinearities:</p>
<p><span class="math display">\[\begin{aligned}
h_1 &amp;= g(W_1 x + b_1) \\
h_2 &amp;= g(W_2 h_1 + b_2) \\
&amp;\vdots \\
y &amp;= \text{softmax}(W_L h_{L-1} + b_L)
\end{aligned}\]</span></p>
<p>Key: <span class="math inline">\(g\)</span> enables non-convex decision boundaries</p>
<hr>
</section>
<section id="sequence-of-transformations" class="level3">
<h3 class="anchored" data-anchor-id="sequence-of-transformations">Sequence of Transformations</h3>
<p>Deep net as composition: <span class="math inline">\(f = f_L \circ \cdots \circ f_2 \circ f_1\)</span></p>
<p>Each <span class="math inline">\(f_i\)</span> transforms the geometry of data space - Early layers: Summarize local features (edges, textures) - Deep layers: Global structure (object categories)</p>
<p>Universal approximation: Sufficient depth + width <span class="math inline">\(\to\)</span> approximate any continuous function</p>
<hr>
</section>
<section id="activation-vs-parameters" class="level3">
<h3 class="anchored" data-anchor-id="activation-vs-parameters">Activation vs Parameters</h3>
<p><strong>Parameters</strong> <span class="math inline">\(\theta = (W, b)\)</span>: Learned during training</p>
<p><strong>Activations</strong> <span class="math inline">\(h_i(x)\)</span>: Computed during inference</p>
<p>Parameters: Slow (updated over many samples)</p>
<p>Activations: Fast (computed per sample)</p>
<p>Both are functions of data, at different udpating scales</p>
<hr>
</section>
<section id="exercise-6" class="level3">
<h3 class="anchored" data-anchor-id="exercise-6">Exercise</h3>
<hr>
</section>
</section>
<section id="data-structures" class="level2">
<h2 class="anchored" data-anchor-id="data-structures">Data Structures</h2>
<hr>
<section id="tensors" class="level3">
<h3 class="anchored" data-anchor-id="tensors">Tensors</h3>
<p>Tensor: Multidimensional array with structure</p>
<ul>
<li>rank 0: scalar</li>
<li>rank 1: vector</li>
<li>rank 2: matrix</li>
<li>rank <span class="math inline">\(n\)</span>: <span class="math inline">\(n\)</span>-array</li>
</ul>
<hr>
</section>
<section id="tensors-in-mlp-example" class="level3">
<h3 class="anchored" data-anchor-id="tensors-in-mlp-example">Tensors in MLP Example</h3>
<p>Layer dimensions:</p>
<ul>
<li>Input: <span class="math inline">\([N, d_{\text{in}}]\)</span> (batch <span class="math inline">\(\times\)</span> input features)</li>
<li>Weight: <span class="math inline">\([d_{\text{out}}, d_{\text{in}}]\)</span></li>
<li>Bias: <span class="math inline">\([d_{\text{out}}]\)</span></li>
<li>Output: <span class="math inline">\([N, d_{\text{out}}]\)</span> (batch <span class="math inline">\(\times\)</span> output features)</li>
</ul>
<p>Matrix multiplication: <span class="math inline">\(z = xW^\mathsf{T} + b\)</span></p>
<hr>
</section>
<section id="tensors-in-satellite-example" class="level3">
<h3 class="anchored" data-anchor-id="tensors-in-satellite-example">Tensors in Satellite Example</h3>
<p>EuroSAT tensor dimensions:</p>
<ul>
<li>Single image: <span class="math inline">\([3, 64, 64]\)</span> (channels <span class="math inline">\(\times\)</span> height <span class="math inline">\(\times\)</span> width)</li>
<li>Batch: <span class="math inline">\([16, 3, 64, 64]\)</span> (batch <span class="math inline">\(\times\)</span> C <span class="math inline">\(\times\)</span> H <span class="math inline">\(\times\)</span> W)</li>
</ul>
<hr>
</section>
<section id="tensors-in-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="tensors-in-pytorch">Tensors in PyTorch</h3>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and move to hardware accelerator</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">16</span>, <span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"mps"</span> <span class="cf">if</span> torch.backends.mps.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="datasets-and-loaders" class="level3">
<h3 class="anchored" data-anchor-id="datasets-and-loaders">Datasets and Loaders</h3>
<p><strong>Dataset</strong>. <span class="math inline">\((x_i, y_i)\)</span> pairs with <code>__getitem__</code> method</p>
<p><strong>DataLoader</strong>. Iterator over batches</p>
<p>Key parameters: - <code>batch_size</code>: Number of samples per gradient update - <code>shuffle</code>: Randomize order each epoch - <code>num_workers</code>: Parallel data loading</p>
<hr>
</section>
<section id="transforms" class="level3">
<h3 class="anchored" data-anchor-id="transforms">Transforms</h3>
<p>We will use these data augmentation steps:</p>
<p><strong>Training</strong>: <code>RandomCrop</code>, <code>HorizontalFlip</code> <span class="math inline">\(\to\)</span> variation</p>
<p><strong>Validation</strong>: <code>CenterCrop</code> <span class="math inline">\(\to\)</span> consistency</p>
<p><strong>Both</strong>: <code>ToTensor</code>, <code>Normalize</code> <span class="math inline">\(\to\)</span> standardization</p>
<p>Purpose: Increase effective dataset size, regularize</p>
<hr>
</section>
<section id="exercise-7" class="level3">
<h3 class="anchored" data-anchor-id="exercise-7">Exercise</h3>
<hr>
</section>
</section>
</section>
<section id="composing-deep-models" class="level1">
<h1>16. Composing Deep Models</h1>
<ul>
<li><p>common layers</p></li>
<li><p>training</p></li>
<li><p>application to case study</p></li>
</ul>
<hr>
<section id="learning-outcomes-13" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-13">Learning outcomes</h3>
<ul>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="reading-10" class="level3">
<h3 class="anchored" data-anchor-id="reading-10">Reading</h3>
<ul>
<li><strong>Chapter 12</strong>.</li>
<li><strong>Getting Started with PyTorch</strong></li>
</ul>
<hr>
</section>
<section id="linear-layers" class="level3">
<h3 class="anchored" data-anchor-id="linear-layers">Linear Layers</h3>
<p><code>nn.Linear(d_in, d_out)</code></p>
<p><strong>Implements</strong>. <span class="math inline">\(y = xW^\mathsf{T} + b\)</span></p>
<p>where <span class="math inline">\(W \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}\)</span>, <span class="math inline">\(b \in \mathbb{R}^{d_{\text{out}}}\)</span></p>
<p><strong>Parameters</strong>. <span class="math inline">\(d_{\text{out}}(d_{\text{in}} + 1)\)</span></p>
<p><strong>Example</strong>. Replace ResNet50 head with <code>Linear(2048, 10)</code></p>
<hr>
</section>
<section id="activation-layers" class="level3">
<h3 class="anchored" data-anchor-id="activation-layers">Activation Layers</h3>
<p><span class="math inline">\(\text{ReLU}(z) = \max(0, z)\)</span></p>
<ul>
<li>Piecewise linear (cheap to compute)</li>
<li>Gradient always equal one for <span class="math inline">\(z &gt; 0\)</span></li>
<li>Sparse activations (exactly zero for <span class="math inline">\(z &lt; 0\)</span>)</li>
</ul>
<p>Without nonlinearity: <span class="math inline">\(f = W_L \cdots W_1\)</span> reduces to single linear map</p>
<hr>
</section>
<section id="normalization-layers" class="level3">
<h3 class="anchored" data-anchor-id="normalization-layers">Normalization Layers</h3>
<p><strong>BatchNorm</strong>: Normalize over batch dimension <span class="math display">\[\hat{z} = \frac{z - \mathbb{E}_{\text{batch}}[z]}{\sqrt{\text{Var}_{\text{batch}}[z]}}\]</span></p>
<p><strong>LayerNorm</strong>: Normalize over feature dimension <span class="math display">\[\hat{z} = \frac{z - \mathbb{E}_{\text{features}}[z]}{\sqrt{\text{Var}_{\text{features}}[z]}}\]</span></p>
<p>Effect: Stabilizes gradient flow, permits larger learning rates</p>
<hr>
</section>
<section id="output-layers" class="level3">
<h3 class="anchored" data-anchor-id="output-layers">Output Layers</h3>
<p><strong>Softmax</strong>: <span class="math inline">\(\mathbb{R}^k \to \Delta^{k-1}\)</span></p>
<p><span class="math display">\[\sigma(z)_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}\]</span></p>
<p>Maps logits to probability simplex</p>
<p>Temperature <span class="math inline">\(\tau\)</span>: <span class="math inline">\(\sigma_\tau(z)_i = \frac{\exp(z_i/\tau)}{\sum_j \exp(z_j/\tau)}\)</span> - <span class="math inline">\(\tau \to 0\)</span>: argmax - <span class="math inline">\(\tau \to \infty\)</span>: uniform</p>
<hr>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<hr>
<section id="nn.module-and-autograd" class="level3">
<h3 class="anchored" data-anchor-id="nn.module-and-autograd"><code>nn.Module</code> and Autograd</h3>
<p><strong><code>nn.Module</code></strong>: Base class for neural net components; manages parameters</p>
<p><strong>Autograd</strong>: PyTorch’s automatic differentiation engine</p>
<p>Tracks operations to compute gradients via chain rule (backpropagation)</p>
<hr>
</section>
<section id="loss-function-optimization" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-optimization">Loss Function &amp; Optimization</h3>
<p><strong>CrossEntropyLoss</strong>: <span class="math inline">\(H(y, \hat{y}) = -\sum_k y_k \log \hat{y}_k\)</span></p>
<p>For one-hot <span class="math inline">\(y\)</span>: Reduces to <span class="math inline">\(-\log \hat{y}_{\text{true class}}\)</span></p>
<p>SGD update: <span class="math inline">\(\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)\)</span> - <span class="math inline">\(\eta\)</span>: Learning rate (step size) - <span class="math inline">\(\nabla_\theta \mathcal{L}\)</span>: Computed via backpropagation (chain rule)</p>
<hr>
</section>
<section id="training-loop-steps" class="level3">
<h3 class="anchored" data-anchor-id="training-loop-steps">Training Loop Steps</h3>
<p>One iteration:</p>
<ol type="1">
<li><strong>Forward</strong>: <span class="math inline">\(\hat{y} = f(x; \theta)\)</span></li>
<li><strong>Loss</strong>: <span class="math inline">\(\mathcal{L} = H(y, \hat{y})\)</span></li>
<li><strong>Backward</strong>: <span class="math inline">\(\nabla_\theta \mathcal{L}\)</span> via autograd</li>
<li><strong>Update</strong>: <span class="math inline">\(\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}\)</span></li>
<li><strong>Zero</strong>: Clear gradient accumulation</li>
</ol>
<p>Repeat over batches, then epochs</p>
<hr>
</section>
<section id="pytorch-lightning" class="level3">
<h3 class="anchored" data-anchor-id="pytorch-lightning">PyTorch Lightning</h3>
<p>PyTorch training contains significant boilerplate (device management, loops, logging)</p>
<p><strong>Lightning Module</strong>: Contains model logic (forward, step, optimizer)</p>
<p><strong>Trainer</strong>: Handles the engineering (hardware, loops, checkpoints)</p>
<p>Helps us focus on modeling research logic.</p>
<hr>
</section>
<section id="the-lightning-trainer" class="level3">
<h3 class="anchored" data-anchor-id="the-lightning-trainer">The Lightning Trainer</h3>
<p>The <code>pl.Trainer</code> automates the execution:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> pl.Trainer(max_epochs<span class="op">=</span><span class="dv">10</span>, accelerator<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>trainer.fit(model, train_loader, val_loader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Used in the EuroSAT case study to manage a 10-epoch training run</p>
<hr>
</section>
</section>
<section id="case-study" class="level2">
<h2 class="anchored" data-anchor-id="case-study">Case Study</h2>
<hr>
<section id="data-preparation-1" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation-1">Data Preparation</h3>
<p><strong>Dataset</strong>: EuroSAT RGB (10 classes)</p>
<p><strong>Split</strong>: 80% Train / 20% Validation</p>
<p><strong>Augmentation</strong>: Random crops and flips on training set to prevent overfitting</p>
<hr>
</section>
<section id="model-class" class="level3">
<h3 class="anchored" data-anchor-id="model-class">Model Class</h3>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EuroSATClassifier(pl.LightningModule):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes: <span class="bu">int</span>):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-trained ResNet50 backbone</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> models.resnet50(weights<span class="op">=</span><span class="st">'DEFAULT'</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Modify the head for 10 classes</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.fc <span class="op">=</span> nn.Linear(</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.model.fc.in_features,</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>            num_classes</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.criterion <span class="op">=</span> nn.CrossEntropyLoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="training-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="training-evaluation">Training &amp; Evaluation</h3>
<p>The module defines <code>training_step</code> and <code>validation_step</code>:</p>
<ul>
<li>Logs <code>train_loss</code> and <code>val_acc</code> per epoch</li>
<li>Uses SGD optimizer with learning rate <span class="math inline">\(10^{-3}\)</span></li>
<li><code>model.eval()</code> and <code>torch.no_grad()</code> used automatically by Lightning during validation</li>
</ul>
<hr>
</section>
<section id="representation-learning" class="level3">
<h3 class="anchored" data-anchor-id="representation-learning">Representation Learning</h3>
<p>Representation learning: Each layer transforms input space</p>
<ul>
<li>Early layers: Local statistical regularities</li>
<li>Middle layers: Part-based features</li>
<li>Late layers: Semantic abstractions</li>
</ul>
<p>Question: What geometric transformations occur layer-by-layer?</p>
<hr>
</section>
<section id="case-study-layer-1-nearest-neighbors" class="level3">
<h3 class="anchored" data-anchor-id="case-study-layer-1-nearest-neighbors">Case Study: Layer 1 Nearest Neighbors</h3>
<p>Layer 1 features: Edge orientations, color blobs</p>
<p>Nearest neighbors grouped by: - Dominant color (green vs.&nbsp;gray) - Simple textures (smooth vs.&nbsp;rough)</p>
<p>Not yet semantic: Forests and crops both green</p>
<hr>
</section>
<section id="case-study-layer-10-nearest-neighbors" class="level3">
<h3 class="anchored" data-anchor-id="case-study-layer-10-nearest-neighbors">Case Study: Layer 10 Nearest Neighbors</h3>
<p>Layer 10 features: Mid-level patterns</p>
<ul>
<li>Repeating grids (urban structures)</li>
<li>Complex textures (foliage patterns)</li>
</ul>
<p>Groupings begin to reflect geometric structures specific to land-use types</p>
<hr>
</section>
<section id="case-study-layer-50-nearest-neighbors" class="level3">
<h3 class="anchored" data-anchor-id="case-study-layer-50-nearest-neighbors">Case Study: Layer 50 Nearest Neighbors</h3>
<p>Layer 50 features: Class-discriminative</p>
<p>Nearest neighbors grouped by semantic category - Invariant to illumination, pose, scale - Different “Residential” images closer than “Residential” vs “Forest”</p>
<p>Network has learned a metric on land-use types</p>
</section>
</section>
</section>
<section id="attention" class="level1">
<h1>17. Attention</h1>
<ul>
<li><p>motivating case study</p></li>
<li><p>data processing goals</p></li>
<li><p>attention</p></li>
<li><p>multihead attention</p></li>
</ul>
<hr>
<section id="learning-outcomes-14" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-14">Learning outcomes</h3>
<ul>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="readings" class="level3">
<h3 class="anchored" data-anchor-id="readings">Readings</h3>
<p>Turner, R. E. (2023). An Introduction to Transformers. http://doi.org/10.48550/ARXIV.2304.10557</p>
<hr>
</section>
<section id="case-study-1" class="level3">
<h3 class="anchored" data-anchor-id="case-study-1">Case Study</h3>
<p><strong>Question</strong>. How do LLMs recognize emotion in text?</p>
<p><strong>Data</strong>. enVent dataset</p>
<ul>
<li>6600 brief emotional narratives</li>
<li>Labels: anger, boredom, disgust, fear, guilt, joy, pride, relief, sadness, shame, surprise, trust</li>
</ul>
<p><strong>Goal</strong>. Identify which tokens and parameters detect emotion</p>
<hr>
</section>
<section id="statistical-formulation-3" class="level3">
<h3 class="anchored" data-anchor-id="statistical-formulation-3">Statistical Formulation</h3>
<p><strong>Text</strong>. Token sequence <span class="math inline">\(x_1, \dots, x_n \in \{0,1\}^V\)</span> <strong>Labels</strong>. <span class="math inline">\(y \in \{0,1\}^{13}\)</span> <strong>Model</strong>. LLM <span class="math inline">\(f(x; \theta)\)</span> <strong>Task</strong>. Which phrases and which parts of <span class="math inline">\(\theta\)</span> recognize emotion?</p>
<hr>
</section>
<section id="attention-mechanism" class="level2">
<h2 class="anchored" data-anchor-id="attention-mechanism">Attention Mechanism</h2>
<hr>
<section id="input-format" class="level3">
<h3 class="anchored" data-anchor-id="input-format">Input Format</h3>
<p><strong>Input</strong>. <span class="math inline">\(N\)</span> vectors <span class="math inline">\(x_n^{(0)} \in \mathbf{R}^D\)</span>, arranged as matrix <span class="math inline">\(X^{(0)} \in \mathbf{R}^{D \times N}\)</span></p>
<p><strong>Examples</strong>.</p>
<ul>
<li>Text: word/subword tokens, each a learned vector</li>
<li>Images: patches reshaped to vectors, embedded via learned <span class="math inline">\(W\)</span></li>
</ul>
<p><em>(Figure 1 from Turner)</em></p>
<hr>
</section>
<section id="transformer-architecture" class="level3">
<h3 class="anchored" data-anchor-id="transformer-architecture">Transformer Architecture</h3>
<p>Iterate <span class="math inline">\(M\)</span> layers: <span class="math display">\[X^{(m)} = \text{block}(X^{(m-1)})\]</span></p>
<p>Each block has two stages:</p>
<ol type="1">
<li><strong>Attention</strong>. Process across sequence (horizontal)</li>
<li><strong>MLP</strong>. Process across features (vertical)</li>
</ol>
<p>This gives a new sequence representation.</p>
<hr>
</section>
<section id="attention-weighted-average" class="level3">
<h3 class="anchored" data-anchor-id="attention-weighted-average">Attention: Weighted Average</h3>
<p>Output at position <span class="math inline">\(n\)</span>: <span class="math display">\[y_n^{(m)} = \sum_{n'=1}^N x_{n'}^{(m-1)} A_{n',n}^{(m)}\]</span></p>
<p><strong>Attention matrix</strong> <span class="math inline">\(A^{(m)} \in \mathbf{R}^{N \times N}\)</span>:</p>
<ul>
<li>Column-normalized: <span class="math inline">\(\sum_{n'} A_{n',n} = 1\)</span></li>
<li><span class="math inline">\(A_{n',n}\)</span> = relevance of position <span class="math inline">\(n'\)</span> to position <span class="math inline">\(n\)</span></li>
</ul>
<p>Matrix form: <span class="math inline">\(Y^{(m)} = X^{(m-1)} A^{(m)}\)</span></p>
<p><em>(Figure 3 from Turner)</em></p>
<hr>
</section>
<section id="self-attention" class="level3">
<h3 class="anchored" data-anchor-id="self-attention">Self-Attention</h3>
<p><strong>Question</strong>. Where does <span class="math inline">\(A\)</span> come from?</p>
<p><strong>Answer</strong>. Compute from input itself</p>
<hr>
</section>
<section id="attempt-1-dot-product" class="level3">
<h3 class="anchored" data-anchor-id="attempt-1-dot-product">Attempt 1: Dot Product</h3>
<p><span class="math display">\[A_{n,n'} = \frac{\exp(x_n^\top x_{n'})}{\sum_{n''} \exp(x_{n''}^\top x_{n'})}\]</span></p>
<p><strong>Problem</strong>. Conflates content and similarity, forces symmetry</p>
<hr>
</section>
<section id="attempt-2-bilinear-form" class="level3">
<h3 class="anchored" data-anchor-id="attempt-2-bilinear-form">Attempt 2: Bilinear Form</h3>
<p><span class="math display">\[A_{n,n'} = \frac{\exp(x_n^\top U^\top U x_{n'})}{\sum_{n''} \exp(x_{n''}^\top U^\top U x_{n'})}\]</span></p>
<p><strong>Progress</strong>. Decouples content/similarity using projection to <span class="math inline">\(K &lt; D\)</span> dimensions <strong>Problem</strong>. Still symmetric</p>
<hr>
</section>
<section id="exercise-8" class="level3">
<h3 class="anchored" data-anchor-id="exercise-8">Exercise</h3>
<hr>
</section>
<section id="final-form-asymmetric-attention" class="level3">
<h3 class="anchored" data-anchor-id="final-form-asymmetric-attention">Final Form: Asymmetric Attention</h3>
<p><span class="math display">\[A_{n,n'} = \frac{\exp(k_n^\top q_{n'})}{\sum_{n''} \exp(k_{n''}^\top q_{n'})}\]</span></p>
<p>where</p>
<ul>
<li><strong>Query</strong>. <span class="math inline">\(q_n = U_q x_n\)</span> (what I’m looking for)</li>
<li><strong>Key</strong>. <span class="math inline">\(k_n = U_k x_n\)</span> (what I contain)</li>
</ul>
<p>Parameters: <span class="math inline">\(U_q, U_k \in \mathbf{R}^{K \times D}\)</span></p>
<hr>
</section>
</section>
<section id="multi-head-attention" class="level2">
<h2 class="anchored" data-anchor-id="multi-head-attention">Multi-Head Attention</h2>
<hr>
<section id="motivation-1" class="level3">
<h3 class="anchored" data-anchor-id="motivation-1">Motivation</h3>
<p><strong>Limitation</strong>. Single attention matrix forces one similarity score per token pair.</p>
<p><strong>Need</strong>. Tokens should be similar in some dimensions, different in others.</p>
<p><strong>Example</strong>. “teacher” and “student”</p>
<ul>
<li>Similar context</li>
<li>Different roles</li>
</ul>
<p><strong>Solution</strong>. Multiple heads capture multiple relationships simultaneously.</p>
<hr>
</section>
<section id="multi-head-self-attention-mhsa" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-self-attention-mhsa">Multi-Head Self-Attention (MHSA)</h3>
<p>Run <span class="math inline">\(H\)</span> attention heads in parallel: <span class="math display">\[Y^{(m)} = \sum_{h=1}^H V_h^{(m)} X^{(m-1)} A_h^{(m)}\]</span></p>
<p>Each head <span class="math inline">\(h\)</span>:</p>
<ul>
<li>Own queries: <span class="math inline">\(q_{h,n} = U_{q,h} x_n\)</span></li>
<li>Own keys: <span class="math inline">\(k_{h,n} = U_{k,h} x_n\)</span></li>
<li>Own attention: <span class="math inline">\(A_h^{(m)}\)</span></li>
</ul>
<p>Project down: <span class="math inline">\(V_h \in \mathbf{R}^{D \times D}\)</span></p>
<p><em>(Figure 4 from Turner)</em></p>
<hr>
</section>
<section id="mlp-stage" class="level3">
<h3 class="anchored" data-anchor-id="mlp-stage">MLP Stage</h3>
<p>Refine features at each position independently: <span class="math display">\[x_n^{(m)} = \text{MLP}_\theta(y_n^{(m)})\]</span></p>
<p><strong>Key</strong>. Same parameters <span class="math inline">\(\theta\)</span> for all positions <span class="math inline">\(n\)</span></p>
<hr>
</section>
<section id="exercise-9" class="level3">
<h3 class="anchored" data-anchor-id="exercise-9">Exercise</h3>
<hr>
</section>
</section>
</section>
<section id="transformers" class="level1">
<h1>18. Transformers</h1>
<ul>
<li><p>MLP component</p></li>
<li><p>transformers</p></li>
<li><p>variations</p></li>
<li><p>application to case study</p></li>
</ul>
<hr>
<section id="learning-outcomes-15" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-15">Learning outcomes</h3>
<ul>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="readings-1" class="level3">
<h3 class="anchored" data-anchor-id="readings-1">Readings</h3>
<p>Turner, R. E. (2023). An Introduction to Transformers. http://doi.org/10.48550/ARXIV.2304.10557</p>
<hr>
</section>
<section id="residual-connections" class="level3">
<h3 class="anchored" data-anchor-id="residual-connections">Residual Connections</h3>
<p><span class="math display">\[x^{(m)} = x^{(m-1)} + \text{res}_\theta(x^{(m-1)})\]</span></p>
<p><strong>Purpose</strong>.</p>
<ul>
<li>Initialize near the identity (stabilize training)</li>
<li>Create “residual stream” that layers build on</li>
</ul>
<hr>
</section>
<section id="layer-normalization" class="level3">
<h3 class="anchored" data-anchor-id="layer-normalization">Layer Normalization</h3>
<p>Normalize each token separately: <span class="math display">\[\tilde{x}_{d,n} = \frac{x_{d,n} - \mu_n}{\sqrt{\sigma_n^2}} \gamma_d + \beta_d\]</span></p>
<p>where <span class="math inline">\(\mu_n = \frac{1}{D}\sum_d x_{d,n}\)</span>, <span class="math inline">\(\sigma_n^2 = \frac{1}{D}\sum_d (x_{d,n} - \mu_n)^2\)</span></p>
<p><strong>Purpose</strong>. Prevent feature explosion through depth</p>
<p><em>(Figure 5 from Turner: compare LayerNorm vs BatchNorm)</em></p>
<hr>
</section>
<section id="transformer-block" class="level3">
<h3 class="anchored" data-anchor-id="transformer-block">Transformer Block</h3>
<p>Combine MHSA + MLP + residuals + normalization:</p>
<ol type="1">
<li><span class="math inline">\(\tilde{X} = \text{LayerNorm}(X^{(m-1)})\)</span></li>
<li><span class="math inline">\(Y^{(m)} = X^{(m-1)} + \text{MHSA}(\tilde{X})\)</span></li>
<li><span class="math inline">\(\tilde{Y} = \text{LayerNorm}(Y^{(m)})\)</span></li>
<li><span class="math inline">\(X^{(m)} = Y^{(m)} + \text{MLP}(\tilde{Y})\)</span></li>
</ol>
<p><em>(Figure 7 from Turner)</em></p>
<hr>
</section>
<section id="pytorch-implementation" class="level3">
<h3 class="anchored" data-anchor-id="pytorch-implementation">PyTorch Implementation</h3>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>block <span class="op">=</span> nn.TransformerEncoderLayer(</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    d_model<span class="op">=</span>D,           <span class="co"># embedding dimension</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    nhead<span class="op">=</span>H,             <span class="co"># number of heads</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    dim_feedforward<span class="op">=</span><span class="dv">2048</span> <span class="co"># MLP hidden size</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>X_next <span class="op">=</span> block(X)  <span class="co"># X: (SeqLen, Batch, Dim)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="variants" class="level2">
<h2 class="anchored" data-anchor-id="variants">Variants</h2>
<hr>
<section id="positional-encoding" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding">Positional Encoding</h3>
<p><strong>Problem</strong>. Token order matters (“herbivores eat plants” <span class="math inline">\(\neq\)</span> “plants eat herbivores”)</p>
<p><strong>Solution</strong>. Add position information to <span class="math inline">\(x_n^{(0)}\)</span> - Fixed (sinusoidal encoding) - Learned (parameter per position)</p>
<hr>
</section>
<section id="autoregressive-masking" class="level3">
<h3 class="anchored" data-anchor-id="autoregressive-masking">Autoregressive Masking</h3>
<p><strong>Task</strong>. Predict <span class="math inline">\(p(w_n | w_{1:n-1})\)</span></p>
<p><strong>Naive approach</strong>. Run transformer <span class="math inline">\(N\)</span> times with growing sequences. Requires <span class="math inline">\(O(N^2)\)</span> passes</p>
<p><strong>Solution</strong>. Mask attention upper-triangular <span class="math display">\[A_{n,n'} = 0 \text{ when } n &gt; n'\]</span></p>
<p><strong>Result</strong>. Token <span class="math inline">\(n\)</span> only sees tokens <span class="math inline">\(1, \dots, n-1\)</span> Can use a single forward pass for all predictions</p>
<hr>
</section>
<section id="classification-head" class="level3">
<h3 class="anchored" data-anchor-id="classification-head">Classification Head</h3>
<p><strong>Task</strong>. Image classification from patches <span class="math inline">\(X^{(0)}\)</span></p>
<p><strong>Approach</strong>. Add learned token <span class="math inline">\(x_0^{(0)}\)</span> at the start. Use <span class="math inline">\(x_0^{(M)}\)</span> for classification: <span class="math display">\[p(y | X^{(0)}) = \text{softmax}(W x_0^{(M)})\]</span></p>
<p>This maintains a global representation that can be refined at each layer.</p>
<hr>
</section>
</section>
<section id="case-study-envent" class="level2">
<h2 class="anchored" data-anchor-id="case-study-envent">Case Study: enVent</h2>
<hr>
<section id="visualizing-attention" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-attention">Visualizing Attention</h3>
<p><strong>Input</strong>. “”</p>
<p><strong>Question</strong>. Does model attend to …</p>
<hr>
</section>
<section id="extracting-weights" class="level3">
<h3 class="anchored" data-anchor-id="extracting-weights">Extracting Weights</h3>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Single-example batches</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract all layers</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> extract_hidden_states(</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    loader,</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    layers<span class="op">=</span><span class="bu">range</span>(num_layers),</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    do_final_cat<span class="op">=</span><span class="va">False</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>torch.save(results, <span class="st">'attention_weights.pt'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="visualization-3" class="level3">
<h3 class="anchored" data-anchor-id="visualization-3">Visualization</h3>
<p><strong>Attention matrix</strong> <span class="math inline">\(A_h^{(m)} \in \mathbb{R}^{N \times N}\)</span>: - Rows: keys (information providers) - Columns: queries (information receivers) - Intensity: attention weight magnitude</p>
<hr>
</section>
<section id="analysis" class="level3">
<h3 class="anchored" data-anchor-id="analysis">Analysis</h3>
<p>Do we see high attention on: - Emotion words (“anger”, “fear”, “pride”) - Causal markers (“because”, “after”, “due to”)</p>
<p>We will compare attention patterns across emotion categories.</p>
<hr>
</section>
<section id="exercise-10" class="level3">
<h3 class="anchored" data-anchor-id="exercise-10">Exercise</h3>
</section>
</section>
</section>
<section id="saliency-maps" class="level1">
<h1>19. Saliency Maps</h1>
<ul>
<li><p>motivating application</p></li>
<li><p>saliency maps</p></li>
<li><p>captum package</p></li>
</ul>
<hr>
<section id="learning-outcomes-16" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-16">Learning outcomes</h3>
<ul>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="readings-2" class="level3">
<h3 class="anchored" data-anchor-id="readings-2">Readings</h3>
<ul>
<li>Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., &amp; Kim, B. (2018). Sanity Checks for Saliency Maps. doi:10.48550/ARXIV.1810.03292</li>
<li>Captum - Model Interpretability for Pytorch. Getting started with Captum - Titanic Data Analysis. (n.d.). https://captum.ai/tutorials/Titanic_Basic_Interpret</li>
</ul>
<hr>
</section>
<section id="motivating-case-study-4" class="level3">
<h3 class="anchored" data-anchor-id="motivating-case-study-4">Motivating Case Study</h3>
<p>India lacks centralized renewable energy databases.</p>
<p>Questions: - Solar plant counts over time? - Geographic distribution?</p>
<p>Needed for energy transition planning (Ortiz, Negandhi, Mysorekar, Nagaraju, Kiesecker, Robinson, Bhatia, Khurana, Wang, Oviedo, and Ferres, 2022; Ortiz, Negandhi, Mysorekar, Kiesecker, Nagaraju, Robinson, Bhatia, Khurana, Wang, Oviedo, and Ferres, 2022).</p>
<hr>
<p><img src="figures/solar_farm_predictions.png"></p>
<hr>
</section>
<section id="scientific-goal-1" class="level3">
<h3 class="anchored" data-anchor-id="scientific-goal-1">Scientific Goal</h3>
<p><strong>Question</strong>. Can we locate solar farms in satellite imagery?</p>
<p><strong>Data</strong>. Sentinel 2 (10-60m/px, 12 bands) + OpenStreetMap labels. 1363 labeled sites.</p>
<p><strong>Constraint</strong>. Must isolate pixels responsible for predictions.</p>
<hr>
</section>
<section id="cautionary-example" class="level3">
<h3 class="anchored" data-anchor-id="cautionary-example">Cautionary Example</h3>
<p>Models can pick up on spurious correlations (like whether there is snow in the background). Leads to generalization failure.</p>
<p><img src="figures/husky_saliency.png" class="center"></p>
<hr>
</section>
<section id="statistical-formulation-4" class="level3">
<h3 class="anchored" data-anchor-id="statistical-formulation-4">Statistical Formulation</h3>
<p><strong>Labels</strong>. <span class="math inline">\(y \in \{0, 1\}^{256 \times 256}\)</span> (pixel labels)</p>
<p><strong>Predictors</strong>. <span class="math inline">\(x \in \mathbb{R}^{10 \times 256 \times 256}\)</span> (spectral patch)</p>
<p><strong>Model</strong>. <span class="math inline">\(f(x; \theta) \to [0, 1]^{256 \times 256}\)</span> (probability mask)</p>
<p><strong>Goal</strong>. Find attributions <span class="math inline">\(\varphi \in \mathbb{R}^{256 \times 256}\)</span> explaining <span class="math inline">\(f(x)\)</span>.</p>
<hr>
</section>
<section id="saliency-maps-1" class="level2">
<h2 class="anchored" data-anchor-id="saliency-maps-1">Saliency Maps</h2>
<hr>
<section id="perturbation-methods" class="level3">
<h3 class="anchored" data-anchor-id="perturbation-methods">Perturbation Methods</h3>
<p>Perturb input <span class="math inline">\(x\)</span> to measure prediction changes.</p>
<p><strong>Occlusion</strong>. Mask patches, observe <span class="math inline">\(\Delta f(x)\)</span></p>
<p><strong>Feature Ablation</strong>. Replace features with baseline, measure marginal effects</p>
<p><img src="figures/perturbation_types.png"></p>
<hr>
</section>
<section id="gradient-saliency" class="level3">
<h3 class="anchored" data-anchor-id="gradient-saliency">Gradient Saliency</h3>
<p><span class="math display">\[E_{\text{grad}}(x) = \frac{\partial S_c(x)}{\partial x}\]</span></p>
<p>Identifies pixels where small changes maximally affect class <span class="math inline">\(c\)</span> score.</p>
<p>Refinement: <span class="math inline">\(x \odot \frac{\partial S_c}{\partial x}\)</span> accounts for feature scale.</p>
<hr>
</section>
<section id="computation" class="level3">
<h3 class="anchored" data-anchor-id="computation">Computation</h3>
<ul>
<li><strong>Mechanism</strong>. Efficiently computed via a single backpropagation pass using <code>torch.autograd</code>.</li>
<li><strong>Aggregation</strong>. For multi-channel inputs (e.g., RGB or 12-band spectral), gradients are typically reduced to a single map using: <span class="math display">\[M(x) = \max_j | \nabla_{x_j} f(x) |\]</span> where <span class="math inline">\(j\)</span> is the channel index.</li>
</ul>
<hr>
</section>
<section id="gradient-limitations" class="level3">
<h3 class="anchored" data-anchor-id="gradient-limitations">Gradient Limitations</h3>
<p><strong>Saturation</strong>. Flat regions (ReLU) yield zero gradients despite importance.</p>
<p><strong>Noise</strong>. High-frequency artifacts from shattered gradients.</p>
<p><strong>Instability</strong>. Prediction-stable regions show large gradient changes.</p>
<hr>
</section>
<section id="integrated-gradients" class="level3">
<h3 class="anchored" data-anchor-id="integrated-gradients">Integrated Gradients</h3>
<p>Satisfies completeness: <span class="math inline">\(\sum \text{IG}_i(x) = f(x) - f(x')\)</span></p>
<p><span class="math display">\[\text{IG}_{i}(x) = \left(x_{i} - x_{i}'\right) \int_{\alpha=0}^{1} \frac{\partial f\left(x' + \alpha\left(x - x'\right)\right)}{\partial x_{i}} d\alpha\]</span></p>
<p><span class="math inline">\(x'\)</span>: baseline (e.g., black image)</p>
<p>Aggregates gradients along path <span class="math inline">\(x' \to x\)</span>.</p>
<hr>
</section>
<section id="integrated-gradients-1" class="level3">
<h3 class="anchored" data-anchor-id="integrated-gradients-1">Integrated Gradients</h3>
<p><img src="figures/integrated_gradients_animation.gif" class="center" width="600/"></p>
<hr>
</section>
<section id="approximation" class="level3">
<h3 class="anchored" data-anchor-id="approximation">Approximation</h3>
<p>Riemann sum with <span class="math inline">\(m \in [50, 200]\)</span> steps:</p>
<p><span class="math display">\[\text{IG}_i(x) \approx (x_i - x'_i) \sum_{k=1}^m \frac{\partial f(x' + \frac{k}{m}(x - x'))}{\partial x_i} \cdot \frac{1}{m}\]</span></p>
<p>Cost: <span class="math inline">\(\mathcal{O}(m)\)</span> backpropagations.</p>
<hr>
</section>
<section id="captum-package" class="level3">
<h3 class="anchored" data-anchor-id="captum-package">captum package</h3>
<p>PyTorch interpretability library:</p>
<ul>
<li><code>captum.attr</code>: Saliency, IG, DeepLift, SHAP</li>
<li><code>captum.metrics</code>: Robustness</li>
<li><code>captum.concept</code>: TCAV (next week)</li>
</ul>
<hr>
</section>
<section id="gradients-in-captum" class="level3">
<h3 class="anchored" data-anchor-id="gradients-in-captum">Gradients in captum</h3>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> captum.attr <span class="im">import</span> Saliency</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>slc <span class="op">=</span> Saliency(model)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>attr <span class="op">=</span> slc.attribute(input_tensor, target<span class="op">=</span>class_idx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="exercise-pseudocode" class="level3">
<h3 class="anchored" data-anchor-id="exercise-pseudocode">Exercise: Pseudocode</h3>
<p>Goal: Write the loop for Integrated Gradients.</p>
<pre><code>    Initialize total_gradients = 0.
    Define baseline and steps = 50.
    For k in range 1 to steps:
        ### ???

    attribution = (input - baseline) * (total_gradients / steps)</code></pre>
<hr>
</section>
</section>
<section id="shap" class="level2">
<h2 class="anchored" data-anchor-id="shap">SHAP</h2>
<hr>
<section id="shap-for-images" class="level3">
<h3 class="anchored" data-anchor-id="shap-for-images">SHAP for Images</h3>
<p>SHAP treats pixels as the players in a cooperative game.</p>
<p><span class="math display">\[\varphi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} [v(S \cup \{i\}) - v(S)]\]</span></p>
<p>Problem: <span class="math inline">\(256 \times 256\)</span> image has <span class="math inline">\(2^{65536}\)</span> coalitions.</p>
<p>Approximations: - KernelSHAP: weighted regression - Superpixels: group pixels to reduce <span class="math inline">\(|N|\)</span></p>
<hr>
</section>
<section id="shap-in-captum" class="level3">
<h3 class="anchored" data-anchor-id="shap-in-captum">SHAP in captum</h3>
<p>Captum provides KernelShap and ShapleyValueSampling.</p>
<ul>
<li>Feature Masking: Users can define a feature_mask to group pixels into semantic regions (e.g., all pixels in a solar panel).</li>
<li>Efficiency: Instead of 2N samples, it uses a limited budget of permutations to estimate the values.</li>
</ul>
<hr>
</section>
<section id="feature-neighborhoods" class="level3">
<h3 class="anchored" data-anchor-id="feature-neighborhoods">Feature Neighborhoods</h3>
<ul>
<li><p>Another idea is to restrict the collection of sets in the summation.</p></li>
<li><p>This is most natural when there is a notion of distance between features. For example, for a word at the start of a sentence, don’t bother with sets of words near the end.</p></li>
</ul>
<p><img width="400" src="figures/l_shaptext.png" class="center"></p>
<hr>
</section>
<section id="l-shapley-local-approximation" class="level3">
<h3 class="anchored" data-anchor-id="l-shapley-local-approximation"><span class="math inline">\(L\)</span>-Shapley (Local Approximation)</h3>
<p>Let <span class="math inline">\(N_k(i)\)</span> = features within distance <span class="math inline">\(k\)</span> of <span class="math inline">\(i\)</span>.</p>
<p><span class="math display">\[\varphi^{L}(f, i) = \frac{1}{|N_k(i)|} \sum_{S \in N_k(i)} \frac{1}{\binom{|N_k(i)|-1}{|S|-1}} [v(S) - v(S \setminus \{i\})]\]</span></p>
<p>Efficient when <span class="math inline">\(k \ll |N|\)</span>.</p>
<p><img width="400" src="figures/l-c-shapley.png" class="center"></p>
<hr>
</section>
<section id="exercise-interpretability-goals-vs.-method" class="level3">
<h3 class="anchored" data-anchor-id="exercise-interpretability-goals-vs.-method">Exercise: Interpretability Goals vs.&nbsp;Method</h3>
</section>
</section>
</section>
<section id="sanity-checks-and-benchmarking" class="level1">
<h1>20. Sanity Checks and Benchmarking</h1>
<ul>
<li><p>application to case study</p></li>
<li><p>sanity checks</p></li>
<li><p>remove and retrain</p></li>
</ul>
<hr>
<section id="learning-outcomes-17" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-17">Learning Outcomes</h3>
<ul>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="readings-3" class="level3">
<h3 class="anchored" data-anchor-id="readings-3">Readings</h3>
<ul>
<li>Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., &amp; Kim, B. (2018). Sanity Checks for Saliency Maps. doi:10.48550/ARXIV.1810.03292</li>
<li>Captum - Model Interpretability for Pytorch. Getting started with Captum - Titanic Data Analysis. (n.d.). https://captum.ai/tutorials/Titanic_Basic_Interpret</li>
</ul>
<hr>
</section>
<section id="dataloader" class="level3">
<h3 class="anchored" data-anchor-id="dataloader">Dataloader</h3>
<hr>
</section>
<section id="getting-predictions" class="level3">
<h3 class="anchored" data-anchor-id="getting-predictions">Getting predictions</h3>
<hr>
</section>
<section id="gradient-based-explanations" class="level3">
<h3 class="anchored" data-anchor-id="gradient-based-explanations">Gradient-based explanations</h3>
<hr>
</section>
<section id="integrated-gradient-explanations" class="level3">
<h3 class="anchored" data-anchor-id="integrated-gradient-explanations">Integrated gradient explanations</h3>
<hr>
</section>
<section id="shap-explanations" class="level3">
<h3 class="anchored" data-anchor-id="shap-explanations">SHAP explanations</h3>
<hr>
</section>
<section id="exercise-saliency-based-model-critique" class="level3">
<h3 class="anchored" data-anchor-id="exercise-saliency-based-model-critique">Exercise: Saliency-based Model Critique</h3>
<hr>
</section>
<section id="sanity-checks-1" class="level2">
<h2 class="anchored" data-anchor-id="sanity-checks-1">Sanity Checks</h2>
<hr>
<section id="independence-from-model" class="level3">
<h3 class="anchored" data-anchor-id="independence-from-model">Independence from Model</h3>
<p>Observation (Adebayo, Gilmer, Muelly, Goodfellow, Hardt, and Kim, 2018): Many saliency methods are invariant to model parameters and data labels.</p>
<p>Visual appeal <span class="math inline">\(\neq\)</span> faithfulness.</p>
<p><img src="figures/sanity_check_edges.png" class="center"></p>
<hr>
</section>
<section id="edge-detection-analogy" class="level3">
<h3 class="anchored" data-anchor-id="edge-detection-analogy">Edge Detection Analogy</h3>
<p>Guided Backprop ≈ edge detector (no model/data dependence).</p>
<p>Highlights boundaries via non-linear image processing, not learned features.</p>
<p><img src="figures/sanity_check_edges.png" class="center"></p>
<hr>
</section>
<section id="model-randomization-check" class="level3">
<h3 class="anchored" data-anchor-id="model-randomization-check">Model randomization check</h3>
<ol type="1">
<li>Randomize weights layer-by-layer</li>
<li>Compute saliency maps for original vs.&nbsp;randomized model</li>
<li>Faithful method: maps differ significantly</li>
</ol>
<p>Results: Gradients/IG pass. Guided Backprop fails.</p>
<hr>
</section>
<section id="cascading-randomization" class="level3">
<h3 class="anchored" data-anchor-id="cascading-randomization">Cascading randomization</h3>
<ul>
<li><strong>Algorithm</strong>. Randomize weights starting from the top (Logits) down to the bottom (Input).</li>
<li><strong>Evaluation</strong>. A faithful method should show a visual degradation that “cascades” as more weights are destroyed.</li>
</ul>
<p>&lt;img src=’figures/cascading_randomization.png” class=“center”/&gt;</p>
<hr>
</section>
<section id="data-randomization-check" class="level3">
<h3 class="anchored" data-anchor-id="data-randomization-check">Data randomization check</h3>
<p>Train on <span class="math inline">\(\{(x_i, \tilde{y}_i)\}\)</span> where <span class="math inline">\(\tilde{y}_i\)</span> are random permutations.</p>
<p>Model trained on random labels should produce noisy saliency maps.</p>
<p>Many methods still highlight “objects” (spurious structure).</p>
<p><img src="figures/sanity_data_randomization.png" class="center"></p>
<hr>
</section>
</section>
<section id="roar" class="level2">
<h2 class="anchored" data-anchor-id="roar">ROAR</h2>
<hr>
<section id="removing-features" class="level3">
<h3 class="anchored" data-anchor-id="removing-features">Removing features</h3>
<p>Can we evaluate feature importance without relying on human visual bias?</p>
<ul>
<li><strong>KAR (Keep and Retrain)</strong>. Keep the top <span class="math inline">\(t\%\)</span> features, mask everything else.</li>
<li><strong>ROAR (Remove and Retrain)</strong>. Mask the top <span class="math inline">\(t\%\)</span> features, keep everything else.</li>
</ul>
<hr>
</section>
<section id="why-retrain" class="level3">
<h3 class="anchored" data-anchor-id="why-retrain">Why Retrain?</h3>
<p>Masking creates distribution shift. Model sees OOD inputs (black squares).</p>
<p>Performance drop reflects: - Information loss (what we want), OR - Mask artifact (confound)</p>
<p>Solution: Retrain on masked data to isolate information loss.</p>
<hr>
</section>
<section id="roar-benchmark" class="level3">
<h3 class="anchored" data-anchor-id="roar-benchmark">ROAR Benchmark</h3>
<p>Remove top <span class="math inline">\(t\%\)</span> important pixels, retrain from scratch.</p>
<p>Measure accuracy drop as function of <span class="math inline">\(t\)</span>.</p>
<p>Better method <span class="math inline">\(\implies\)</span> steeper drop (removed truly informative pixels).</p>
<hr>
</section>
<section id="synthetic-validation" class="level3">
<h3 class="anchored" data-anchor-id="synthetic-validation">Synthetic Validation</h3>
<p>Generate <span class="math inline">\(x \in \mathbb{R}^{16}\)</span> with 4 informative features:</p>
<p><span class="math display">\[x = \frac{az}{10} + d\eta + \frac{\epsilon}{10}, \quad y = \mathbb{1}(z &gt; 0)\]</span></p>
<p>where <span class="math inline">\(z, \eta, \epsilon \sim \mathcal{N}(0,1)\)</span> and <span class="math inline">\(a \in \mathbb{R}^{16}\)</span> is nonzero only in 4 positions.</p>
<hr>
</section>
<section id="synthetic-validation-1" class="level3">
<h3 class="anchored" data-anchor-id="synthetic-validation-1">Synthetic Validation</h3>
<p>Fit least-squares model.</p>
<ul>
<li>Ground truth ranking (known informative features)</li>
<li>Random ranking</li>
<li>Inverted ground truth (worst case)</li>
</ul>
<p>Results</p>
<ol type="1">
<li>Without retraining: All methods appear to work (distribution shift confound)</li>
<li>With ROAR: Correctly identifies worst-case estimator at 75% threshold</li>
<li>SmoothGrad-Squared, VarGrad outperform base methods</li>
</ol>
<hr>
</section>
<section id="roar-results" class="level3">
<h3 class="anchored" data-anchor-id="roar-results">ROAR Results</h3>
<p>Lower curve <span class="math inline">\(\implies\)</span> more informative feature selection.</p>
<p><img src="figures/roar_synthetic_results.png" class="center"></p>
<hr>
</section>
<section id="roar-benchmark-1" class="level3">
<h3 class="anchored" data-anchor-id="roar-benchmark-1">ROAR Benchmark</h3>
<p><img src="figures/roar_overview.png" class="center"></p>
<hr>
</section>
<section id="baseline-random-removal" class="level3">
<h3 class="anchored" data-anchor-id="baseline-random-removal">Baseline: Random Removal</h3>
<p>Key finding: Guided Backprop, IG, Gradient ≈ random removal on ImageNet.</p>
<p>These methods failed to identify informative features.</p>
<p>Ensemble variants of IG do better.</p>
<hr>
</section>
<section id="roar-benchmark-2" class="level3">
<h3 class="anchored" data-anchor-id="roar-benchmark-2">ROAR Benchmark</h3>
<p><img src="figures/roar_example-1.png" class="center"></p>
<hr>
</section>
<section id="roar-benchmark-3" class="level3">
<h3 class="anchored" data-anchor-id="roar-benchmark-3">ROAR Benchmark</h3>
<p><img src="figures/roar_example-2.png" class="center"></p>
<hr>
</section>
<section id="pseudocode" class="level3">
<h3 class="anchored" data-anchor-id="pseudocode">Pseudocode</h3>
<p>For <span class="math inline">\(t \in \{0.1, 0.3, 0.5, 0.7, 0.9\}\)</span>: 1. Generate saliency <span class="math inline">\(\varphi\)</span> for dataset <span class="math inline">\(\mathcal{D}\)</span> 2. Mask top <span class="math inline">\(t\)</span> pixels with mean: <span class="math inline">\(\tilde{x}_i = x_i \mathbb{1}(\varphi_i &lt; q_t) + \mu \mathbb{1}(\varphi_i \geq q_t)\)</span> 3. Initialize <span class="math inline">\(f_{\text{new}}\)</span>, train on <span class="math inline">\(\{(\tilde{x}_i, y_i)\}\)</span> 4. Record <span class="math inline">\(\text{Acc}(f_{\text{new}}, \tilde{\mathcal{D}}_{\text{test}})\)</span></p>
<p>Plot accuracy vs.&nbsp;<span class="math inline">\(t\)</span>.</p>
<hr>
</section>
<section id="exercise-jargon-free-summary" class="level3">
<h3 class="anchored" data-anchor-id="exercise-jargon-free-summary">Exercise: Jargon-free Summary</h3>
</section>
</section>
</section>
<section id="linear-probes-and-concepts" class="level1">
<h1>21. Linear Probes and Concepts</h1>
<ul>
<li><p>motivating application</p></li>
<li><p>linear probes</p></li>
<li><p>training concept vectors</p></li>
<li><p>probes vs.&nbsp;concepts</p></li>
</ul>
<hr>
<section id="learning-outcomes-18" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-18">Learning Outcomes</h3>
<ul>
<li>Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks</li>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="readings-4" class="level3">
<h3 class="anchored" data-anchor-id="readings-4">Readings</h3>
<p>Alain, G., &amp; Bengio, Y. (2017). Understanding intermediate layers using linear classifier probes. OpenReview. https://openreview.net/forum?id=ryF7rTqgl</p>
<p>Schmalwasser, L., Penzel, N., Denzler, J., &amp; Niebling, J. (2025). FastCAV: Efficient computation of concept activation vectors for explaining deep neural networks. In Proceedings of the 42nd International Conference on Machine Learning. https://openreview.net/forum?id=kRmfzTfIGe</p>
<hr>
</section>
<section id="motivating-case-study-5" class="level3">
<h3 class="anchored" data-anchor-id="motivating-case-study-5">Motivating Case Study</h3>
<p>Predict livability from aerial imagery.</p>
<ul>
<li>Can we trust what the model learns?</li>
<li>Will it generalize?</li>
</ul>
<p><img src="figures/"></p>
<hr>
</section>
<section id="statistical-formulation-5" class="level3">
<h3 class="anchored" data-anchor-id="statistical-formulation-5">Statistical Formulation</h3>
<p><strong>Data</strong>. 51,781 grid cells (100m²) in Netherlands. Each has livability score <span class="math inline">\(y \in \mathbf{R}\)</span> and 500×500px aerial image <span class="math inline">\(x\)</span>.</p>
<p><strong>Supplemental</strong>. FLAIR land use labels from French National Institute.</p>
<p><strong>Goal</strong>. Which visual concepts does <span class="math inline">\(f(x;\theta)\)</span> use to predict <span class="math inline">\(y\)</span>?</p>
<p><strong>Concept labels</strong> <span class="math inline">\(c \in \{1,\ldots,K\}\)</span> from auxiliary dataset.</p>
<hr>
<p><img src="figures/kim_iclr.png" class="center"></p>
<hr>
</section>
<section id="linear-probes" class="level2">
<h2 class="anchored" data-anchor-id="linear-probes">Linear Probes</h2>
<hr>
<section id="notation" class="level3">
<h3 class="anchored" data-anchor-id="notation">Notation</h3>
<p><span class="math inline">\(h_l(x)\)</span> — activation vector at layer <span class="math inline">\(l\)</span></p>
<p><span class="math inline">\(p_l(h) = \text{softmax}(W_l h + b_l)\)</span> — linear classifier predicting <span class="math inline">\(y\)</span> from <span class="math inline">\(h_l\)</span></p>
<p><strong>Diode constraint</strong>. <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \theta} = 0\)</span> (probe training doesn’t affect base model)</p>
<hr>
</section>
<section id="estimation" class="level3">
<h3 class="anchored" data-anchor-id="estimation">Estimation</h3>
<p>Minimize empirical risk on hidden representations: <span class="math display">\[\min_{W_l, b_l} \sum_{i=1}^N \mathcal{L}(p_l(h_l(x_i)), y_i)\]</span></p>
<p><strong>Validation</strong>. Test on held-out data to ensure genuine learned features, not memorization.</p>
<p><strong>Information vs.&nbsp;Accessibility</strong>. Total mutual information <span class="math inline">\(I(X; h_l)\)</span> may decrease with depth (Data Processing Inequality), but <em>linear accessibility</em> of task-relevant information typically increases.</p>
<hr>
</section>
<section id="cartoon" class="level3">
<h3 class="anchored" data-anchor-id="cartoon">Cartoon</h3>
<p><img src="figures/probe_cartoon.png" width="600"></p>
<p>Early layers: generic features (edges). Deep layers: task-specific, linearly separable.</p>
<hr>
</section>
<section id="outputs" class="level3">
<h3 class="anchored" data-anchor-id="outputs">Outputs</h3>
<ul>
<li>Layer-wise error <span class="math inline">\(\to\)</span> where model “solves” task</li>
<li>Training dynamics <span class="math inline">\(\to\)</span> epochs until layer becomes useful</li>
<li>Representational capacity <span class="math inline">\(\to\)</span> signal preserved at each bottleneck</li>
</ul>
<hr>
</section>
<section id="example-skip-connections" class="level3">
<h3 class="anchored" data-anchor-id="example-skip-connections">Example: Skip Connections</h3>
<p><strong>Bug detection via probes</strong>. Skip connection from layer 1→64 rendered layers 2–63 “dead” (flat error rates).</p>
<p><img width="500" src="figures/pathological_training.gif"> <br></p>
<hr>
</section>
<section id="exercise-linear-probe-pseudocode" class="level3">
<h3 class="anchored" data-anchor-id="exercise-linear-probe-pseudocode">Exercise: Linear Probe Pseudocode</h3>
<p>Write the pseudocode to extract activations and train a probe for a single layer.</p>
<hr>
</section>
</section>
<section id="concept-activation-vectors" class="level2">
<h2 class="anchored" data-anchor-id="concept-activation-vectors">Concept Activation Vectors</h2>
<hr>
<section id="notation-1" class="level3">
<h3 class="anchored" data-anchor-id="notation-1">Notation</h3>
<p><strong>CAV</strong>. <span class="math inline">\(v_c^l \in \mathbf{R}^{d_l}\)</span> — direction of concept <span class="math inline">\(c\)</span> in layer <span class="math inline">\(l\)</span></p>
<p><strong>Concept Sensitivity</strong>. Directional derivative of class <span class="math inline">\(k\)</span> logit along concept vector: <span class="math display">\[S_{c,k}(x) = \nabla_{h} f_k(h_l(x)) \cdot v_c^l\]</span></p>
<p><strong>TCAV Score</strong>. <span class="math inline">\(T_{c,k} = \frac{|\{x \in X_k : S_{c,k}(x) &gt; 0\}|}{|X_k|}\)</span></p>
<p>Fraction of class-<span class="math inline">\(k\)</span> images where concept <span class="math inline">\(c\)</span> increases model confidence.</p>
<hr>
<p><strong>Step 1</strong>. Define concept images <span class="math inline">\(x_n \in D_c\)</span> (positive set). Construct control pool <span class="math inline">\(x_n' \in N\)</span> (negative set, random images).</p>
<p><span style="font-size: 18px;"> <img src="figures/concepts_step1.png" width="600/"><br> Concept: “stripes” </span></p>
<hr>
<p><strong>Step 2</strong>. Find direction <span class="math inline">\(v_c^l\)</span> that best separates <span class="math inline">\(h_l(D_c)\)</span> from <span class="math inline">\(h_l(N)\)</span> in activation space.</p>
<p><strong>Standard</strong>. Train linear SVM. CAV is the normal vector to decision boundary.</p>
<p><span style="font-size: 18px;"> <img src="figures/concepts_step2.png" width="530/"> </span></p>
<hr>
<p><strong>Step 3</strong>. Compute sensitivity of model’s prediction to this direction.</p>
<p><span class="math display">\[\text{Sensitivity} = \text{Model Gradient} \cdot \text{Concept Direction}\]</span></p>
<p>Global explanation: “To what extent does concept ‘stripes’ contribute to the model’s definition of ‘zebra’?”</p>
<p><span style="font-size: 18px;"> <img src="figures/concepts_step3.png" width="700/"><br> <span class="math inline">\(\nabla y_k(h(x))\)</span> is the steepest ascent direction for class <span class="math inline">\(k\)</span>’s logit. </span></p>
<hr>
</section>
<section id="alternative-step-2-fastcav" class="level3">
<h3 class="anchored" data-anchor-id="alternative-step-2-fastcav">Alternative Step 2: FastCAV</h3>
<p><span class="math inline">\(v_{\text{FastCAV}} = \frac{\bar{h}_c - \bar{h}_r}{\|\bar{h}_c - \bar{h}_r\|}\)</span></p>
<p>Equivalent to SVM under isotropic covariance (<span class="math inline">\(\Sigma = \sigma^2 I\)</span>).</p>
<p>Leads to 46.4× average speedup.</p>
<hr>
</section>
<section id="testing-for-significance" class="level3">
<h3 class="anchored" data-anchor-id="testing-for-significance">Testing for Significance</h3>
<ol type="1">
<li>Generate CAVs from multiple random negative sets</li>
<li>Build null distribution of TCAV scores</li>
<li>Two-sided t-test: is actual concept significant vs.&nbsp;random baseline?</li>
</ol>
<p>More details in next lecture.</p>
<hr>
</section>
</section>
<section id="probes-vs.-concepts" class="level2">
<h2 class="anchored" data-anchor-id="probes-vs.-concepts">Probes vs.&nbsp;Concepts</h2>
<hr>
<section id="similarities" class="level3">
<h3 class="anchored" data-anchor-id="similarities">Similarities</h3>
<ul>
<li>Post-hoc (frozen model)</li>
<li>Linear accessibility assumption</li>
<li>Activation-based (<span class="math inline">\(h_l(x)\)</span>)</li>
</ul>
<hr>
</section>
<section id="differences" class="level3">
<h3 class="anchored" data-anchor-id="differences">Differences</h3>
<p><strong>Probes-specific</strong></p>
<ul>
<li>Align using model’s task labels (<span class="math inline">\(y\)</span>)</li>
<li>Final output is probe accuracy</li>
<li>Goal is to measure the available task-specific information</li>
</ul>
<p><strong>Concepts-specific</strong></p>
<ul>
<li>Align using human semantics (<span class="math inline">\(c\)</span>)</li>
<li>Final output is TCAV sensitivity score</li>
<li>Goal is to model reasoning with human-understandable features</li>
</ul>
<hr>
</section>
<section id="exercise-agency-vs.-automation" class="level3">
<h3 class="anchored" data-anchor-id="exercise-agency-vs.-automation">Exercise: Agency vs.&nbsp;Automation</h3>
</section>
</section>
</section>
<section id="applying-probes-and-concepts" class="level1">
<h1>22. Applying Probes and Concepts</h1>
<ul>
<li><p>application to case study</p></li>
<li><p>testing concept vectors</p></li>
<li><p>relationship with sufficiency</p></li>
<li><p>user study evaluation</p></li>
</ul>
<hr>
<section id="learning-outcomes-19" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-19">Learning Outcomes</h3>
<ul>
<li>Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks</li>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="readings-5" class="level3">
<h3 class="anchored" data-anchor-id="readings-5">Readings</h3>
<p>Alain, G., &amp; Bengio, Y. (2017). Understanding intermediate layers using linear classifier probes. OpenReview. https://openreview.net/forum?id=ryF7rTqgl</p>
<p>Schmalwasser, L., Penzel, N., Denzler, J., &amp; Niebling, J. (2025). FastCAV: Efficient computation of concept activation vectors for explaining deep neural networks. In Proceedings of the 42nd International Conference on Machine Learning. https://openreview.net/forum?id=kRmfzTfIGe</p>
<hr>
</section>
<section id="extracting-activations" class="level3">
<h3 class="anchored" data-anchor-id="extracting-activations">Extracting activations</h3>
<p>In PyTorch, use <code>register_forward_hook</code> to capture <span class="math inline">\(h_l(x)\)</span> during forward pass.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>activations <span class="op">=</span> {}</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> activation_fun(name):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hook(model, <span class="bu">input</span>, output):</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        activations[name] <span class="op">=</span> output.detach()</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hook</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Register hooks on layers of interest</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torchvision.models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>).<span class="bu">eval</span>()</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>model.layer3.register_forward_hook(activation_fun(<span class="st">'layer3'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="testing-cavs" class="level2">
<h2 class="anchored" data-anchor-id="testing-cavs">Testing CAVs</h2>
<hr>
<section id="spurious-concepts-why-test" class="level3">
<h3 class="anchored" data-anchor-id="spurious-concepts-why-test">Spurious Concepts: Why Test?</h3>
<ul>
<li><p>High-dimensional spaces (<span class="math inline">\(d=2048\)</span>): almost any two 50-image sets are separable. Random directions may align with gradients by chance.</p></li>
<li><p>Need to calibrate. Is TCAV=0.6 high or just noise?</p></li>
</ul>
<hr>
<p><strong>Step 4</strong>. Let <span class="math inline">\(N_k\)</span> be the number of images in class <span class="math inline">\(k\)</span>. The <strong>TCAV Score</strong> (<span class="math inline">\(T_k\)</span>) is:</p>
<p><span class="math display">\[T_k = \frac{1}{N_k}\left|\{x \in \text{Class } k : S_k(x) &gt; 0\}\right|\]</span></p>
<ul>
<li><span class="math inline">\(T_k &gt; 0.5\)</span>: concept generally increases prediction</li>
<li><span class="math inline">\(T_k &lt; 0.5\)</span>: concept generally decreases prediction</li>
</ul>
<p><span style="font-size: 18px;"> <img src="figures/concepts_step4.png" width="600/"><br> Concept: “stripes” </span></p>
<hr>
<p><strong>Step 5</strong>. Test significance by learning reference <span class="math inline">\(\vec{v}\)</span> trained on random image sets.</p>
<ul>
<li>Run CAV process 50+ times with different random negative sets</li>
<li>Generate null distribution of TCAV scores</li>
<li>Two-sided t-test: is concept’s <span class="math inline">\(T_k\)</span> statistically significant (<span class="math inline">\(p &lt; 0.05\)</span>)?</li>
</ul>
<p><span style="font-size: 18px;"> <img src="figures/concepts_step5.png" width="600/"><br> Concept: “stripes” </span></p>
<hr>
</section>
<section id="real-world-example-medical-imaging" class="level3">
<h3 class="anchored" data-anchor-id="real-world-example-medical-imaging">Real-World Example: Medical Imaging</h3>
<ul>
<li><strong>Model</strong>. Melanoma detector</li>
<li><strong>TCAV audit</strong>. High sensitivity to “ruler markings”</li>
<li><strong>Conclusion</strong>. Spurious correlation (rulers mark dangerous lesions)</li>
</ul>
<hr>
</section>
<section id="concepts-data-structure" class="level3">
<h3 class="anchored" data-anchor-id="concepts-data-structure">Concepts data structure</h3>
<p>We need three things.</p>
<ol type="1">
<li><strong>Concept Sets</strong>. Folders of images representing “trees,” “water,” “parking,” etc.</li>
<li><strong>Random Pool</strong>. Large, diverse set of images (e.g., ImageNet) for null hypothesis</li>
<li><strong>Layer Activations</strong>. Dictionary mapping <code>layer_name</code> <span class="math inline">\(\to\)</span> <code>tensor(N, dim)</code></li>
</ol>
<hr>
</section>
<section id="concepts-data-structure-1" class="level3">
<h3 class="anchored" data-anchor-id="concepts-data-structure-1">Concepts data structure</h3>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> captum.concept <span class="im">import</span> TCAV, Concept</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> captum.attr <span class="im">import</span> LayerIntegratedGradients</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. load model and specify layers</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torchvision.models.googlenet(pretrained<span class="op">=</span><span class="va">True</span>).<span class="bu">eval</span>()</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [<span class="st">'inception4c'</span>, <span class="st">'inception4d'</span>, <span class="st">'inception4e'</span>]</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. folders contain example images</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>stripes <span class="op">=</span> Concept(<span class="bu">id</span><span class="op">=</span><span class="dv">0</span>, name<span class="op">=</span><span class="st">"striped"</span>, data_iter<span class="op">=</span>load_concept(<span class="st">"striped/"</span>))</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>random <span class="op">=</span> Concept(<span class="bu">id</span><span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"random"</span>, data_iter<span class="op">=</span>load_concept(<span class="st">"random/"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="estimating-concept-directions" class="level3">
<h3 class="anchored" data-anchor-id="estimating-concept-directions">Estimating concept directions</h3>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. initialize tcav class</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>tcav <span class="op">=</span> TCAV(model<span class="op">=</span>model, layers<span class="op">=</span>layers,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>            layer_attr_method<span class="op">=</span>LayerIntegratedGradients(model, <span class="va">None</span>))</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. TCAV test: does "stripes" influence "zebra" predictions?</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> tcav.interpret(inputs<span class="op">=</span>zebra_images,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                        experimental_sets<span class="op">=</span>[[stripes, random]],</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>                        target<span class="op">=</span><span class="dv">340</span>)  <span class="co"># zebra class</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="fastcav" class="level3">
<h3 class="anchored" data-anchor-id="fastcav">FastCAV</h3>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastcav <span class="im">import</span> FastCAVCaptumClassifier</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>tcav <span class="op">=</span> TCAV(model<span class="op">=</span>model, layers<span class="op">=</span>layers,</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>            classifier<span class="op">=</span>FastCAVCaptumClassifier(),  <span class="co"># &lt;-- only change</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>            layer_attr_method<span class="op">=</span>LayerIntegratedGradients(model, <span class="va">None</span>))</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Same workflow, identical results, faster</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> tcav.interpret(inputs<span class="op">=</span>zebra_images, experimental_sets<span class="op">=</span>[[stripes, random]], target<span class="op">=</span><span class="dv">340</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="outputs-1" class="level3">
<h3 class="anchored" data-anchor-id="outputs-1">Outputs</h3>
<p><strong>TCAV Score</strong>. Nested dictionary structure</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>tcav_scores <span class="op">=</span> {</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'0-3'</span>: {  <span class="co"># concept pair IDs</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">'inception4c'</span>: {</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>            <span class="st">'sign_count'</span>: tensor([<span class="fl">0.98</span>, <span class="fl">0.02</span>]),  <span class="co"># fraction positive</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">'magnitude'</span>: tensor([<span class="fl">1.97</span>, <span class="op">-</span><span class="fl">1.97</span>])    <span class="co"># average sensitivity</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>            ...</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>sign_count</strong>. Fraction of inputs where concept increases prediction (TCAV score proper)</p>
<p><strong>magnitude</strong>. Average directional sensitivity across inputs</p>
<p><code>sign_count[0] = 0.98</code> means “stripes” positively influences 98% of zebra predictions</p>
<hr>
</section>
</section>
<section id="user-studies" class="level2">
<h2 class="anchored" data-anchor-id="user-studies">User Studies</h2>
<hr>
<section id="goals" class="level3">
<h3 class="anchored" data-anchor-id="goals">Goals</h3>
<hr>
</section>
<section id="example-study" class="level3">
<h3 class="anchored" data-anchor-id="example-study">Example Study</h3>
<hr>
</section>
<section id="experimental-design" class="level3">
<h3 class="anchored" data-anchor-id="experimental-design">Experimental Design</h3>
<hr>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<hr>
</section>
<section id="interpretation-2" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-2">Interpretation</h3>
<hr>
</section>
<section id="exercise-complementarity" class="level3">
<h3 class="anchored" data-anchor-id="exercise-complementarity">Exercise: Complementarity</h3>
</section>
</section>
</section>
<section id="sparse-autoencoder-design" class="level1">
<h1>23. Sparse Autoencoder Design</h1>
<ul>
<li><p>motivating application</p></li>
<li><p>optimization objective</p></li>
<li><p>learning algorithm</p></li>
<li><p>interpreting outputs</p></li>
</ul>
<hr>
<section id="learning-outcomes-20" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-20">Learning Outcomes</h3>
<ul>
<li>Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks</li>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="motivating-case-study-6" class="level3">
<h3 class="anchored" data-anchor-id="motivating-case-study-6">Motivating Case Study</h3>
<p>Proteins are the building blocks of cells. Each protein is a sequence of amino acids, and the structure and function of these proteins can be studied using protein language models.</p>
<hr>
</section>
<section id="questions" class="level3">
<h3 class="anchored" data-anchor-id="questions">Questions</h3>
<p>What are the most important biological features that have been learned by protein language models?</p>
<hr>
</section>
<section id="statistical-formulation-6" class="level3">
<h3 class="anchored" data-anchor-id="statistical-formulation-6">Statistical Formulation</h3>
<p><strong>Protein Sequences</strong>. One-hot encoded amino acid sequences <span class="math inline">\(x \in \mathbf{R}^{26 \times T}\)</span>. Lengths <span class="math inline">\(T\)</span> may vary.</p>
<p><strong>Foundation Model</strong> <span class="math inline">\(f\left(x; \theta\right)\)</span>. Transforms raw sequence data into representations <span class="math inline">\(h^{l} = \left(index, feature\right)\)</span> across layers <span class="math inline">\(l\)</span>.</p>
<hr>
</section>
<section id="the-polysemanticity-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-polysemanticity-problem">The Polysemanticity Problem</h3>
<p>Individual neurons in neural networks respond to multiple unrelated concepts.</p>
<p>For example, a single neuron might activate for: * Arabic script * Genetic sequences * Mathematical notation * Base64 encoded data</p>
<p>This <strong>polysemanticity</strong> makes interpretation impossible at the neuron level.</p>
<hr>
</section>
<section id="superposition-hypothesis" class="level3">
<h3 class="anchored" data-anchor-id="superposition-hypothesis">Superposition Hypothesis</h3>
<p><strong>Linear Representation</strong>. Features <span class="math inline">\(f_1, \ldots, f_F\)</span> are represented as directions <span class="math inline">\(W_1, \ldots, W_F \in \mathbb{R}^D\)</span>: <span class="math display">\[x = \sum_{i=1}^F x_{f_i} W_i\]</span> where <span class="math inline">\(x_{f_i}\)</span> is the activation strength of feature <span class="math inline">\(i\)</span>.</p>
<p><strong>Naive expectation</strong>. Can only represent <span class="math inline">\(F \leq D\)</span> features (one per dimension).</p>
<p><strong>Superposition</strong>. When features are sparse (each <span class="math inline">\(x_{f_i} &gt; 0\)</span> rarely), can represent <span class="math inline">\(F \gg D\)</span> features.</p>
<ul>
<li>Sparsity means most interference terms <span class="math inline">\(x_{f_i} x_{f_j} \langle W_i, W_j \rangle\)</span> are zero</li>
<li>Neurons observe <span class="math inline">\(\langle w_n, x \rangle = \sum_i x_{f_i} \langle w_n, W_i \rangle\)</span> (mixture → polysemanticity)</li>
<li>Feature directions <span class="math inline">\(W_i\)</span> are recoverable via sparse dictionary learning</li>
</ul>
<hr>
</section>
<section id="the-dictionary-learning-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-dictionary-learning-problem">The Dictionary Learning Problem</h3>
<p>Given activations <span class="math inline">\(x \in \mathbf{R}^D\)</span>, find:</p>
<ul>
<li>Feature directions <span class="math inline">\(W^{dec} \in \mathbf{R}^{D \times F}\)</span> (the “dictionary atoms”)</li>
<li>Sparse coefficients <span class="math inline">\(f(x) \in \mathbf{R}^F\)</span></li>
</ul>
<p>Such that: <span class="math inline">\(x \approx \sum_{i=1}^F f_i(x) W_{\cdot,i}^{dec}\)</span></p>
<p><strong>Constraint</strong>. <span class="math inline">\(\|f(x)\|_0 \ll F\)</span> (most coefficients are zero)</p>
<hr>
</section>
<section id="sparse-autoencoder-architecture" class="level2">
<h2 class="anchored" data-anchor-id="sparse-autoencoder-architecture">Sparse Autoencoder Architecture</h2>
<hr>
<section id="model-structure" class="level3">
<h3 class="anchored" data-anchor-id="model-structure">Model Structure</h3>
<p><strong>Encoder</strong>. <span class="math display">\[f(x) = \text{ReLU}(W^{enc} x + b^{enc})\]</span> where <span class="math inline">\(W^{enc} \in \mathbf{R}^{F \times D}\)</span></p>
<p><strong>Decoder</strong>. <span class="math display">\[\hat{x} = W^{dec} f(x) + b^{dec}\]</span> where <span class="math inline">\(W^{dec} \in \mathbf{R}^{D \times F}\)</span></p>
<p>Overcomplete Basis: <span class="math inline">\(F \gg D\)</span></p>
<hr>
</section>
<section id="objective-function-1" class="level3">
<h3 class="anchored" data-anchor-id="objective-function-1">Objective Function</h3>
<p><span class="math display">\[\mathcal{L} = \mathbf{E}_{x} \left[ \|x - \hat{x}\|_2^2 + \lambda \sum_{i=1}^F f_i(x) \|W_{\cdot,i}^{dec}\|_2 \right]\]</span></p>
<p><strong>Terms</strong>. 1. Reconstruction error 2. Sparsity penalty weighted by atom norm</p>
<p>The weights <span class="math inline">\(f_{i}\left(x\right)\)</span> replace the usual unit norm constraint.</p>
<hr>
</section>
<section id="soft-thresholding" class="level3">
<h3 class="anchored" data-anchor-id="soft-thresholding">Soft Thresholding</h3>
<p>The encoder <span class="math inline">\(\text{ReLU}(W^{enc} x + b^{enc})\)</span> approximates the solution to: <span class="math display">\[\min_f \frac{1}{2}\|x - W^{dec}f\|_2^2 + \lambda\|f\|_1\]</span></p>
<p>This is a soft-thresholding operation: <span class="math inline">\(S_\lambda(v)_i = \text{sign}(v_i) \max(0, |v_i| - \lambda)\)</span></p>
<hr>
</section>
<section id="sae-vs-pca" class="level3">
<h3 class="anchored" data-anchor-id="sae-vs-pca">SAE vs PCA</h3>
<table class="table">
<thead>
<tr class="header">
<th>Property</th>
<th>PCA</th>
<th>SAE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Basis</td>
<td>Orthogonal</td>
<td>Overcomplete, non-orthogonal</td>
</tr>
<tr class="even">
<td>Objective</td>
<td>Maximize variance</td>
<td>Sparse reconstruction</td>
</tr>
<tr class="odd">
<td>Features</td>
<td>Dense</td>
<td>Sparse</td>
</tr>
<tr class="even">
<td>Semantics</td>
<td>Polysemantic</td>
<td>Monosemantic</td>
</tr>
</tbody>
</table>
<p>PCA finds directions of variation. SAEs find directions of meaning.</p>
<hr>
</section>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">Optimization</h2>
<hr>
<section id="proximal-gradient-methods" class="level3">
<h3 class="anchored" data-anchor-id="proximal-gradient-methods">Proximal Gradient Methods</h3>
<p>When <span class="math inline">\(f\)</span> is differentiable and <span class="math inline">\(g\)</span> is convex (not necessarily differentiable), we can solve</p>
<p><span class="math display">\[\min f(x) + g(x)\]</span></p>
<p>using iterates</p>
<p><span class="math display">\[x^{(k+1)} = \text{prox}_{\eta g} (x^{(k)} - \eta \nabla f(x^{(k)}))\]</span></p>
<p>For SAEs, <span class="math inline">\(g = \lambda\|f\|_1\)</span>: 1. Gradient step on reconstruction error 2. Soft-threshold to enforce sparsity: <span class="math inline">\(\text{prox}_{\lambda\eta}(v) = S_{\lambda\eta}(v)\)</span></p>
<hr>
</section>
<section id="fista-fast-iterative-soft-thresholding" class="level3">
<h3 class="anchored" data-anchor-id="fista-fast-iterative-soft-thresholding">FISTA (Fast Iterative Soft-Thresholding)</h3>
<p>Achieves optimal <span class="math inline">\(O(1/k^2)\)</span> convergence via momentum:</p>
<ol type="1">
<li><span class="math inline">\(f^{(k)} = S_{\lambda/L} (y^{(k)} - \frac{1}{L} \nabla_f \|x - W^{dec}f\|_2^2|_{f=y^{(k)}})\)</span></li>
<li><span class="math inline">\(t_{k+1} = \frac{1 + \sqrt{1 + 4t_k^2}}{2}\)</span></li>
<li><span class="math inline">\(y^{(k+1)} = f^{(k)} + \frac{t_k - 1}{t_{k+1}} (f^{(k)} - f^{(k-1)})\)</span></li>
</ol>
<hr>
</section>
<section id="experimental-setup" class="level3">
<h3 class="anchored" data-anchor-id="experimental-setup">Experimental Setup</h3>
<p><strong>Model</strong>. Claude 3 Sonnet (middle layer residual stream)</p>
<p><strong>Dictionary sizes</strong>. 1M, 4M, 34M features</p>
<p><strong>Training</strong>. * Data: Mix similar to pretraining (The Pile, Common Crawl) * No RLHF-style dialogue data * Single epoch</p>
<p><strong>Dead features</strong>. 2% (1M), 35% (4M), 65% (34M)</p>
<hr>
</section>
<section id="hyperparameter-tuning-1" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning-1">Hyperparameter Tuning</h3>
<p>Observe empirical scaling laws.</p>
<p><img src="figures/sae_scaling_law.png" class="center"></p>
<hr>
</section>
</section>
<section id="interpreting-outputs" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-outputs">Interpreting Outputs</h2>
<hr>
<section id="the-learned-dictionary" class="level3">
<h3 class="anchored" data-anchor-id="the-learned-dictionary">The Learned Dictionary</h3>
<ul>
<li><p>Columns <span class="math inline">\(W_{\cdot,i}^{dec}\)</span> are feature directions in activation space</p></li>
<li><p>Coefficients <span class="math inline">\(f_i(x)\)</span> indicate strength of feature <span class="math inline">\(i\)</span> for input <span class="math inline">\(x\)</span></p></li>
</ul>
<p>For Claude 3 Sonnet (34M SAE features):</p>
<ul>
<li>Average <span class="math inline">\(L_0 &lt; 300\)</span> active features per token</li>
<li>Reconstruction explains 65% of variance</li>
</ul>
<hr>
</section>
<section id="visualization-method" class="level3">
<h3 class="anchored" data-anchor-id="visualization-method">Visualization Method</h3>
<p>For each feature <span class="math inline">\(i\)</span>:</p>
<ol type="1">
<li>Compute <span class="math inline">\(f_i(x)\)</span> over dataset</li>
<li>Collect top-<span class="math inline">\(k\)</span> examples by activation strength</li>
<li>Inspect contexts where feature fires</li>
</ol>
<p>What concept consistently appears in high-activation contexts?</p>
<hr>
</section>
<section id="feature-examples" class="level3">
<h3 class="anchored" data-anchor-id="feature-examples">Feature Examples</h3>
<p>We’ll go over these in detail next lecture.</p>
<p><strong>Golden Gate Bridge</strong> (34M/31164353): * Descriptions in multiple languages * Images of the bridge * Related SF landmarks (weaker)</p>
<p><strong>Immunology</strong> (1M/533737): * Neighborhood includes immunodeficiency, vaccines, organ systems * Separate cluster for legal immunity</p>
<hr>
</section>
<section id="exercise-implement-fista" class="level3">
<h3 class="anchored" data-anchor-id="exercise-implement-fista">Exercise: Implement FISTA</h3>
<p>Given <span class="math inline">\(x \in \mathbf{R}^D\)</span>, <span class="math inline">\(W^{dec} \in \mathbf{R}^{D \times F}\)</span>, <span class="math inline">\(\lambda &gt; 0\)</span>:</p>
<p>Implement FISTA to solve: <span class="math display">\[\min_f \frac{1}{2}\|x - W^{dec}f\|_2^2 + \lambda\|f\|_1\]</span></p>
<p>Compare convergence rate to standard ISTA.</p>
</section>
</section>
</section>
<section id="interpreting-and-applying-saes" class="level1">
<h1>24. Interpreting and Applying SAEs</h1>
<ul>
<li><p>interpreting features</p></li>
<li><p>application to case study</p></li>
<li><p>model safety and control</p></li>
</ul>
<hr>
<section id="learning-outcomes-21" class="level3">
<h3 class="anchored" data-anchor-id="learning-outcomes-21">Learning Outcomes</h3>
<ul>
<li>Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks</li>
<li>Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.</li>
<li>Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.</li>
</ul>
<hr>
</section>
<section id="monosemanticity" class="level3">
<h3 class="anchored" data-anchor-id="monosemanticity">Monosemanticity</h3>
<p><strong>Definition</strong>. A feature responds to a single coherent concept.</p>
<p>They observe this empirically in their LLM:</p>
<ul>
<li><strong>Cross-lingual</strong>. Same concept in multiple languages</li>
<li><strong>Cross-modal</strong>. Same concept in text and images</li>
<li><strong>Cross-abstraction</strong>. Abstract discussion and concrete instances</li>
</ul>
<p>For example, the code vulnerability feature responds to:</p>
<ul>
<li>Buffer overflow in C code</li>
<li>Blog post discussing CVEs</li>
<li>Image of XKCD comic about SQL injection</li>
</ul>
<hr>
</section>
<section id="feature-geometry" class="level3">
<h3 class="anchored" data-anchor-id="feature-geometry">Feature Geometry</h3>
<p>Cosine similarity between atom vectors <span class="math inline">\(W_{\cdot,i}^{dec}\)</span> and <span class="math inline">\(W_{\cdot,j}^{dec}\)</span> correlates with semantic similarity.</p>
<p><strong>Neighborhoods</strong>.</p>
<ul>
<li>Close features share meaning</li>
<li>Distances form semantic hierarchies</li>
</ul>
<p><strong>Example</strong>. Golden Gate Bridge feature neighbors: Alcatraz (closest), Lake Tahoe, Yosemite, other landmarks (further)</p>
<hr>
</section>
<section id="feature-splitting" class="level3">
<h3 class="anchored" data-anchor-id="feature-splitting">Feature Splitting</h3>
<p>As <span class="math inline">\(F\)</span> increases, coarse features split into finer ones. For example,</p>
<ul>
<li>1M SAE: “San Francisco” (single feature)</li>
<li>4M SAE: 2 features</li>
<li>34M SAE: 11 fine-grained features</li>
</ul>
<p>Rare concepts require larger dictionaries.</p>
<hr>
</section>
<section id="frequency-size-relationship" class="level3">
<h3 class="anchored" data-anchor-id="frequency-size-relationship">Frequency-Size Relationship</h3>
<p>Dictionary size needed for concept with frequency <span class="math inline">\(p\)</span>: <span class="math display">\[F \approx \frac{c}{p}\]</span></p>
<p>50% coverage threshold inversely proportional to number of “alive” features.</p>
<p>Suggests <span class="math inline">\(F\)</span> should be comparable to inverse rarest concept frequency.</p>
<hr>
</section>
<section id="validation-methods" class="level2">
<h2 class="anchored" data-anchor-id="validation-methods">Validation Methods</h2>
<hr>
<section id="manual-inspection" class="level3">
<h3 class="anchored" data-anchor-id="manual-inspection">Manual Inspection</h3>
<ol type="1">
<li>Examine top-20 activating examples</li>
<li>Hypothesize feature meaning</li>
<li>Check random samples from activation bins</li>
<li>Test on synthetic prompts</li>
</ol>
<p>This is limited: subjective, labor-intensive, doesn’t scale to 1M+ features.</p>
<hr>
</section>
<section id="automated-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="automated-interpretability">Automated Interpretability</h3>
<ol type="1">
<li>Show an LLM high-activation examples</li>
<li>Request natural language description</li>
<li>Score held-out examples against the description</li>
</ol>
<p><strong>Rubric</strong>.</p>
<ul>
<li>0: Irrelevant</li>
<li>1: Vaguely related</li>
<li>2: Loosely related</li>
<li>3: Perfectly matches</li>
</ul>
<p>They found strong activations <span class="math inline">\(\to\)</span> high scores.</p>
<hr>
</section>
<section id="specificity" class="level3">
<h3 class="anchored" data-anchor-id="specificity">Specificity</h3>
<p><strong>Definition</strong>. When a feature fires, is the concept present?</p>
<p><strong>Measurement</strong>. Automated scoring of activating examples.</p>
<p><strong>Observation</strong>.</p>
<ul>
<li>Strong activations: high specificity</li>
<li>Weak activations: lower specificity</li>
</ul>
<p><strong>Possible causes</strong>.</p>
<ul>
<li>Activation encodes confidence</li>
<li>Central vs peripheral examples</li>
<li>Dictionary learning is not perfect</li>
</ul>
<hr>
</section>
<section id="sensitivity" class="level3">
<h3 class="anchored" data-anchor-id="sensitivity">Sensitivity</h3>
<p><strong>Definition</strong>. When a concept is present, does the feature fire?</p>
<p><strong>Challenge</strong>. Hard to sample unbiased concept instances.</p>
<p><strong>Sanity check</strong>.</p>
<ul>
<li>Translate triggering text to multiple languages – does the feature still fire?</li>
<li>Example showing the Golden Gate Bridge feature fires on Wikipedia article in Chinese, Japanese, Russian, French, Greek.</li>
</ul>
<hr>
</section>
</section>
<section id="feature-types" class="level2">
<h2 class="anchored" data-anchor-id="feature-types">Feature Types</h2>
<hr>
<section id="addition-feature-1m697189" class="level3">
<h3 class="anchored" data-anchor-id="addition-feature-1m697189">Addition feature (1M/697189)</h3>
<ul>
<li>Fires on function names that add</li>
<li>Fires at end of addition function definitions</li>
<li>Handles function composition: fires on <code>bar()</code> that calls <code>add()</code></li>
</ul>
<p><strong>Check</strong>. Clamping feature on non-addition code <span class="math inline">\(\to\)</span> model treats it as addition.</p>
<hr>
</section>
<section id="code-error-feature" class="level3">
<h3 class="anchored" data-anchor-id="code-error-feature">Code Error Feature</h3>
<p>Activates on…</p>
<ul>
<li>Array overflow</li>
<li>Divide by zero</li>
<li>Type errors</li>
<li>Null pointer dereference</li>
<li><code>exit(1)</code></li>
</ul>
<p><strong>Steering</strong>.</p>
<ul>
<li>Clamp high <span class="math inline">\(\to\)</span> hallucinates errors</li>
<li>Clamp low <span class="math inline">\(\to\)</span> ignores real bugs</li>
</ul>
<hr>
</section>
<section id="safety-relevant-features" class="level3">
<h3 class="anchored" data-anchor-id="safety-relevant-features">Safety-Relevant Features</h3>
<p>Probably important for an LLM company to resolve…</p>
<ul>
<li>Deception and manipulation</li>
<li>Power-seeking and treacherous turns</li>
<li>Bias and discrimination</li>
<li>Dangerous content (CBRN, exploits)</li>
<li>Sycophancy</li>
</ul>
<hr>
</section>
<section id="exercise-sae-applications" class="level3">
<h3 class="anchored" data-anchor-id="exercise-sae-applications">Exercise: SAE Applications</h3>
<hr>
</section>
</section>
<section id="case-study-2" class="level2">
<h2 class="anchored" data-anchor-id="case-study-2">Case Study</h2>
<hr>
<section id="computational-intermediates" class="level3">
<h3 class="anchored" data-anchor-id="computational-intermediates">Computational Intermediates</h3>
<p>Features represent intermediate reasoning steps.</p>
<p>“The capital of the state where Kobe Bryant played basketball is ____”</p>
<p><strong>Top features by attribution</strong>.</p>
<ol type="1">
<li>Kobe Bryant</li>
<li>California</li>
<li>“Capital”</li>
<li>Los Angeles</li>
<li>Lakers</li>
</ol>
<hr>
</section>
<section id="comparison-to-neurons" class="level3">
<h3 class="anchored" data-anchor-id="comparison-to-neurons">Comparison to Neurons</h3>
<p><strong>Correlation test</strong>. For random features, measure max correlation with any neuron.</p>
<p><strong>Result</strong>. 82% of features have <span class="math inline">\(r &lt; 0.3\)</span> with all neurons.</p>
<p>Features appear more interpretable than neurons. Qualitatively different, not just neuron activity.</p>
<hr>
</section>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications</h2>
<hr>
<section id="activation-steering" class="level3">
<h3 class="anchored" data-anchor-id="activation-steering">Activation Steering</h3>
<ol type="1">
<li>Decompose: <span class="math inline">\(x = \text{SAE}(x) + \text{error}(x)\)</span></li>
<li>Modify: <span class="math inline">\(f_i(x) \leftarrow s\)</span></li>
<li>Reconstruct: <span class="math inline">\(x_{new} = b^{dec} + \sum_j f_j(x) W_{\cdot,j}^{dec} + \text{error}(x)\)</span></li>
<li>Continue forward pass with <span class="math inline">\(x_{new}\)</span></li>
</ol>
<p>Notes:</p>
<ul>
<li>Typically <span class="math inline">\(s \in [-10, 10] \times \max_x f_i(x)\)</span></li>
<li>Golden Gate Bridge feature at <span class="math inline">\(10\times\)</span> max <span class="math inline">\(\to\)</span> model self-identifies as the bridge.</li>
</ul>
<hr>
</section>
<section id="safety-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="safety-monitoring">Safety Monitoring</h3>
<p><strong>Proposal</strong>. Monitor safety-relevant features during inference.</p>
<p><strong>Example checks</strong>.</p>
<ul>
<li>Does “deception” feature activate during helpful responses?</li>
<li>What activates before dangerous content generation?</li>
<li>Which features necessary for CBRN refusal?</li>
</ul>
<hr>
</section>
<section id="comparison-to-linear-probes" class="level3">
<h3 class="anchored" data-anchor-id="comparison-to-linear-probes">Comparison to Linear Probes</h3>
<p><strong>Few-shot probe baseline</strong>.</p>
<ul>
<li>Mean difference of residual stream on positive vs negative examples</li>
<li>2/7 cases: probes and features equally effective</li>
<li>5/7 cases: features work, probes don’t</li>
</ul>
<p><strong>Benefit of SAEs</strong>.</p>
<ul>
<li>Unsupervised discovery</li>
<li>Amortized cost (one training, many searches)</li>
<li>Unsupervised <span class="math inline">\(\to\)</span> Can find unexpected abstractions</li>
</ul>
<hr>
</section>
<section id="exercise-feature-search" class="level3">
<h3 class="anchored" data-anchor-id="exercise-feature-search">Exercise: Feature Search</h3>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>