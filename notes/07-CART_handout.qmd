---
title: "Gaussian Process Fundamentals"
author: Kris Sankaran
date: 2026-02-04
format:
  html:
    embed-resources: true
    html-math-method: katex
execute:
    echo: true
    message: false
    warning: false
    cache: true
---

$$
\newcommand{\bs}[1]{\mathbf{#1}}
\newcommand{\reals}[1]{\mathbb{R}}
$$

<style>
.purple { color: #7458d1ff; } /* pastel purple */
.orange { color: #F4D4A8; } /* pastel orange */
.green { color: #3bbe67ff; } /* pastel green */
.darkblue { color: #4a9ceaff; } /* pastel dark blue */
.pink { color: #ee6ec3ff; } /* pastel pink */
</style>

```{r}
#| echo: false
library(tidyverse)
theme_set(theme_classic() + theme(panel.border= element_rect(fill = NA, linewidth = .5)))
set.seed(2026)
```

_Readings: [ISLR Section 8.1](https://hastie.su.domains/ISLR2/ISLRv2_corrected_June_2023.pdf.download.html), _[Code](https://github.com/krisrs1128/stat479_notes/blob/master/notes/07-CART_handout.qmd)_

## Setup

**Goal.** Given data $\{\left({x_i, y_i}\right)\}_{i = 1}^{N}$, develop a predictor for $y_{i}$ that handles both regression (when $y_i$ is continuous) and classification (when $y_i$ is categorical).

![](figures/tree_motivation_diagram.png){width="400px"}

**Requirements.**

  - Tree structure. The decision function $\hat{f}\left(x\right)$ must be
  representable as a tree. Each prediction can be traced through a sequence of
  decision nodes. This supports interpretability.
  - Flexible inputs. The features $x_{i}$ may contain continuous, categorical,
  or mixed-type components.
  - Feature interactions. The model must have the flexibility to learn
  interactions between features. That is, $\hat{f}\left(x\right)$ can represent
  relationships where the effect of feature $x_{j}$ on $y$ depends on the value
  of $x_{k}$ (e.g., $y_{i}$ is in class A if and only if _both_ $x_{i1}$ and
  $x_{i2}$ are positive).

**Approach.**

- Partition representation. Observe a correspondence between the $L$ leaves of a
tree diagram and size $L$ partitions of the input space $\mathcal{X} = R_{1}
\cup R_{2} \cup \dots \cup R_{L}$.
- Prediction rule. For a test point $x^* \in R_{l}$, predict using the training
samples in $R_{l}$: average $y_{i}$ for regression or majority class for
classification.
- Tree construction. Grow the tree recursively using a greedy algorithm that
optimizes locally for good prediction performance. Prune the tree using a
validation set to ensure generalization ability.

## Trees $\iff$ Partitions

1. For now, let's consider $x = \left(x_{1}, \dots, x_{J}\right)$ with only
continuous features. Each node in a decision tree tests whether a feature $x_{j}
\geq s$ or $x_{j} < s$ for some threshold $s$. We proceed down the right branch
iff $x \geq s$.

1. A leaf node is a node without any descendants. Following the path from the
root to a leaf, we impose a series of constraints on $x$ corresponding to
axis-aligned rectangles.

1. Since each $x$ follows exactly one root-to-leaf path, the $L$ leaves define
$L$ nonoverlapping regions $\left(R_{l}\right)_{l = 1}^{L}$ that cover the
entire input space: $\mathcal{X} = R_{1} \cup R_{2} \cup \dots \cup R_{L}$.

## Growing Algorithm

1. **1D case**. How should we learn the partition? If we have only one feature
and are only allowed one split, then it's natural to pick $s$ to minimize

   $$
\begin{align*}
\sum_{i: x_i \in R^{\text{left}}(s)} (y_i - \hat{y}_{R}^{\text{left}})^2 + \sum_{i: x_i \in R^{\text{right}}(s)} (y_i - \hat{y}_{R^{\text{right}}})^2
\end{align*}
   $$

   where $\hat{y}_{R} = \frac{1}{|R|}\sum_{i: x_i \in R} y_i$ is the mean
training response in region $R_{l}$.

1. **Inductive step**. Suppose we have already made $\ell$ splits, so we have
$\ell + 1$ disjoint intervals $R_{1}, \dots, R_{\ell + 1}$ with current total
residual sum of squares (RSS):
$$
\sum_{l=1}^{\ell+1} \sum_{i: x_i \in R_l} (y_i - \hat{y}_{R_l})^2
$$
To make the next split, consider the RSS reduction when splitting region $l$ at threshold $s$:
$$
\Delta \text{RSS}(l, s) = \sum_{i: x_i \in R_l} (y_i - \hat{y}_{R_l})^2 - \left[\sum_{i: x_i \in R_l^\text{left}(s)} (y_i - \hat{y}_{R_l^\text{left}})^2 + \sum_{i: x_i \in R_l^\text{right}(s)} (y_i - \hat{y}_{R_l^\text{right}})^2\right]
$$
Here, $R_l^\text{left}(s)$ and $R_l^\text{right}(s)$ denotes the left and right
intervals made when splitting $R_l$ at $s$. We can test all regions $l \in \{1,
\dots, \ell + 1\}$ and a fine grid of $s$ within each $R_{l}$. The best split is:
$$
\left(l^*, s^*\right) := \arg \max_{l = 1, \dots, \ell + 1} \Delta \text{RSS}\left(l, s\right)
$$

1. We stop growing the tree when we've reached a stopping criterion. For
example, the best possible improvement in RSS might be too small, or all the
leaves might have too few samples for it to be worth considering any more
splits.

1. A similar approach works in higher dimensions. For the first split, we search over all dimensions $j$ and split points $s$ hat minimize
$$
\begin{align*}
\sum_{i: x_i \in R_{j}^{\text{left}}(s)} (y_i - \hat{y}_{R_{j}^{\text{left}}})^2 + \sum_{i: x_i \in R_{j}^{\text{right}}(s)} (y_i - \hat{y}_{R_{j}^{\text{right}}})^2
\end{align*}
$$

1. A similar approach works in higher dimensions -- we just need to also
optimize over which feature to split on in each inductive step. Specifically,
consider splitting leaf $l$ along feature $j$ at threshold $s$. The associated
RSS reduction is:
   $$
\Delta \text{RSS}(l, j, s) = \sum_{i: x_i \in R_l} (y_i - \hat{y}_{R_l})^2 - \left[\sum_{i: x_i \in R_l^{\text{left}}(j,s)} (y_i - \hat{y}_{R_l^{\text{left}}})^2 + \sum_{i: x_i \in R_l^{\text{right}}(j,s)} (y_i - \hat{y}_{R_l^{\text{right}}})^2\right]
   $$
   where $R_l^{\text{left}}(j,s) = \{x \in R_l : x_j \leq s\}$ and
$R_l^{\text{right}}(j,s) = \{x \in R_l : x_j > s\}$. We search over all
candidate regions $l$, features $j$, and thresholds $s$ to find:
   $$
(l^*, j^*, s^*) := \arg\max_{l, j, s} \Delta \text{RSS}(l, j, s)
$$

## Classification

## Pruning

## Code Example
