---
title: "Classification and Regression Trees"
author: Kris Sankaran
date: 2026-02-09
format:
  html:
    embed-resources: true
    html-math-method: katex
execute:
    echo: true
    message: false
    warning: false
    cache: false
---

::: {style="display: none;"}
$$
\newcommand{\bs}[1]{\mathbf{#1}}
\newcommand{\reals}[1]{\mathbb{R}}
$$
:::

<style>
.purple { color: #7458d1ff; } /* pastel purple */
.orange { color: #F4D4A8; } /* pastel orange */
.green { color: #3bbe67ff; } /* pastel green */
.darkblue { color: #4a9ceaff; } /* pastel dark blue */
.pink { color: #ee6ec3ff; } /* pastel pink */
</style>

```{r}
#| echo: false
library(tidyverse)
library(scico)
theme_set(theme_classic() + theme(panel.border= element_rect(fill = NA, linewidth = .5)))
set.seed(2026)
```

_Readings: [ISLR Section 8.1](https://hastie.su.domains/ISLR2/ISLRv2_corrected_June_2023.pdf.download.html)_, _[Code](https://github.com/krisrs1128/stat479_notes/blob/master/notes/07-CART_handout.qmd)_

Bullet items with $^{\dagger}$ are not in the reading, so not tested.

## Setup

**Goal.** Given data $\{\left({x_i, y_i}\right)\}_{i = 1}^{N}$, develop a predictor for $y_{i}$ that handles both regression (when $y_i$ is continuous) and classification (when $y_i$ is categorical).

**Requirements.**

  - _Tree structure_. The decision function $\hat{f}\left(x\right)$ must be
  representable as a tree. Each prediction can be traced through a sequence of
  decision nodes. This supports interpretability.
  - _Flexible inputs_. The features $x_{i}$ may contain continuous, categorical,
  or mixed-type components.
  - _Feature interactions_. The model must have the flexibility to learn
  feature interactions. That is, $\hat{f}\left(x\right)$ can represent
  relationships where the effect of feature $x_{j}$ on $y$ depends on the value
  of $x_{k}$ (e.g., $y_{i}$ is in class A if and only if _both_ $x_{i1}$ and
  $x_{i2}$ are positive).

![A heuristic decision tree used by the San Diego Medical Center for assessing heart attack patients, one of the [inspirations](https://rafalab.dfci.harvard.edu/pages/649/section-11.pdf) behind CART.](figures/heart_attack_cart.gif){width="500px"}

**Approach.**

- _Partition representation_. Observe a correspondence between the $L$ leaves of a
tree diagram and size $L$ partitions of the input space $\mathcal{X} = R_{1}
\cup R_{2} \cup \dots \cup R_{L}$.
- _Prediction rule_. For a test point $x^* \in R_{l}$, predict using the training
samples in $R_{l}$: average $y_{i}$ for regression or majority class for
classification.
- _Tree construction_. Grow the tree recursively using a greedy algorithm that
optimizes locally for good prediction performance. Prune the tree using a
validation set to ensure generalization ability.

## Trees $\iff$ Partitions

1. For now, let's consider $x = \left(x_{1}, \dots, x_{J}\right)$ with only
continuous features. Each node in a decision tree tests whether a feature $x_{j}
\geq s$ or $x_{j} < s$ for some threshold $s$. We proceed down the right branch
if and only if $x_j \geq s$.


1. A leaf node is a node without any descendants. Following the path from the
root to a leaf, we impose a series of constraints on $x$ corresponding to
axis-aligned rectangles.

   :::: {.columns}
   ::: {.column width="50%"}
   ![](figures/partition_equivalence.png){width="400px"}
   :::
   ::: {.column width="50%"}
   ![](figures/two_dimensional_split.png){width="400px"}
   :::
   ::::

1. Since each $x$ follows exactly one root-to-leaf path, the $L$ leaves define
$L$ nonoverlapping regions $\left(R_{l}\right)_{l = 1}^{L}$ that cover the
entire input space: $\mathcal{X} = R_{1} \cup R_{2} \cup \dots \cup R_{L}$.

   ![](figures/two_dimensional_split_further.png){width="300px"}

   _Exercise: What tree does this partition correspond to?_

## Growing Algorithm

1. **1D case**. How should we learn the partition from data $\{\left({x_i,
y_i}\right)\}_{i = 1}^{N}$? First suppose we have only one feature and are only
allowed one split. The resulting partition has only two elements
$R^{\text{left}}$ and $R^{\text{right}}$, divided at a threshold $s$. We choose
the $s$ that minimizes
   $$
\begin{align*}
\sum_{i: x_i \in R^{\text{left}}(s)} (y_i - \bar{y}_{R^{\text{left}}})^2 + \sum_{i: x_i \in R^{\text{right}}(s)} (y_i - \bar{y}_{R^{\text{right}}})^2
\end{align*}
   $$

   where $\bar{y}_{R} = \frac{1}{|R|}\sum_{i: x_i \in R} y_i$ is the mean
training response in region $R_{l}$.

   ![](figures/one_dimensional_fit_Rs.png){width="300px"}


1. **Inductive step**. Suppose we have already made $L$ splits, so we have
$L + 1$ disjoint intervals $R_{1}, \dots, R_{L + 1}$ with current total
residual sum of squares (RSS):
   $$
\sum_{l=1}^{L +1} \sum_{i: x_i \in R_l} (y_i - \bar{y}_{R_l})^2
   $$
   To make the next split, consider the RSS reduction from splitting region $l$ at threshold $s$
   $$
\Delta(l, s) = \sum_{i: x_i \in R_l} (y_i - \bar{y}_{R_l})^2 - \left[\sum_{i: x_i \in R_l^\text{left}(s)} (y_i - \bar{y}_{R_l^\text{left}})^2 + \sum_{i: x_i \in R_l^\text{right}(s)} (y_i - \bar{y}_{R_l^\text{right}})^2\right]
   $$
   Here, $R_l^\text{left}(s)$ and $R_l^\text{right}(s)$ denotes the left and right
intervals made when splitting $R_l$ at $s$. We can test all regions $l \in \{1,
\dots, L + 1\}$ and a fine grid of $s$ within each $R_{l}$. The best split is:
   $$
\left(l^*, s^*\right) := \arg \max_{l = 1, \dots, L + 1} \Delta\left(l, s\right)
$$

   :::: {.columns}
   ::: {.column width="50%"}
   ![](figures/before_second_split.png){width="300px"}
   :::
   ::: {.column width="50%"}
   ![](figures/means_after_splits_1d.png){width="300px"}
   :::
   ::::

1. We stop growing the tree when we've reached a stopping criterion; e.g., when
the best RSS improvement is too small or the leaves have too few samples to
split further.

1. A similar approach works in higher dimensions -- we just need to also
optimize over which feature to split on in each inductive step.

   :::: {.columns}
   ::: {.column width="50%"}
   ![](figures/objective_l1.png)
   :::
   ::: {.column width="50%"}
   ![](figures/objective_l2.png)
   :::
   ::::

   Specifically, consider splitting leaf $l$ along feature $j$ at threshold $s$.
The associated RSS reduction is:
   $$
\Delta(l, j, s) = \sum_{i: x_i \in R_l} (y_i - \bar{y}_{R_l})^2 - \left[\sum_{i: x_i \in R_l^{\text{left}}(j,s)} (y_i - \bar{y}_{R_l^{\text{left}}})^2 + \sum_{i: x_i \in R_l^{\text{right}}(j,s)} (y_i - \bar{y}_{R_l^{\text{right}}})^2\right]
   $$
   where $R_l^{\text{left}}(j,s) = \{x \in R_l : x_j \leq s\}$ and
$R_l^{\text{right}}(j,s) = \{x \in R_l : x_j > s\}$. We search over all
candidate regions $l$, features $j$, and thresholds $s$ to find:
   $$
(l^*, j^*, s^*) := \arg\max_{l, j, s} \Delta(l, j, s)
$$

## Classification

1. So far we've focused on regression. For classification, we still split
recursively but can't use $\Delta\left(l, j, s\right)$ as our objective.

1. Our first thought might be to use classification error rate instead of RSS.
This is not a good idea, because we might want to split a region $R_{l}$ to
improve "node purity," even if it doesn't improve the overall training error
rate. For example, the split below is still worth making, even though the
overall error rate is 40\% in both cases. This is because the error rate in
$R_3$ is now $\approx 27\%$ and is likely to generalize well.

   ```
   R_init (100 points): [●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ○○○○○○○○○○○○○○○○○○○○] 60:40 ratio

   After:
   R_2 (70 pts): [●●●●●●●●●●●●●●●●●●● ○○○○○○○○○○○○○○○○] 38:32
   R_3 (30 pts): [●●●●●●●●●●● ○○○○] 22:8
   ```

1. To measure node purity, we can use either the Gini Index or the Entropy. If
$y$ belongs to one of $K$ classes and if $\hat{p}_{lk}$ denotes the proportion
of class $k$ samples in region $R_{l}$, these are defined as,

   $$
   \begin{align*}
\text{Gini}\left(R_{l}\right) &= \sum_{k= 1}^{K} \hat{p}_{lk}\left(1 - \hat{p}_{lk}\right)^{2}\\
\text{Entropy}\left(R_{l}\right) &= -\sum_{k= 1}^{K} \hat{p}_{lk}\log\left(\hat{p}_{lk}\right)
   \end{align*}
   $$

1. Geometrically, these two functions have similar forms. The corners of the
simplex below correspond to cases where most samples come from one of the $K =
3$ classes. A good classification tree will have learned regions $R_{l}$ whose
class distributions are close to one of the $K$ corners of the simplex
(similarly to how a good regression tree learns regions $R_{l}$ with low RSS).

   ```{r}
   #| echo: false
   #| fig-width: 8
   #| fig-height: 4
   library(ggtern)
   grid <- expand_grid(p1 = seq(0.01, 0.98, length.out = 80),
                       p2 = seq(0.01, 0.98, length.out = 80)) |>
     mutate(p3 = 1 - p1 - p2) |>
     filter(p3 > 0.01) |>
     mutate(
       Gini = p1 * (1 - p1) + p2 * (1 - p2) + p3 * (1 - p3),
       Entropy = -(p1 * log(p1) + p2 * log(p2) + p3 * log(p3))
     ) |>
     pivot_longer(c(Gini, Entropy), names_to = "measure", values_to = "value")

   ggtern(grid, aes(x = p1, y = p2, z = p3, fill = value)) +
     geom_point(shape = 21, size = 0.8, stroke = 0) +
     scale_fill_distiller(palette = "RdYlBu", direction = -1) +
     facet_wrap(~ measure) +
     labs(fill = "Value",
          x = expression(hat(p)[1]),
          y = expression(hat(p)[2]),
          z = expression(hat(p)[3])) +
     theme_minimal()
   ```

1. Given this new objective, we can define the tree growing induction as before:
   $$
\Delta(l, j, s) = F\left(R_l\right) - \left[F\left(R_{l}^{\text{left}}(j, s)\right) + F\left(R_{l}^{\text{right}}(j, s)\right) \right]
   $$
   where $F$ denotes either the Gini index or Entropy. As before, we search over candidate leaves, directions, and thresholds to select the best possible split,
   $$
(l^*, j^*, s^*) := \arg\max_{l, j, s} \Delta(l, j, s)
   $$

## Categorical Predictors

1. So far we've only considered continuous features $x \in \reals^{J}$. For categorical feature $x_j$ with $Q$ levels, we split by choosing a subset $A \subset \{1, \dots, Q\}$ and splitting on whether $x_{j} \in A$.

   ![](figures/candidate_splits_categorical.png){width="400px"}

1. The RSS reduction for a categorical split becomes,

   $$
   \Delta(l, j, A) = \sum_{i: x_i \in R_l} (y_i - \bar{y}_{R_l})^2 - \left[\sum_{i: x_i \in R_l, \, x_{ij} \in A} (y_i - \bar{y}_{A})^2 + \sum_{i: x_i \in R_l, \, x_{ij} \in A^C} (y_i - \bar{y}_{A^C})^2\right]
   $$
   where $R_l^{A}(j) = \{x \in R_l : x_j \in A\}$ and similarly for $A^{C}$. We
   search over all candidate $l, j$, and subsets $A$ to find a choice that
   optimizes the RSS.

1. $^{\dagger}$ When $Q$ is large, there are $2^{Q - 1}$
possible subsets of $A$.  For regression, we can reduce this to just
$Q - 1$ splits.

   - For each level $q \in \{1, \ldots, Q\}$, compute the average $y_i$ among training points in $R_l$ with that level:
     $$
     \bar{y}_{q} = \frac{1}{|\{i : x_i \in R_l, \, x_{ij} = q\}|} \sum_{i : x_i \in R_l, \, x_{ij} = q} y_{i}
     $$
   - Sort the levels by these averages: $\bar{y}_{q_1} \leq \bar{y}_{q_2} \leq \cdots \leq \bar{y}_{q_Q}$
   - Only consider splits that respect this ordering: $A = \{q_1, \ldots, q_s\}$ for $s = 1, \ldots, Q-1$

   Intuitively, levels with similar average responses are grouped together in
   candidate splits.

   _Exercise: In the figure above, why don't we consider $A = \{1\}$ as a candidate split?_

1. Similar ordering strategies are possible for classification trees, but they
are beyond the scope of this class.

## Cost Complexity Pruning

1. The tree growing algorithm may overfit, producing large trees with poor test performance and interpretability.

1. We could consider requiring larger improvements in $\Delta\left(l, j,
s\right)$ before making a split. However, this is not a good idea, -- splits
that don't seem promising at first might enable valuable splits later,
especially when features interact. For example, the initial split for $x_{1}$
below only gives a modest $\Delta$, since there is a mix of high and low $y$
values. After the split on $x_{2}$, though, the prediction is nearly perfect. A
conservative rule that rejects the first split would miss this signal entirely.

   ```{r}
   #| echo: false
   #| fig-width: 5
   #| fig-height: 4
   #| cache: false
   n <- 200
   and_data <- tibble(
     x1 = runif(n),
     x2 = runif(n),
     y = 3 * (x1 > 0.5 & x2 > 0.5) + rnorm(n, sd = 0.5)
   )

   ggplot(and_data, aes(x1, x2, col = y)) +
     geom_point(size = 1.2) +
     geom_vline(xintercept = 0.5, linetype = "dashed", linewidth = 0.5) +
     geom_hline(yintercept = 0.5, linetype = "dashed", linewidth = 0.5) +
     scale_color_scico(palette = "glasgow", direction = -1) +
     theme_classic() + theme(panel.border= element_rect(fill = NA, linewidth = .5)) +
     labs(x = expression(x[1]), y = expression(x[2]), col = "y")
   ```

1. Let $T_0$ be the full tree. For penalty $\alpha \geq 0$, we find a subtree $T \subset T_0$ (formed by collapsing internal nodes) that minimizes:
   $$\mathcal{L}_{\alpha} = \sum_{l=1}^{|T|} \sum_{i: x_i \in R_l} (y_i - \bar{y}_{R_l})^2 + \alpha |T|$$
   where $|T|$ is the number of leaves in $T$. The first term is the training
   error, while $\alpha \left|T\right|$ penalizes tree complexity.  When $\alpha
   = 0$, we keep the original tree. When $\alpha$ increases, the optimal subtree
   gets smaller. We tune $\alpha$ with cross-validation.

   ![](figures/leaf_pruning.png){width="400px"}

1. $^{\dagger}$ For an internal node $t$ in the tree, define,
      - $T_t$: The subtree rooted at $t$. I.e., the tree with root $t$ and all its
   descendants.
      - $\text{RSS}\left(t\right)$: The RSS if we collapse $T_t$ into a single leaf $t$,
     $$
   \begin{align*}
   \text{RSS}\left(t\right) &= \sum_{i \in T_{t}} \left(y_i - \bar{y}_{T_t}\right)^{2}
   \end{align*}
     $$
     where $\bar{y}_{T_{t}}$ is the average response among training samples in $T_{t}$.
     - $\text{RSS}\left(T_t\right)$: The RSS when keeping all leaves in $T_t$,
     $$
   \begin{align*}
   \text{RSS}\left(T_t\right) &= \sum_{l \in T_t}\sum_{i \in R_{l}} \left(y_i - \bar{y}_{R_{l}}\right)^{2}.
   \end{align*}
     $$

1. $^{\dagger}$ Keeping $T_t$ vs. collapsing it improves the RSS by $\text{RSS}(t) - \text{RSS}(T_t)$ at a cost of $|T_t| - 1$ extra leaves. The cost-benefit ratio is:
   $$
   \alpha_t := \frac{\text{RSS}(t) - \text{RSS}(T_t)}{|T_t| - 1}
   $$
   This is the RSS reduction per additional leaf. Nodes with small $\alpha_t$
   provide little benefit relative to their complexity.

1. $^{\dagger}$ This suggests the weakest-link pruning algorithm.
   - Compute $\alpha_t$ for each internal node $t$ in the current tree
   - Collapse the subtree $T_t$ with the smallest $\alpha_t$ (the "weakest link")
   - Repeat until only the root remains

   This yields a sequence of subtrees $T_0 \supset T_1 \supset \cdots \supset
   T_M$. The $m$-th pruning step removes a node with cost-benefit ratio
   $\alpha_{(m)}$, giving thresholds $0 = \alpha_{(0)} < \alpha_{(1)} < \cdots <
   \alpha_{(M)}$. For any penalty $\alpha \in [\alpha_{(m)}, \alpha_{(m+1)})$,
   the optimal subtree minimizing $\mathcal{L}_{\alpha}(T)$ is $T_m$.

## Code Example

1. We'll use `rpart` to fit regression trees to the 2D dataset below, which has
strong interactions between $x_1$ and $x_2$ and isn't a simple piecewise
constant surface.

   ```{r}
library(rpart)
library(rpart.plot)

sim_data <- tibble(
  x1 = runif(500),
  x2 = runif(500),
  y = 10 * sin(pi * x1 * x2) + 20 * (x2 - 0.5)^2 + rnorm(500)
)
   ```

   ```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
grid <- expand_grid(
    x1 = seq(0, 1, length.out = 100),
    x2 = seq(0, 1, length.out = 100)
  ) |>
  mutate(y_true = 10 * sin(pi * x1 * x2) + 20 * (x2 - 0.5)^2)

ggplot(grid, aes(x1, x2)) +
  geom_tile(aes(fill = y_true)) +
  geom_point(data = sim_data, col = "black", size = 1.5) +
  geom_point(data = sim_data, aes(col = y), size = 1) +
  scale_color_scico(palette = "glasgow", guide = FALSE) +
  scale_fill_scico(palette = "glasgow") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  labs(fill = "f(x)")
   ```

1. We fit a regression tree with default $\alpha$. Each leaf region $R_{l}$ predicts $\bar{y}_{R_{l}}$.

   ```{r}
fit <- rpart(y ~ x1 + x2, data = sim_data)
rpart.plot(fit)
   ```

   This is the associated partition of the feature space $\mathcal{X} = \left[0,
1\right]^{2}$. Each leaf in the tree is one axis-aligned rectangle.

   ```{r}
grid$pred <- predict(fit, newdata = grid)
   ```

   ```{r}
#| fig-width: 5
#| fig-height: 4
#| echo: false
ggplot(grid, aes(x1, x2, fill = pred)) +
  geom_tile() +
  scale_fill_scico(palette = "glasgow") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  labs(fill = expression(hat(f)(x)))
   ```

1. Let's study the influence of $\alpha$.  The `cp` hyperparameter in rpart is
$\alpha$ normalized by the root node's RSS; the normalization makes `cp`
comparable across datasets. We can see that when cp is small, we are left with a
large tree (fine partition), while when cp is large, we have a simpler tree
(coarse partition).

   ```{r}
#| fig-width: 9
#| fig-height: 3
par(mfrow = c(1, 3), mar = c(2, 2, 3, 1))
for (cp_val in c(0.1, 0.01, 0.001)) {
  rpart.plot(
    rpart(y ~ x1 + x2, data = sim_data, cp = cp_val),
    main = paste("cp =", cp_val)
  )
}
   ```

   ```{r}
#| echo: false
#| fig-width: 9
#| fig-height: 3
cp_preds <- map_dfr(c(0.1, 0.01, 0.001), \(cp_val) {
  fit_cp <- rpart(y ~ x1 + x2, data = sim_data, cp = cp_val)
  grid |>
    select(x1, x2) |>
    mutate(pred = predict(fit_cp, newdata = grid), cp = factor(cp_val))
})

ggplot(cp_preds, aes(x1, x2, fill = pred)) +
  geom_tile() +
  facet_wrap(~cp, labeller = label_both) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_scico(palette = "glasgow") +
  labs(fill = expression(hat(f)(x)))
   ```

1. `fit$frame` and `fit$where` help inspect individual nodes. `fit$frame` gives node statistics: sample counts, average values $\bar{y}_{R_{l}}$, and pruning thresholds $\alpha_{m}$.

   ```{r}
fit$frame
   ```

   `fit$where` tells which leaf each point belongs to; i.e., it groups similar
   points from the tree's point of view. The vector names are the data.frame row
   numbers from the frame object above.

   ```{r}
table(fit$where)
   ```
