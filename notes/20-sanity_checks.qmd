
### 20. Sanity Checks and Benchmarking

* application to case study

* sanity checks

* remove and retrain

---

### Learning Outcomes

* Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.
* Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.

---

### Readings

* Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., & Kim, B. (2018). Sanity Checks for Saliency Maps. doi:10.48550/ARXIV.1810.03292
* Captum - Model Interpretability for Pytorch.  Getting started with Captum - Titanic Data Analysis. (n.d.).  https://captum.ai/tutorials/Titanic_Basic_Interpret


---

### Dataloader

---

### Getting predictions

---

### Gradient-based expalantions

---

### Integrated gradient explanations

---

### SHAP explanations

---

### Exercise: Model critique

---

## Sanity Checks

---

### Troubling observation

---

### Explanations $\iff$ edge detectors??

---

### Model randomization check

---

### Cascading randomization

---

### Data randomization check

---

### Statistical testing

---

## ROAR

---

### Removing features

---

### Need for retraining

---

### Remove-and-retrain (ROAR)

---

### Random ranking

---

### Simulation Validation

---

### ROAR curves

---

### Pseudocode

---

### Exercise: Purpose and comparison

why benchmark, and how do evaluation methods differ?

---
