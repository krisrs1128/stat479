### 21. Linear Probes and Concepts

* motivating application

* linear probes

* training concept vectors

* probes vs. concepts

---

### Learning Outcomes

* Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks
* Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.
* Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.

---

### Readings

Alain, G., & Bengio, Y. (2017). Understanding intermediate layers using linear classifier probes. OpenReview. https://openreview.net/forum?id=ryF7rTqgl

Schmalwasser, L., Penzel, N., Denzler, J., & Niebling, J. (2025). FastCAV: Efficient computation of concept activation vectors for explaining deep neural networks. In Proceedings of the 42nd International Conference on Machine Learning. https://openreview.net/forum?id=kRmfzTfIGe

---

### Motivating Case Study

---

### Case Study

**Question**. Can we complement on-the-ground socioeconomic surveys with passively observed aerial imagery?

**Main Data**. Coupled survey data + aerial imagery.
- Survey responses for 51,781 "liveability grid cells" of size 100m^2
- Imagery (1m/px). 500 x 500 patches.
- Gathered from 13 areas in the Netherlands.

**Supplemental Data**.
- FLAIR. Land use from French National Institute of Geographical and Forest Information.

**Goal**. Underestand what concepts the model has learned, so it can be deployed with more confidence.

---

### Formulation

**Livability**. $y \in \mathbf{R}$
**Predictors**. $\mathcal{D}_{r} = x \in \mathbf{R}^{3 \times 500 \times 500}$. Imagery.

**Concept labels**. $c \in \{1, \dots, K\}$.
**Concepts**. $\mathcal{D}_{c} = x \in \mathbf{R}^{3 \times 500 \times 500}$.

**Model**. $f(x; \theta)$
**Task**. Which "concepts" does the

---

<img src="figures/kim_iclr.png" class="center"/>

---

## Linear Probes

---

### Notation

---

### Estimation

---

### Cartoon

---

### Outputs

---

### Example: Skip Connections

This flags bugs in complex architectures.

.center[
<span style="font-size: 18px;">
<img width="500" src="figures/pathological_training.gif"/> <br/>
In this example from `r Citep(bib, "alain2017understanding")`, a skip connection across layers 1 - 64  has prevented learning across those layers.
</span>
]

---

### Exercise: Pseudocode

---

## Concept Activation Vectors

---

### Notation

---

**Step 1**. Define a collection of images $\*x_{n}$ that represents the concept. Also construct a control pool of random images $\*x_{n}^\prime$.

.center[
<span style="font-size: 18px;">
<img src="figures/concepts_step1.png" width=600/><br/>
Here, we are interested in the concept "stripes." Figure from `r Citep(bib, "tcav")`.
</span>
]

---

**Step 2**. Use an intermediate layer's activations $h\left(\*x\right)$ as
predictors for a linear classifier to distinguish the groups. This is like
probes, except the task is derived from Step 1.

.center[
<span style="font-size: 18px;">
<img src="figures/concepts_step2.png" width=530/><br/>
Let $\*v$ denote the normal to the decision boundary.
</span>
]

---

**Step 3**. Compare the direction $v$ to the directions in the embedding spac
that increase each sample's class $k$ probability.

\begin{align*}
S_{k}\left(\*x\right) = \nabla y_{k}\left(h\left(\*x\right)\right)^\top \*v
\end{align*}

.center[
<span style="font-size: 18px;">
<img src="figures/concepts_step3.png" width=700/><br/>
$\nabla y_{k}\left(h\left(\*x\right)\right)$ is the direction in the activation
space with the steepest increase in class $k$'s logit.
</span>
]

---

### Alternative Step 2: FastCAV

---

### Testing

- We should check that we're not seeing patterns in noise. It's possible the
model has learned nothing about a concept.

- Next time, define a hypothesis test using "negative control" CAVs.

---

### Exercise: CAV for a 2-layer network

---

## Probes vs. Concepts

---

### Similarities

---

### Probes-specific

---

### Concepts-specific

---