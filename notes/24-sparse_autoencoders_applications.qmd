# 22. Applying Sparse Autoencoders

* interpreting clusters

* application to case study

* model safety and control

---

### Learning Outcomes

* Describe the theoretical foundation of post-hoc explanation methods like SHAP and linear probes values and apply them to realistic case studies with appropriate validation checks
* Analyze large-scale foundation models using methods like sparse autoencoders and describe their relevance to problems of model control and AI-driven design.
* Within a specific application context, evaluate the trade-offs associated with competing interpretable machine learning techniques.

---

### Monosemanticity

---

### Examples of Superposition

---

### Manual Interpretability

---

### Automated Interpretability

---

### Feature Activation Distributions

---

### Golden Gate Feature

---

### Code Error Feature

---

### Reading $t$-SNE Plots

---

### Feature Neighborhoods

---

### Feature Completeness

---

### Computational Intermediates

---

## Case Study

---

### Data Overview

(add description of the data, how it was put into a loader)

---

### Extracting Activations

(add figure from code demo)

---

### Fitting SAE

(add code snippet)

---

### Training Losses

(add figures)

---

### Example Features

(will add based on code)

---

### Example Features

(will add based on code)

---

### Takeaways

- assumptions
- checks

---

## Safety and Control

---

### Steering Generations

---

### Mathematical Implementation

---

### Steering Protein Language Models

---

### Exercise

(group bias adaptation)