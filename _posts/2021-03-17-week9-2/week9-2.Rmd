---
title: "Hierarchical Clustering"
description: |
  Clustering data at multiple scales using trees.
author:
  - name: Kris Sankaran
    affiliation: UW Madison
date: 2021-03-22
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE)
```

_[Reading](https://rafalab.github.io/dsbook/clustering.html), [Recording](https://mediaspace.wisc.edu/media/Week%209%20%5B2%5D%20Hierarchical%20Clustering/1_8f9hgzpc), [Rmarkdown](https://github.com/krisrs1128/stat479/blob/master/_posts/2021-03-17-week9-2/week9-2.Rmd)_

```{r}
library("dplyr")
library("ggplot2")
library("ggraph")
library("knitr")
library("readr")
library("robservable")
library("tidygraph")
theme_set(theme_graph())
```

1. In reality, data are rarely separated into a clear number of homogeneous
clusters. More often, even once a cluster formed, it's possible to identify a
few subclusters. For example, if you initially clustered movies into "drama" and
"scifi", you might be able to further refine the scifi cluster into "time
travel" and "aliens."

2. $K$-means only allows clustering at a single level of magnification. To
instead simultaneously cluster across scales, you can use an approach called
hierarchical clustering. As a first observation, note that a tree can be used to
implicitly store many clusterings at once. You can get a standard clustering by
cutting the tree at some level.

```{r, echo = FALSE, fig.cap = "We can recover clusters at different levels of granularity, by cutting a hierarchical clustering tree."}
include_graphics("https://drive.google.com/uc?id=11YZtI_rbkRNAwj5rArUBh1nNYZBmSRVG")
```

3. These hierarchical clustering trees can be thought of abstract versions of
the taxonomic trees. Instead of relating species, they relate observations in a
dataset.

```{r}
robservable("@mbostock/tree-of-life", height = 1150)
```

4. Elaborating on this analogy, the leaves of a hierarchical clustering tree are
the original observations. The more recently two nodes share a common
ancestor, the more similar those observations are.

5. The specific algorithm proceeds as follows,

	* Initialize: Associate each point with a cluster $C_i := \{x_i\}$.
	* Iterate until only one cluster: Look at all pairs of clusters. Merge the pair
	$C_k, C_{k^{\prime}}$ which are the most similar.
	
```{r, fig.cap = "At initialization, the hierarchical clustering routine has a cluster for each observation.", echo = FALSE}
include_graphics("https://drive.google.com/uc?id=1-VCD5H7YZMLlgCsm02qHCqZO-l0xadRo")
```
	
```{r, fig.cap = "Next, the two closest observations are merged into one cluster. This is the first merge point on the tree." , echo = FALSE}
include_graphics("https://drive.google.com/uc?id=1bxFQlxfq2BZ6owPw3K2Ft5e7jE6eB6HE")
```
	
```{r, fig.cap = "We continue this at the next iteration, though this time we have compute the pairwise distance between all clusters, not observations (technically, all the observations were their own cluster at the first step, and in both cases, we compare the pairwise distances between clusters).", echo = FALSE}
include_graphics("https://drive.google.com/uc?id=1Jcmlq5qFuNA42IRruIKqlYAc8JM5wys8")
```

```{r, fig.cap = "We can continue this process...", echo = FALSE}
include_graphics("https://drive.google.com/uc?id=1kYqnHe_ARcpYmaMGlwQq9uAM7urGdzjn")
```

```{r, fig.cap = "... and eventually we will construct the entire tree.", echo= FALSE}
include_graphics("https://drive.google.com/uc?id=1_n0bm0raZkexZgmUuhD4wO0sRlpGHHCc")
```

```{r, echo = FALSE}
include_graphics("https://drive.google.com/uc?id=11_zq5WYue-FICbWbnIg8OQoQDVaG--x-")
```
	
6. In R, this can be accomplished by using the `hclust` function. First, we
compute the distances between all pairs of observations (this provides the
similarities used in the algorithm). Then, we apply `hclust` to the matrix of
pairwise distances.

7. We apply this to a movie ratings dataset. Movies are considered similar if
they tend to receive similar ratings across all audience members. The result is
visualized below.

```{r, fig.height= 7}
movies_mat <- read_csv("https://uwmadison.box.com/shared/static/wj1ln9xtigaoubbxow86y2gqmqcsu2jk.csv")

D <- movies_mat %>%
  column_to_rownames(var = "title") %>%
  dist()

hclust_result <- hclust(D)
plot(hclust_result, cex = 0.5)
```

8. We can customize our tree visualization using the ggraph package. We can
convert the hclust object into a ggraph, using the same `as_tbl_graph` function
from the network and trees lectures.

```{r}
hclust_graph <- as_tbl_graph(hclust_result, height = height)
hclust_graph <- hclust_graph %>%
  mutate(height = ifelse(height == 0, 27, height)) # shorten the final edge
hclust_graph
```

```{r, fig.width = 10, fig.height= 10}
ggraph(hclust_graph, "dendrogram", height = height, circular = TRUE) +
  geom_edge_elbow() +
  geom_node_text(aes(label = label), size = 4) +
  coord_fixed()
```

8. We can cut the tree to recover a standard clustering. This is where the
grammar-of-graphics approach from ggraph becomes useful -- we can encode the
cluster membership of a movie using color, for example.

```{r}
cluster_df <- cutree(hclust_result, k = 10) %>% # try changing K and regenerating the graph below
  tibble(label = names(.), cluster = as.factor(.))
cluster_df
```

```{r, fig.width = 10, fig.height = 10}
# colors chosen using https://medialab.github.io/iwanthue/
cols <- c("#51b48c", "#cf3d6e", "#7ab743", "#7b62cb", "#c49644", "#c364b9", "#6a803a", "#688dcd", "#c95a38", "#c26b7e")
hclust_graph %>%
  left_join(cluster_df) %>%
  ggraph("dendrogram", height = height, circular = TRUE) +
  geom_edge_elbow() +
  geom_node_text(aes(label = label, col = cluster), size = 4) +
  coord_fixed() +
  scale_color_manual(values = cols) +
  theme(legend.position = "none")
```