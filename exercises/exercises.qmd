# Interpretable Machine Learning Exercises

## Motivation

1. [Past Experience] The need for interpretable machine learning arises across many areas of study. This question asks you to share your experience with some of these areas.

    - Introduce yourself to your neighbors. What is your name and degree program? What are your areas of interest? How might interpretability or explainability be helpful in the work that you do?
    - Have you encountered a situation in the past where you think interpretability or explainability would be useful? Describe the situation and your overall goals. How did you address them and are there any ways the approach could have been improved?

1. [Jargon-free summary] Imagine you are working as a statistical consultant
and have chosen to include a model explanation technique from this week. Your
clients are familiar with the basics of machine learning (e.g., supervised vs.
unsupervised learning) but not the interpretability literature. Give a
jargon-free but precise description of {filled in during class}. What does it
output and what is the correct interpretation?

1. [Example Critique] Summarize example {given in class} from {this week's
reading}. What kinds of real-world situations is that example supposed to
reflect?  Do you think the proposed method would actually work in those
situations, and what kinds of difficulties can you imagine arising in practice?

1. [Clear/Confusing] For this week's material, describe,

    - A concept that you learned for the first time.
    - A check you can apply to ensure your interpretations are reliable.
    - A point that are not confident you could apply in practice.

    Be as specific as possible.

## Intrinsically Interpretable Models

1. [Linear Model Interpretability]

    - simulatability
    - modularity

1. [Prior and Posterior Predictive Distributions]

1. [GP Missing Data]

1. [GPs Marginal Likelihood].

1. diagram tree <--> partition

1. kernels and GP samples
    - plot kernel k(x, x' = 1) for a few kernels
    - draw samples
    - same with product or sum of kernels

1. [Tree Importances] In your own words, summarize how one of these techniques
measures variable importance in tree models: Permutation VI, MDI, MDI+. For your
chosen approach, what is one strength and one weakness?

1. [Split Choice Visual Explanation] Develop a visual explanation of the
following excerpt from our reading (equations 8.2 - 8.3 in ISLR). Make sure to
explain how the mathematical symbols relate to the parts of your visual
explanation.  Precisely describe any simplifying assumptions to make the figure
clearer.

    > For any $j$ and $s$, we define the pair of half-planes
    $$R_1(j, s)=\left\{X \mid X_j<s\right\} \text { and } R_2(j, s)=\left\{X \mid X_j \geq s\right\},$$
    and we seek the value of j and s that minimize the equation
    $$\sum_{i: x_i \in R_1(j, s)}\left(y_i-\hat{y}_{R_1}\right)^2+\sum_{i: x_i \in R_2(j, s)}\left(y_i-\hat{y}_{R_2}\right)^2.$$


1. [Intrinsic Interpretability Portfolio] The goal of this exercise is to give
you a chance to experiment with two intrinsic interpretability techniques in a
problem of your choice. The most important aspect of the exercise is that you
have a chance to get hands-on experience with the topics discussed in this
course, so that you can develop a more critical eye towards what does and does
not work well in a topic that you care about.

    a. **Problem Formulation**. Describe the problem context, a representative dataset,
    and why intrinsic interpretability is important. What types of people would be
    the main beneficiaries of effective interpretation/explanation, and what exactly
    would they be able to get out of it? Search for related literature applying
    interpretable ML methods to this type of data and briefly summarize the main
    approaches you identify.

    b. **Method 1 Application**. Train an intrinsically interpretable model to
    the dataset you described above. Report the out-of-sample prediction
    accuracy and an interpretability-related output (e.g., random forest
    variable importance plot or GP posterior samples).

    c. **Method 2 Application**. Repeat the previous part with the a new
    intrinsically interpretable model.

    d. **Discussion**. Compare the results from the two methods. Which
    conclusions are stable and which are not, and can you offer potential
    reasons for differences?  What would you share in a report to one of the
    beneficiaries you identified above?  Include 1 - 2 figures resulting from
    your analysis and summarize how to read them.

    e. **Sanity Check**. Prepare a follow-up check to evaluate the reliability
    of one of the models. It is okay if the results raise potential problems,
    just make sure to discuss how they affect the takeaways. For example, you
    may:

        - Repeat the analysis with alternative hyperparameters to see that the key conclusions are robust.
        - Permute labels in a labeled dataset to serve as a no-signal control.
        - Repeat the analysis on bootstrap versions of the data, to check that conclusions are stable.

1. random forest variable importance after trimming top variables

    - train RF, tune over mtry
    - explain how variable importance is defined
    - identify most important features
    - remove those features and retrain the model
    - what is the drop in performance? explain observations

1. collinearity with lasso and decision trees

    - Use EDA to identify the features that are most collinear
    - write a function that bootstraps the lasso. Return matrix B x D of coefficient values across bootstrap samples.
    - write a function that does the same with trees (optionally, may write a single function that solves both)

1. boosting from scratch

    - fit shallow decision tree to simulated data. plot x vs. y + f1(x)
    - extract residuals. plot x vs. e
    - fit new decision tree to residuals. plot x vs. e + f2(x) and x vs. y + ( f1(x) + f2(x) )
    - comment on implications for interpretability.

## Post-hoc Explanations

1. [SHAP Waterfall] We saw a visualization of $\varphi_{x}\left(f, i\right)$ for
a single sample $i$ in the census dataset. The plot below gives the analogous
visualization for all N samples in that data. How do you interpret it?

    ```{r}
    #| label: waterfall
    #| out-width: 70%
    #| align: center
    knitr::include_graphics("figures/shap_waterfall.png")
    ```

1. [SHAP Visual Explanation] We gave a geometric interpretation of $v({1})$ in a
toy example. Can you give the geometric interpretation of: $v({1, 2}), v({2})$, or
$v({\empty})$?

    ```{r}
    #| label: shap-visual
    #| out-width: 70%
    #| align: center
    knitr::include_graphics("figures/shap_visual_exp.png")
    ```

1. [LinearSHAP Derivation] For certain choices of $v$, SHAP values can be
computed in closed form. For example, in class we showed that if
    \begin{align}
    f(x) = \beta_0 + \sum_{j=1}^d \beta_j x_j\\
    v(S) = f(x_S, x^{0}_{\bar{S}})
    \end{align}
    then $\phi_j(f, x) = \beta_j(x_j - x_j^{0}).$ In this problem, we will
    consider the alternative
    $$v(S) = \mathbb{E}[f(x_S^e, x_{\bar{S}})]$$
    where the randomness in the expectation is over $x_{\bar{S}}$.

    a. Describe the qualitative differences between the choices of $v$ in equations \eqref{}.

    b. Show that with the choice of $v$ in equation \eqref{}, the associated SHAP values are

    $$\phi_j(f, x) = \beta_j\left(x_j - \mathbf{E}\left[x_i\right]\right)$$

    c. Compare and contrast the SHAP values associated with the two choices of
    $v$. On what types of data do you expect the explanations to be similar or different?

1. [KernelSHAP Code] These questions accompany the KernelSHAP code snippet below.

        ```{python}
        #| eval: false
        # initialize data structures
        mask_matrix = np.zeros((nsamples, M))
        kernel_weights = np.zeros(nsamples)
        synth_data = np.tile(self.data, (nsamples, 1))

        # keep values at random subsets of features
        for i in range(nsamples):
            mask = np.random.choice([0, 1], size=M)
            mask_matrix[i] = mask
            synth_data[i * self.N:(i + 1) * self.N, mask == 1] = x[0, mask == 1]
            kernel_weights[i] = (M - np.sum(mask)) * np.sum(mask)

        # model predictions on masked data
        model_out = self.model(synth_data)
        f = np.mean(model_out.reshape(nsamples, self.N, -1), axis=1) - self.f_bar

        # weighted regression on adjusted f
        X = mask_matrix - mask_matrix[:, -1][:, None]
        y = f.flatten() - mask_matrix[:, -1] * (self.model(x) - self.f_bar)
        lm = LinearRegression(fit_intercept=False).fit(X, y, sample_weight=kernel_weights)
        ```

    a. How does the mask variable relate to the subsets $S$ discussed in the notes?
    b. How many linear regressions are run to compute SHAP values across all
    samples? How large are the input datasets for each of those regressions, and
    how is it related to nsamples?
    c. What is the relationship between the returned attributions $\varphi$ and
    the coefficients of our weighted linear regression?
    d. `synth_data` can be viewed as `nsamples` blocks of $N$ rows each. How do
    the blocks differ from one another?
    e. If we ignore the terms involving the last column of mask_matrix (i.e.,
    `mask_matrix[:, -1]`), then how can we interpret the response of the
    weighted linear regression?

1. [marginal vs. conditional shap]. In a dataset with highly colinear features,
see how conditional SHAP can give high attributions even when coefficients are
small. Explain why this discrepancy arises.

1. [importance of baselines] Two data scientists get different explanations for
the same model. They chose different baselines. Explain why they get the
different results, and explain the

1. [KernelSHAP variance] Compute KernelSHAP across different sample sizes and
for many replicates. What is the dependence on sample size? What would you
recommend?

1. [SHAP Stability]. Fix a model. Evaluate overall SHAP value when applied to
different subsamples.

1. [Comparing models with SHAP]
    - data splitting, then EDA on training set. Report one interesting finding.
    - fit RF + Lasso
    - compare prediction accuracy
    - compute SHAP values for a single run
    - Run bootstrap on training set. How do explanations change?

1. [Compare and Contrast] Compare and contrast the explainability techniques {A}
and {B} (A and B given in class). Comment on:
    - What are the final outputs of the method?
    - How do the mathematical formulations relate to one another?
    - Who is the assumed audience for the method's final output?

## Deep Learning

1. [Two moons]

1. [width vs. depth] Models with same total number of parameters, but change the

1. [Layernorm from Scratch]

1. [Normalization Ablations]

1. [Hotel Review Embeddings] This exercise explores how large language models
capture meaning from a corpus of [hotel
reviews](https://github.com/krisrs1128/stat436_f24/raw/refs/heads/main/data/reviews.csv).
We will use the llama3.2:1b model. (give details on tokenizing the reviews and
loading the models using either R or python)

    a. How many heads are used in the MHA mechanism in the first transformer layer?

    b. Extract embeddings at layers 2 and 9. Store the results in arrays with
    shape $N \times D_{l}$.

    c. Compute the pairwise distances $\|h_{i}^{l} - h_{i'}^{l}\|$ between all review
    embeddings at layers $l = 2$ and 9. Make a scatterplot where each point
    corresponds to a single pair $ii'$, the $x$-axis corresponds to $l = 2$, and
    the $y$-axis corresponds to $l = 9$.

    d. Find a pair of reviews that lies far from the identity line in the
    scatterplot from part (c). Comment on how this relates to property that
    deeper layers learn more abstract representations of the input.

some helper code:

        ```{python}
        #| eval: false
        def extract_embeddings(text, model, tokenizer, layer=-1):
            inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=False)
            with torch.no_grad():
                outputs = model(**inputs, output_hidden_states=True)
            return outputs.hidden_states[layer].mean(axis=(0, 1))


        # Load pre-trained model and tokenizer
        tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-125m")
        model = GPTNeoModel.from_pretrained("EleutherAI/gpt-neo-125m")
        model = model.eval()
        ```

1. [Integrated Gradients Loop]

1. [Self-Attention from Scratch]

    - implementation with matrix multiplications
    - implementation with einsum
    - comparison with pytorch implementation

1. [Sequence Length Scaling] For attention mechanism, see how computational cost
grows as sequence length increases.

1. [Sanity Checks Review] The reviews for Adebayo et al. 2018 are publicly
available. Reviewer 1 has some excellent critiques. Discuss the points below.
Why do you think the reviewer raised them, and what types of follow-up analysis
do they suggest?

    - "Role of and relation to human judgement: Visual explanations are useless
    if humans do not interpret them correctly (see framework in [1])... Do the
    proposed sanity checks help identify explanation methods which are more
    human friendly? Even if the answer to the last question is no, it would be
    useful to discuss."

    - What if the layers are randomized in the other direction (from input to
    output)? Is it still the classifier layer that matters most?

    - The analysis of a conv layer is rather hand wavy. It is not clear to me
    that edges should appear in the produced saliency mask as claimed at l241.
    The evidence in figure 6 helps, but it is not completely convincing and the
    visualizations do not (strictly speaking) immitate an edge detector (e.g.,
    look at the vegitation in front of the lighthouse).

1. [Saliency Map Context] In week 1, we introduced the following vocabulary:
inherently interpretable models, post-hoc explanations, local explanations,
global explanations. How do saliency maps relate to each of these terms?

1. [Saliency-based Model Critique]

1. [Model Developers vs. Users] Linear probes were proposed with model
developers in mind. How would you explain the outputs of either approach to an
audience of model users? First, explain what the main outputs are, and then
relate them to outputs they may be familiar from more basic statistics or
machine learning courses.

1. [Linear Probe Pseudocode]

1. [Probe Applications] Linear probes were presented in the context of natural
image classification problems. To what extent can these ideas be adapted to
other types of data? Choose one of the following dataset types below, and
discuss which elements of the two techniques do/do not seem generalizable to
that context:

    - Node classification in a network dataset
    - Text generation in a natural language dataset
    - Loan default prediction in a financial dataset
    - Cell type annotation in a single-cell dataset
    - Galaxy type classification from a telescope images dataset
    - Cancer tumor segmentation in a histopathology dataset

1. [FastCAV Visual Explanation] Develop a visual explanation for the following
excerpt from our reading (equations 6 in Schmalwasser et al. 2025). Make sure to
explain how the mathematical symbols relate to the parts of your visual
explanation.  Precisely describe any simplifying assumptions to make the figure
clearer.

    > In Equation (3), we again apply the respective maximum likelihood estimator $\hat{\mu}_{D_c}$ meaning $v_l^c \propto \hat{\mu}_{D_c} - \hat{\mu}_{D_c \cup D_r}$. Under the assumptions specified above and again drawing an expectation over the sample of random inputs and concept examples, we get $\mathbb{E}[v_l^c] \propto \mu_c - \frac{\mu_c + \mu_r}{2} = \frac{\mu_c - \mu_r}{2}$.

1. [FastCAV inputs/outputs] Review the concept-based explanation experiments
discussed in section 4.3 of Schmalwasser et al. (2025).

    - What are the inputs required for CAV extraction.
    - How do you read Figure 3 and what are the main claims the authors make based on it?
    - In your view, how strong is the evidence for the authors’ interpretation?

1. [Agency vs. Automation] The excerpt below comes from Agency plus automation:
Designing artificial intelligence into interactive systems (Heer 2018). Consider
how the techniques discussed this week relate to the intelligence augmentation
research agenda discussed here.

    > Much contemporary rhetoric regards the prospects and pitfalls of using artificial intelligence techniques to automate an increasing range of tasks, especially those once considered the purview of people alone….This long-standing focus on purely automated methods unnecessarily cedes a promising design space: one in which computational assistance augments and enriches, rather than replaces, people’s intellectual work. This tension between human agency and machine automation poses vital challenges for design and engineering…To improve outcomes and support learning by both people and machines, we describe the use of shared representations of tasks augmented with predictive models of human capabilities and actions. We conclude with a discussion of future prospects and scientific frontiers for intelligence augmentation research.

1. [Implement FISTA]

1. [SAE Applications] Can Sparse Autoencoders be applied to these problems? If so, explain how, if not, explain why not.

    - Updating all parameters when training an LLM is expensive. You would like to develop a new training strategy that pauses every 100,000 steps and freezes weights from layers that have not changed much since the last check.
    - You have extracted activations from a medical image classification model across two different datasets, one related to a vulnerable patient population and another from a healthy control. You would like to see what aspects of the learned representations differ across these groups.
    - A new deep learning model architecture has been introduced in your research area. A critic argues that the learned representations are not fundamentally different, despite the improved performance. You would like to resolve the dispute by objectively comparing the representations.

1. [SAE vs. Concepts] Compare and contrast testing concept activation vectors
with feature surveys in sparse autoencoders.

    - How do the methods compare in the way that they extract interpretable directions from model embedding spaces?
    - How do the methods compare with respect to the types of problems they apply their methods to?
